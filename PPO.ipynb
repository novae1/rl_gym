{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "N_OBSERVATIONS = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_AGENTS = 2\n",
    "T = 50\n",
    "GAMMA = 1.\n",
    "\n",
    "TEST = True\n",
    "TRAIN = False\n",
    "\n",
    "layer_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and selecting actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the same parameters for the policy and value networks. If this doesn't work well I can change it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAndValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(N_OBSERVATIONS, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            # +1 comes from value prediction\n",
    "            nn.Linear(layer_dim, N_ACTIONS + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select action based on log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_logits):\n",
    "    action_probs = torch.softmax(action_logits, dim=0)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_env = gym.make('CartPole-v1', render_mode='human')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "\n",
    "    for _ in range(1):\n",
    "        state, info = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        while not (terminated or truncated):\n",
    "            tensor_state = torch.from_numpy(state)\n",
    "            logits = test_model(tensor_state)\n",
    "            action_logits = logits[:-1]\n",
    "            action = select_action(action_logits)\n",
    "            state, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first collect the data while marking episode ends when they happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector():\n",
    "    def __init__(self, n_agents):\n",
    "        self.states = [[] for _ in range(n_agents)]\n",
    "        self.actions = [[] for _ in range(n_agents)]\n",
    "        self.rewards = [[] for _ in range(n_agents)]\n",
    "        self.pis = [[] for _ in range(n_agents)]\n",
    "        self.episode_ends = [[] for _ in range(n_agents)]\n",
    "\n",
    "    def add_step(self, agent_id, state, action, reward, pi):\n",
    "        self.states[agent_id].append(state)\n",
    "        self.actions[agent_id].append(action)\n",
    "        self.rewards[agent_id].append(reward)\n",
    "        self.pis[agent_id].append(pi)\n",
    "\n",
    "    def mark_episode_end(self, agent_id, timestep):\n",
    "        self.episode_ends[agent_id].append(timestep)\n",
    "\n",
    "    def fetch_data(self):\n",
    "        list_of_agent_states = [torch.stack(agent_states) for agent_states in self.states]\n",
    "        states = torch.stack([agent_states for agent_states in list_of_agent_states])\n",
    "        actions = self.actions\n",
    "        rewards = torch.tensor(self.rewards)\n",
    "        pis = torch.tensor(self.pis)\n",
    "        episode_ends = self.episode_ends\n",
    "\n",
    "        return states, actions, rewards, pis, episode_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: tensor([[[ 3.6938e-02, -3.6720e-02,  4.2823e-02, -1.0182e-02],\n",
      "         [ 3.6204e-02, -2.3243e-01,  4.2620e-02,  2.9570e-01],\n",
      "         [ 3.1555e-02, -4.2813e-01,  4.8534e-02,  6.0151e-01],\n",
      "         [ 2.2993e-02, -2.3372e-01,  6.0564e-02,  3.2450e-01],\n",
      "         [ 1.8318e-02, -4.2965e-01,  6.7054e-02,  6.3565e-01],\n",
      "         [ 9.7253e-03, -2.3553e-01,  7.9767e-02,  3.6482e-01],\n",
      "         [ 5.0148e-03, -4.1622e-02,  8.7064e-02,  9.8316e-02],\n",
      "         [ 4.1823e-03,  1.5215e-01,  8.9030e-02, -1.6568e-01],\n",
      "         [ 7.2253e-03,  3.4589e-01,  8.5716e-02, -4.2900e-01],\n",
      "         [ 1.4143e-02,  5.3970e-01,  7.7136e-02, -6.9348e-01],\n",
      "         [ 2.4937e-02,  3.4360e-01,  6.3267e-02, -3.7754e-01],\n",
      "         [ 3.1809e-02,  1.4764e-01,  5.5716e-02, -6.5603e-02],\n",
      "         [ 3.4762e-02, -4.8235e-02,  5.4404e-02,  2.4413e-01],\n",
      "         [ 3.3797e-02, -2.4409e-01,  5.9286e-02,  5.5346e-01],\n",
      "         [ 2.8916e-02, -4.3999e-01,  7.0356e-02,  8.6422e-01],\n",
      "         [ 2.0116e-02, -2.4589e-01,  8.7640e-02,  5.9446e-01],\n",
      "         [ 1.5198e-02, -5.2102e-02,  9.9529e-02,  3.3062e-01],\n",
      "         [ 1.4156e-02,  1.4147e-01,  1.0614e-01,  7.0906e-02],\n",
      "         [ 1.6985e-02,  3.3493e-01,  1.0756e-01, -1.8649e-01],\n",
      "         [ 2.3684e-02,  5.2836e-01,  1.0383e-01, -4.4340e-01],\n",
      "         [ 3.4251e-02,  7.2187e-01,  9.4962e-02, -7.0163e-01],\n",
      "         [ 4.8688e-02,  9.1556e-01,  8.0929e-02, -9.6298e-01],\n",
      "         [ 6.6999e-02,  1.1095e+00,  6.1669e-02, -1.2292e+00],\n",
      "         [ 8.9189e-02,  1.3038e+00,  3.7086e-02, -1.5019e+00],\n",
      "         [ 1.1527e-01,  1.1082e+00,  7.0474e-03, -1.1979e+00],\n",
      "         [ 1.3743e-01,  9.1301e-01, -1.6911e-02, -9.0301e-01],\n",
      "         [ 1.5569e-01,  7.1813e-01, -3.4971e-02, -6.1569e-01],\n",
      "         [ 1.7005e-01,  5.2351e-01, -4.7285e-02, -3.3422e-01],\n",
      "         [ 1.8052e-01,  3.2909e-01, -5.3969e-02, -5.6819e-02],\n",
      "         [ 1.8710e-01,  5.2494e-01, -5.5105e-02, -3.6603e-01],\n",
      "         [ 1.9760e-01,  3.3065e-01, -6.2426e-02, -9.1219e-02],\n",
      "         [ 2.0422e-01,  5.2660e-01, -6.4250e-02, -4.0293e-01],\n",
      "         [ 2.1475e-01,  7.2258e-01, -7.2309e-02, -7.1515e-01],\n",
      "         [ 2.2920e-01,  5.2853e-01, -8.6612e-02, -4.4608e-01],\n",
      "         [ 2.3977e-01,  3.3473e-01, -9.5533e-02, -1.8190e-01],\n",
      "         [ 2.4646e-01,  1.4109e-01, -9.9172e-02,  7.9178e-02],\n",
      "         [ 2.4929e-01, -5.2476e-02, -9.7588e-02,  3.3900e-01],\n",
      "         [ 2.4824e-01,  1.4389e-01, -9.0808e-02,  1.7207e-02],\n",
      "         [ 2.5112e-01,  3.4019e-01, -9.0464e-02, -3.0269e-01],\n",
      "         [ 2.5792e-01,  1.4646e-01, -9.6518e-02, -3.9851e-02],\n",
      "         [ 2.6085e-01, -4.7151e-02, -9.7315e-02,  2.2089e-01],\n",
      "         [ 2.5991e-01, -2.4076e-01, -9.2897e-02,  4.8136e-01],\n",
      "         [ 2.5509e-01, -4.3445e-01, -8.3270e-02,  7.4338e-01],\n",
      "         [ 2.4640e-01, -6.2833e-01, -6.8402e-02,  1.0087e+00],\n",
      "         [ 2.3383e-01, -8.2248e-01, -4.8228e-02,  1.2792e+00],\n",
      "         [ 2.1738e-01, -1.0170e+00, -2.2644e-02,  1.5564e+00],\n",
      "         [ 1.9705e-01, -1.2118e+00,  8.4836e-03,  1.8419e+00],\n",
      "         [ 1.7281e-01, -1.4070e+00,  4.5322e-02,  2.1372e+00],\n",
      "         [ 1.4467e-01, -1.6026e+00,  8.8066e-02,  2.4435e+00],\n",
      "         [ 1.1262e-01, -1.4083e+00,  1.3694e-01,  2.1791e+00]],\n",
      "\n",
      "        [[ 1.2853e-02, -5.5555e-03,  3.5108e-02, -2.0320e-03],\n",
      "         [ 1.2742e-02,  1.8905e-01,  3.5067e-02, -2.8343e-01],\n",
      "         [ 1.6523e-02,  3.8365e-01,  2.9399e-02, -5.6485e-01],\n",
      "         [ 2.4196e-02,  5.7835e-01,  1.8101e-02, -8.4813e-01],\n",
      "         [ 3.5763e-02,  7.7322e-01,  1.1388e-03, -1.1351e+00],\n",
      "         [ 5.1227e-02,  5.7808e-01, -2.1563e-02, -8.4203e-01],\n",
      "         [ 6.2789e-02,  3.8326e-01, -3.8403e-02, -5.5620e-01],\n",
      "         [ 7.0454e-02,  5.7890e-01, -4.9527e-02, -8.6073e-01],\n",
      "         [ 8.2032e-02,  3.8449e-01, -6.6742e-02, -5.8403e-01],\n",
      "         [ 8.9722e-02,  5.8048e-01, -7.8422e-02, -8.9697e-01],\n",
      "         [ 1.0133e-01,  3.8650e-01, -9.6362e-02, -6.2993e-01],\n",
      "         [ 1.0906e-01,  1.9285e-01, -1.0896e-01, -3.6908e-01],\n",
      "         [ 1.1292e-01, -5.7309e-04, -1.1634e-01, -1.1264e-01],\n",
      "         [ 1.1291e-01, -1.9385e-01, -1.1859e-01,  1.4119e-01],\n",
      "         [ 1.0903e-01,  2.7508e-03, -1.1577e-01, -1.8643e-01],\n",
      "         [ 1.0908e-01, -1.9054e-01, -1.1950e-01,  6.7606e-02],\n",
      "         [ 1.0527e-01,  6.0736e-03, -1.1815e-01, -2.6026e-01],\n",
      "         [ 1.0540e-01, -1.8718e-01, -1.2335e-01, -7.0558e-03],\n",
      "         [ 1.0165e-01, -3.8034e-01, -1.2349e-01,  2.4430e-01],\n",
      "         [ 9.4045e-02, -5.7350e-01, -1.1861e-01,  4.9562e-01],\n",
      "         [ 8.2575e-02, -3.7692e-01, -1.0870e-01,  1.6804e-01],\n",
      "         [ 7.5036e-02, -1.8043e-01, -1.0533e-01, -1.5686e-01],\n",
      "         [ 7.1428e-02,  1.6034e-02, -1.0847e-01, -4.8082e-01],\n",
      "         [ 7.1748e-02,  2.1251e-01, -1.1809e-01, -8.0563e-01],\n",
      "         [ 7.5999e-02,  4.0903e-01, -1.3420e-01, -1.1330e+00],\n",
      "         [ 8.4179e-02,  6.0563e-01, -1.5686e-01, -1.4646e+00],\n",
      "         [ 9.6292e-02,  4.1274e-01, -1.8615e-01, -1.2247e+00],\n",
      "         [-2.9743e-02, -9.2481e-03,  3.9615e-02, -4.1780e-02],\n",
      "         [-2.9928e-02, -2.0492e-01,  3.8779e-02,  2.6313e-01],\n",
      "         [-3.4027e-02, -1.0367e-02,  4.4042e-02, -1.7070e-02],\n",
      "         [-3.4234e-02,  1.8410e-01,  4.3701e-02, -2.9554e-01],\n",
      "         [-3.0552e-02,  3.7857e-01,  3.7790e-02, -5.7413e-01],\n",
      "         [-2.2981e-02,  1.8294e-01,  2.6307e-02, -2.6978e-01],\n",
      "         [-1.9322e-02,  3.7767e-01,  2.0912e-02, -5.5405e-01],\n",
      "         [-1.1768e-02,  5.7250e-01,  9.8306e-03, -8.4007e-01],\n",
      "         [-3.1855e-04,  7.6748e-01, -6.9708e-03, -1.1296e+00],\n",
      "         [ 1.5031e-02,  9.6270e-01, -2.9564e-02, -1.4245e+00],\n",
      "         [ 3.4285e-02,  1.1582e+00, -5.8054e-02, -1.7263e+00],\n",
      "         [ 5.7448e-02,  1.3539e+00, -9.2580e-02, -2.0365e+00],\n",
      "         [ 8.4527e-02,  1.5499e+00, -1.3331e-01, -2.3563e+00],\n",
      "         [ 1.1552e-01,  1.7459e+00, -1.8043e-01, -2.6868e+00],\n",
      "         [-2.5440e-02,  3.7933e-02, -8.5903e-03, -4.3257e-02],\n",
      "         [-2.4682e-02,  2.3318e-01, -9.4555e-03, -3.3864e-01],\n",
      "         [-2.0018e-02,  4.2843e-01, -1.6228e-02, -6.3429e-01],\n",
      "         [-1.1449e-02,  2.3354e-01, -2.8914e-02, -3.4676e-01],\n",
      "         [-6.7786e-03,  4.2906e-01, -3.5849e-02, -6.4842e-01],\n",
      "         [ 1.8027e-03,  6.2466e-01, -4.8818e-02, -9.5217e-01],\n",
      "         [ 1.4296e-02,  4.3023e-01, -6.7861e-02, -6.7522e-01],\n",
      "         [ 2.2901e-02,  6.2623e-01, -8.1365e-02, -9.8847e-01],\n",
      "         [ 3.5425e-02,  8.2234e-01, -1.0113e-01, -1.3056e+00]]])\n",
      "actions: [[0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]]\n",
      "rewards: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "pis: tensor([[-0.0689, -0.0680,  0.0181, -0.0678,  0.0182,  0.0176,  0.0146,  0.0148,\n",
      "          0.0176, -0.0657, -0.0676, -0.0699, -0.0701, -0.0697,  0.0183,  0.0171,\n",
      "          0.0148,  0.0142,  0.0152,  0.0183,  0.0208,  0.0242,  0.0282, -0.0513,\n",
      "         -0.0551, -0.0578, -0.0644, -0.0684,  0.0182, -0.0683,  0.0178,  0.0188,\n",
      "         -0.0646, -0.0685, -0.0690, -0.0705,  0.0192,  0.0186, -0.0687, -0.0691,\n",
      "         -0.0719, -0.0735, -0.0739, -0.0776, -0.0804, -0.0860, -0.0903, -0.0926,\n",
      "          0.0393, -0.0902],\n",
      "        [ 0.0154,  0.0156,  0.0191,  0.0235, -0.0630, -0.0668,  0.0174, -0.0677,\n",
      "          0.0172, -0.0688, -0.0702, -0.0699, -0.0682,  0.0200, -0.0680,  0.0205,\n",
      "         -0.0680, -0.0686, -0.0679,  0.0216,  0.0230,  0.0202,  0.0175,  0.0225,\n",
      "          0.0280, -0.0613, -0.0665, -0.0692,  0.0171,  0.0149,  0.0156, -0.0677,\n",
      "          0.0153,  0.0189,  0.0239,  0.0272,  0.0307,  0.0340,  0.0338,  0.0353,\n",
      "         -0.0478,  0.0151,  0.0161, -0.0688,  0.0159,  0.0203, -0.0677,  0.0202,\n",
      "          0.0261, -0.0642]])\n",
      "episode ends: [[], [26, 40]]\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    # Initialize training data, test environment\n",
    "    test_data_collector = DataCollector(N_AGENTS)\n",
    "    test_env = gym.make('CartPole-v1')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "    \n",
    "    for agent_id in range(N_AGENTS):\n",
    "        state, _ = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        \n",
    "        for step in range(T):\n",
    "            # Compute and divide model output\n",
    "            with torch.no_grad():\n",
    "                state = torch.from_numpy(state)\n",
    "                output = test_model(state)\n",
    "            action_logits, value_logit = output[:-1], output[-1]\n",
    "\n",
    "            # Select and perform action\n",
    "            action = select_action(action_logits)\n",
    "            next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "            # Compute pi\n",
    "            pi = action_logits[action]\n",
    "\n",
    "            # Store data\n",
    "            test_data_collector.add_step(agent_id, state, action, reward, pi)\n",
    "\n",
    "            # If terminated reset env and mark end, otherwise update state\n",
    "            if terminated or truncated:\n",
    "                test_data_collector.mark_episode_end(agent_id, step)\n",
    "                state, _ = test_env.reset()\n",
    "                terminated, truncated = False, False\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    states, actions, rewards, pis, episode_ends = test_data_collector.fetch_data()\n",
    "    print(f\"states: {states}\")\n",
    "    print(f\"actions: {actions}\")\n",
    "    print(f\"rewards: {rewards}\")\n",
    "    print(f\"pis: {pis}\")\n",
    "    print(f\"episode ends: {episode_ends}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute advantages and target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll need to compute advantages and target values. We'll start from the end of the data for each agent and move backwards. This makes the desired values easier to compute and to take into account episode ends.\n",
    "\n",
    "Gradients should be on here because I'll use the computations here to optimize the model.\n",
    "\n",
    "> If the trajectory terminated due to the maximal trajectory length T\n",
    "being reached, Vωold (st+n ) denotes the state value associated with state st+n as predicted by the state value\n",
    "network. Otherwise, Vωold (st+n ) is set to 0\n",
    "\n",
    "Instead of computing r_t + V(s_t+1) I'm using V(s_t). It's similar enough and shouldn't be that much of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages_and_target_values(model, states, rewards, episode_ends):\n",
    "    n_agents = len(episode_ends)\n",
    "\n",
    "    # We'll add them from last to first, then reverse list\n",
    "    reversed_target_values = [[] for _ in range(n_agents)]\n",
    "    reversed_advantages = [[] for _ in range(n_agents)]\n",
    "\n",
    "    for agent in range(n_agents):\n",
    "        for i in range(T):\n",
    "            # Get step state and reward and compute predicted value\n",
    "            state = states[agent, -i-1]\n",
    "            reward = rewards[agent, -i-1]\n",
    "            predicted_value = model(state)[-1]\n",
    "\n",
    "            # If step is final\n",
    "            if i in episode_ends[agent]:\n",
    "                target_value = reward\n",
    "            # If step is last but not final\n",
    "            elif i == 0:\n",
    "                target_value = predicted_value\n",
    "            # non-last non-final step\n",
    "            else:\n",
    "                # Get previous target value and compute \n",
    "                previous_target_value = reversed_target_values[agent][-1]\n",
    "                target_value = reward + GAMMA * previous_target_value\n",
    "            advantage = target_value - predicted_value\n",
    "\n",
    "            # Append target value and advantage\n",
    "            reversed_target_values[agent].append(target_value)\n",
    "            reversed_advantages[agent].append(advantage)\n",
    "\n",
    "    target_values = [torch.stack(agent_target_values[::-1]) for agent_target_values in reversed_target_values]\n",
    "    advantages = [torch.stack(agent_advantage[::-1]) for agent_advantage in reversed_advantages]\n",
    "\n",
    "    return target_values, advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target values: [tensor([ 2.0000e+01,  1.9000e+01,  1.8000e+01,  1.7000e+01,  1.6000e+01,\n",
      "         1.5000e+01,  1.4000e+01,  1.3000e+01,  1.2000e+01,  1.1000e+01,\n",
      "         1.0000e+01,  9.0000e+00,  8.0000e+00,  7.0000e+00,  6.0000e+00,\n",
      "         5.0000e+00,  4.0000e+00,  3.0000e+00,  2.0000e+00,  1.0000e+00,\n",
      "         2.8972e+01,  2.7972e+01,  2.6972e+01,  2.5972e+01,  2.4972e+01,\n",
      "         2.3972e+01,  2.2972e+01,  2.1972e+01,  2.0972e+01,  1.9972e+01,\n",
      "         1.8972e+01,  1.7972e+01,  1.6972e+01,  1.5972e+01,  1.4972e+01,\n",
      "         1.3972e+01,  1.2972e+01,  1.1972e+01,  1.0972e+01,  9.9725e+00,\n",
      "         8.9725e+00,  7.9725e+00,  6.9725e+00,  5.9725e+00,  4.9725e+00,\n",
      "         3.9725e+00,  2.9725e+00,  1.9725e+00,  9.7249e-01, -2.7508e-02],\n",
      "       grad_fn=<StackBackward0>), tensor([ 1.0000e+00,  1.1000e+01,  1.0000e+01,  9.0000e+00,  8.0000e+00,\n",
      "         7.0000e+00,  6.0000e+00,  5.0000e+00,  4.0000e+00,  3.0000e+00,\n",
      "         2.0000e+00,  1.0000e+00,  3.6999e+01,  3.5999e+01,  3.4999e+01,\n",
      "         3.3999e+01,  3.2999e+01,  3.1999e+01,  3.0999e+01,  2.9999e+01,\n",
      "         2.8999e+01,  2.7999e+01,  2.6999e+01,  2.5999e+01,  2.4999e+01,\n",
      "         2.3999e+01,  2.2999e+01,  2.1999e+01,  2.0999e+01,  1.9999e+01,\n",
      "         1.8999e+01,  1.7999e+01,  1.6999e+01,  1.5999e+01,  1.4999e+01,\n",
      "         1.3999e+01,  1.2999e+01,  1.1999e+01,  1.0999e+01,  9.9986e+00,\n",
      "         8.9986e+00,  7.9986e+00,  6.9986e+00,  5.9986e+00,  4.9986e+00,\n",
      "         3.9986e+00,  2.9986e+00,  1.9986e+00,  9.9860e-01, -1.3967e-03],\n",
      "       grad_fn=<StackBackward0>)]\n",
      "advantages: [tensor([20.0224, 19.0272, 18.0224, 17.0139, 16.0027, 15.0123, 14.0208, 13.0107,\n",
      "        12.0200, 11.0090, 10.0188,  9.0235,  8.0266,  7.0228,  6.0265,  5.0228,\n",
      "         4.0167,  3.0032,  2.0153,  1.0018, 28.9722, 27.9727, 26.9824, 25.9889,\n",
      "        24.9787, 23.9698, 22.9750, 21.9829, 20.9719, 19.9796, 18.9689, 17.9963,\n",
      "        16.9902, 15.9784, 14.9767, 13.9777, 12.9765, 11.9764, 10.9873,  9.9931,\n",
      "         8.9980,  7.9930,  6.9975,  5.9928,  4.9972,  3.9991,  2.9974,  1.9996,\n",
      "         0.9945,  0.0000], grad_fn=<StackBackward0>), tensor([ 1.0245, 11.0298, 10.0248,  9.0300,  8.0251,  7.0200,  6.0075,  5.0193,\n",
      "         4.0068,  3.0182,  2.0057,  1.0043, 37.0024, 36.0131, 35.0190, 34.0111,\n",
      "        33.0003, 32.0084, 31.0168, 30.0213, 29.0151, 28.0200, 27.0133, 26.0005,\n",
      "        25.0111, 24.0166, 23.0081, 22.0147, 21.0200, 20.0123, 19.0008, 18.0091,\n",
      "        17.0167, 16.0062, 15.0128, 14.0026, 13.0091, 12.0189, 11.0056, 10.0210,\n",
      "         9.0133,  8.0012,  7.0118,  6.0004,  5.0014,  3.9998,  3.0009,  2.0008,\n",
      "         0.9997,  0.0000], grad_fn=<StackBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    target_values, advantages = compute_advantages_and_target_values(test_model, states, rewards, episode_ends)\n",
    "    print(f\"target values: {target_values}\")\n",
    "    print(f\"advantages: {advantages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to modify the data so that I can sample random batches from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: torch.Size([2, 50, 4])\n",
      "states: torch.Size([100, 4])\n",
      "actions: (2, 50)\n",
      "actions: 100\n",
      "rewards: torch.Size([2, 50])\n",
      "rewards: torch.Size([100])\n",
      "pis: torch.Size([2, 50])\n",
      "pis: torch.Size([100])\n",
      "target values: (2, torch.Size([50]))\n",
      "target values: torch.Size([100])\n",
      "advantages: (2, torch.Size([50]))\n",
      "advantages: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"states: {states.shape}\")\n",
    "print(f\"states: {states.reshape(N_AGENTS * T, -1).shape}\")\n",
    "\n",
    "print(f\"actions: {len(actions), len(actions[0])}\")\n",
    "print(f\"actions: {len([action for agent in actions for action in agent])}\")\n",
    "\n",
    "print(f\"rewards: {rewards.shape}\")\n",
    "print(f\"rewards: {rewards.reshape(N_AGENTS * T).shape}\")\n",
    "\n",
    "print(f\"pis: {pis.shape}\")\n",
    "print(f\"pis: {pis.reshape(N_AGENTS * T).shape}\")\n",
    "\n",
    "print(f\"target values: {len(target_values), target_values[0].shape}\")\n",
    "print(f\"target values: {torch.cat(target_values).shape}\")\n",
    "\n",
    "print(f\"advantages: {len(advantages), advantages[0].shape}\")\n",
    "print(f\"advantages: {torch.cat(advantages).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPODataset(Dataset):\n",
    "    def __init__(self, states, actions, pis, target_values, advantages):\n",
    "        # Reshape all into (N_AGENTS * T, whatever (possibly 0))\n",
    "        self.states = states.reshape(N_AGENTS * T, -1)\n",
    "        self.actions = [action for agent in actions for action in agent]\n",
    "        self.pis = pis.reshape(N_AGENTS * T)\n",
    "        self.target_values = torch.cat(target_values)\n",
    "        self.advantages = torch.cat(advantages)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        state = self.states[i]\n",
    "        action = self.actions[i]\n",
    "        pi = self.pis[i]\n",
    "        target_value = self.target_values[i]\n",
    "        advantage = self.advantages[i]\n",
    "\n",
    "        return state, action, pi, target_value, advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transition.__new__() takes 5 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m pi \u001b[38;5;241m=\u001b[39m action_logits\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Store data\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtest_training_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Update state\u001b[39;00m\n\u001b[1;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mTrainingData.push\u001b[0;34m(self, state, action, reward, pi)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpush\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, reward, pi):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Initially we don't have the advantage and target value\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# they'll be computed during training\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     advantage, target_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mappend(\u001b[43mTransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_value\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Transition.__new__() takes 5 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    # Initialize training data, test environment\n",
    "    test_training_data = TrainingData()\n",
    "    test_env = gym.make('CartPole-v1')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "    \n",
    "    state, _ = test_env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    \n",
    "    while not (terminated or truncated):\n",
    "        # Compute and divide model output\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state)\n",
    "            output = test_model(state)\n",
    "        action_logits, value_logit = output[:-1], output[-1]\n",
    "\n",
    "        # Select and perform action\n",
    "        action = select_action(action_logits)\n",
    "        next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "        # Compute pi\n",
    "        pi = action_logits\n",
    "\n",
    "        # Store data\n",
    "        test_training_data.push(state, action, reward, pi)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "    # Unpack training data\n",
    "    state, action, reward, pi, _, _ = test_training_data.unpack()\n",
    "    print(f\"state: {state}\")\n",
    "    print(f\"action: {action}\")\n",
    "    print(f\"reward: {reward}\")\n",
    "    print(f\"pi: {pi}\")\n",
    "\n",
    "    # Compute advantage and target value for each transition\n",
    "    for i in range(len(test_training_data)):\n",
    "        print(test_training_data.batch_index)\n",
    "        i_data = test_training_data.fetch_batch(1)\n",
    "        print(i_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
