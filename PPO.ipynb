{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,\n",
    "    gravity=-10.0,\n",
    "    enable_wind=False,\n",
    "    wind_power=15.0,\n",
    "    turbulence_power=1.5,\n",
    ")\n",
    "\n",
    "N_OBSERVATIONS = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "\n",
    "TEST = False\n",
    "TRAIN = False\n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_AGENTS = 8\n",
    "T = 512\n",
    "GAMMA = 0.99\n",
    "EPSILON = 0.2\n",
    "H = 0.01 # Entropy constant\n",
    "V = 1.0 # Value constant\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "N_EPOCHS = 4\n",
    "N_TRAIN_LOOPS = 200\n",
    "MAX_GRAD_NORM = 0.5\n",
    "layer_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAndValueNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, layer_dim):\n",
    "        super().__init__()\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(n_observations, layer_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, n_actions)\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_output = self.shared_layers(x)\n",
    "        action_logits = self.policy_head(shared_output)\n",
    "        value = self.value_head(shared_output)\n",
    "        return torch.cat([action_logits, value], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select action based on action logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_logits):\n",
    "    action_probs = torch.softmax(action_logits, dim=0)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPODataset(Dataset):\n",
    "\tdef __init__(self, n_agents, n_timesteps, n_observations, n_actions, gamma):\n",
    "\t\t# Store variables\n",
    "\t\tself.n_agents = n_agents\n",
    "\t\tself.n_timesteps = n_timesteps\n",
    "\t\tself.n_observations = n_observations\n",
    "\t\tself.n_actions = n_actions\n",
    "\t\tself.gamma = gamma\n",
    "\n",
    "\t\t# Create tensors\n",
    "\t\tself.states = torch.zeros((n_agents, n_timesteps, n_observations))\n",
    "\t\tself.actions = torch.zeros((n_agents, n_timesteps))\n",
    "\t\tself.rewards = torch.zeros((n_agents, n_timesteps))\n",
    "\t\tself.logprobs = torch.zeros((n_agents, n_timesteps, n_actions))\n",
    "\t\tself.target_values = torch.zeros((n_agents, n_timesteps))\n",
    "\t\tself.advantages = torch.zeros((n_agents, n_timesteps))\n",
    "\n",
    "\t\t# Store episode ends\n",
    "\t\tself.episode_ends = [[] for _ in range(n_agents)]\n",
    "\n",
    "\tdef add_step(self, agent, t, state, action, reward, logprob):\n",
    "\t\tself.states[agent, t] = state\n",
    "\t\tself.actions[agent, t] = action\n",
    "\t\tself.rewards[agent, t] = reward\n",
    "\t\tself.logprobs[agent, t] = logprob\n",
    "\n",
    "\tdef mark_episode_end(self, agent, t):\n",
    "\t\tself.episode_ends[agent].append(t)\n",
    "\n",
    "\tdef compute_advantages_and_target_values(self, model):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor agent in range(self.n_agents):\n",
    "\t\t\t\t# Iterate from last to first\n",
    "\t\t\t\tfor t in range(self.n_timesteps-1, -1, -1):\n",
    "\t\t\t\t\t# Get step and reward and compute predicted value\n",
    "\t\t\t\t\tstate = self.states[agent, t]\n",
    "\t\t\t\t\treward = self.rewards[agent, t]\n",
    "\t\t\t\t\tpred_value = model(state)[-1]\n",
    "\n",
    "\t\t\t\t\t# If step is terminal\n",
    "\t\t\t\t\tif t in self.episode_ends[agent]:\n",
    "\t\t\t\t\t\ttarget_value = reward\n",
    "\t\t\t\t\t# If step is last but not terminal\n",
    "\t\t\t\t\telif t == self.n_timesteps - 1:\n",
    "\t\t\t\t\t\ttarget_value = pred_value\n",
    "\t\t\t\t\t# non-terminal non-last step\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Get previous target value (of t+1) and compute\n",
    "\t\t\t\t\t\tprevious_target_value = self.target_values[agent][t+1]\n",
    "\t\t\t\t\t\ttarget_value = reward + self.gamma * previous_target_value\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Compute advantage\n",
    "\t\t\t\t\tadvantage = target_value - pred_value\n",
    "\n",
    "\t\t\t\t\t# Store target value and advantage\n",
    "\t\t\t\t\tself.target_values[agent, t] = target_value\n",
    "\t\t\t\t\tself.advantages[agent, t] = advantage\n",
    "\n",
    "\tdef process_data(self):\n",
    "\t\t# Flatten all tensors but keep last dimension of states and lobprobs\n",
    "\t\tself.states = self.states.view(-1, self.n_observations)\n",
    "\t\tself.actions = self.actions.view(-1)\n",
    "\t\tself.rewards = self.rewards.view(-1)\n",
    "\t\tself.logprobs = self.logprobs.view(-1, self.n_actions)\n",
    "\t\tself.target_values = self.target_values.view(-1)\n",
    "\t\tself.advantages = self.advantages.view(-1)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n_agents * self.n_timesteps\n",
    "\t\n",
    "\tdef __getitem__(self, i):\n",
    "\t\t# Don't use before calling compute_advantages_and_target_values and flatten_data\n",
    "\t\tstate = self.states[i]\n",
    "\t\taction = self.actions[i]\n",
    "\t\tlogprob = self.logprobs[i]\n",
    "\t\ttarget_value = self.target_values[i]\n",
    "\t\tadvantage = self.advantages[i]\n",
    "\n",
    "\t\treturn state, action, logprob, target_value, advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 3-in-1 loss function. We want to minimize\n",
    "\n",
    "- -L_CLIP  + V * L_V - H * L_H\n",
    "\n",
    "I'll write each of the separate functions then combine them in one that will be backpropagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_objective_fn(log_probs, pi_batch, action_batch, advantage_batch):\n",
    "    # Compute pi_new\n",
    "    pi_new = log_probs[torch.arange(len(action_batch)), action_batch]\n",
    "\n",
    "    # Compute p_ratio, ratio of new and old probabilities\n",
    "    # note that it's unnormalized\n",
    "    p_ratio = torch.exp(pi_new - pi_batch)\n",
    "\n",
    "    # Compute unclipped and clipped surrogate objectives\n",
    "    unclipped_surrogate_objective = p_ratio * advantage_batch\n",
    "    clipped_surrogate_objective = torch.clamp(p_ratio, 1. - EPSILON, 1. + EPSILON) * advantage_batch\n",
    "\n",
    "    # Compute elementwise minimum of two and return\n",
    "    clipped_objective = torch.mean(torch.min(unclipped_surrogate_objective, clipped_surrogate_objective))\n",
    "    return clipped_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_bonus_fn(log_probs):\n",
    "    # Compute probabilities\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    # Compute and return entropy\n",
    "    entropy = -torch.mean(torch.sum(probs * log_probs, dim=-1))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_loss_fn(pred_values, target_value_batch):\n",
    "    # Compute and return loss\n",
    "    smoothl1 = nn.SmoothL1Loss(reduction='mean')\n",
    "    loss = smoothl1(pred_values, target_value_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, state_batch, action_batch, pi_batch, target_value_batch, advantage_batch):\n",
    "    # Get pred log probs and values\n",
    "    output = model(state_batch)\n",
    "    log_probs, pred_values = torch.log_softmax(output[:, :-1], dim=-1), output[:, -1]\n",
    "\n",
    "    # Compute individual losses\n",
    "    clipped_objective = clipped_objective_fn(log_probs, pi_batch, action_batch, advantage_batch)\n",
    "    value_loss = value_loss_fn(pred_values, target_value_batch)\n",
    "    entropy_bonus = entropy_bonus_fn(log_probs)\n",
    "\n",
    "    # Compute and return total loss\n",
    "    loss = -clipped_objective + V * value_loss - H * entropy_bonus\n",
    "    return loss, clipped_objective.item(), V * value_loss.item(), H * entropy_bonus.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not complete but almost the only thing left is to fix and add wrappers to the envs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, env, loss_fn, optimizer, batch_size, n_epochs, n_trainsteps, n_agents, n_timesteps, n_observations, n_actions, gamma):\n",
    "\tlosses = []\n",
    "\tavg_rewards = []\n",
    "\t\n",
    "\t# ADD CODE TO TRANSFORM ENVIRONMENT INTO PYTORCH WHATEVER AND MAKE COPIES\n",
    "\t# Initilize all environments\n",
    "\tenvs = [[env] for _ in range(n_agents)]\n",
    "\tfor agent_env in envs:\n",
    "\t\tstate, _ = agent_env[0].reset()\n",
    "\t\tterminated, truncated = False, False\n",
    "\t\tagent_env += [state, terminated, truncated]\n",
    "\t\t\t\n",
    "\tfor i in range(n_trainsteps):\n",
    "\t\t# Initialize dataset\n",
    "\t\tdataset = PPODataset(n_agents, n_timesteps, n_observations, n_actions, gamma)\n",
    "\n",
    "\t\t# Collect data\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor agent in range(n_agents):\n",
    "\t\t\t\tenv = envs[agent][0]\n",
    "\t\t\t\tstate, terminated, truncated = envs[agent][1:]\n",
    "\t\n",
    "\t\t\t\tfor t in range(n_timesteps):\n",
    "\t\t\t\t\t# Compute and split model output\n",
    "\t\t\t\t\toutput = model(state)\n",
    "\t\t\t\t\taction_logits = output[:-1]\n",
    "\n",
    "\t\t\t\t\t# Select and perform action\n",
    "\t\t\t\t\taction = select_action(action_logits)\n",
    "\t\t\t\t\tnext_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "\t\t\t\t\t# Compute pi\n",
    "\t\t\t\t\tlogprob = torch.log_softmax(action_logits, dim=-1)[action]\n",
    "\n",
    "\t\t\t\t\t# Store data\n",
    "\t\t\t\t\tdataset.add_step(agent, t, state, action, reward, logprob)\n",
    "\n",
    "\t\t\t\t\t# If terminated reset env and mark end, otherwise update state\n",
    "\t\t\t\t\tif terminated or truncated:\n",
    "\t\t\t\t\t\tdataset.mark_episode_end(agent, t)\n",
    "\t\t\t\t\t\tstate, _ = env.reset()\n",
    "\t\t\t\t\t\tterminated, truncated = False, False\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tstate = next_state\n",
    "\n",
    "\t\t# Compute target values and advantages and process data\n",
    "\t\tdataset.compute_advantages_and_target_values(model)\n",
    "\t\tdataset.process_data()\n",
    "\n",
    "\t\t# Create dataloader\n",
    "\t\tdataloader = DataLoader(\n",
    "\t\t\tdataset=dataset,\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t)\n",
    "\n",
    "\t\t# Train model on policy for N_EPOCHS\n",
    "\t\tfor _ in range(n_epochs):\n",
    "\t\t\tfor state_batch, action_batch, pi_batch, target_value_batch, advantage_batch in dataloader:\n",
    "\t\t\t\t# Compute loss and optimize\n",
    "\t\t\t\tloss, clipped_objective, value_loss, entropy_bonus = loss_fn(\n",
    "\t\t\t\t\tmodel,\n",
    "\t\t\t\t\tstate_batch,\n",
    "\t\t\t\t\taction_batch,\n",
    "\t\t\t\t\tpi_batch,\n",
    "\t\t\t\t\ttarget_value_batch,\n",
    "\t\t\t\t\tadvantage_batch\n",
    "\t\t\t\t)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\t# Clip gradients\n",
    "\t\t\t\tclip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\t# Append losses\n",
    "\t\t\t\tlosses.append(loss.item())\n",
    "\n",
    "\treturn losses, avg_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolicyAndValueNetwork()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot(data, title=\"\", xlabel=\"\", ylabel=\"\", grid=True, sleep=0.01):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(grid)\n",
    "    plt.show()\n",
    "    time.sleep(sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN:\n\u001b[1;32m----> 2\u001b[0m     losses, avg_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[100], line 55\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     51\u001b[0m avg_rewards\u001b[38;5;241m.\u001b[39mappend(avg_reward)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#update_plot(avg_rewards, title=\"Average Rewards\")\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Compute advantages and target values\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m target_values, advantages \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_advantages_and_target_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_ends\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_ends\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Add everything to dataset and create dataloader\u001b[39;00m\n\u001b[0;32m     63\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PPODataset(states, actions, pis, target_values, advantages)\n",
      "Cell \u001b[1;32mIn[90], line 12\u001b[0m, in \u001b[0;36mcompute_advantages_and_target_values\u001b[1;34m(model, states, rewards, episode_ends)\u001b[0m\n\u001b[0;32m     10\u001b[0m state \u001b[38;5;241m=\u001b[39m states[agent, \u001b[38;5;241m-\u001b[39mi\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     11\u001b[0m reward \u001b[38;5;241m=\u001b[39m rewards[agent, \u001b[38;5;241m-\u001b[39mi\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 12\u001b[0m predicted_value \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# If step is final\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m episode_ends[agent]:\n",
      "File \u001b[1;32mc:\\Users\\novae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\novae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[87], line 22\u001b[0m, in \u001b[0;36mPolicyAndValueNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m shared_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_layers(x)\n\u001b[0;32m     21\u001b[0m action_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_head(shared_output)\n\u001b[1;32m---> 22\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([action_logits, value], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\novae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\novae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\novae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\novae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\novae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\novae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    losses, avg_rewards = train_loop(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlosses\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(losses, label=\"losses\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\novae\\AppData\\Local\\Temp\\ipykernel_6488\\1507193917.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_model.load_state_dict(torch.load('best_lunar_lander.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.6601765585789\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,\n",
    "    gravity=-10.0,\n",
    "    enable_wind=False,\n",
    "    wind_power=15.0,\n",
    "    turbulence_power=1.5,\n",
    "    render_mode='human'\n",
    ")\n",
    "\n",
    "test_model = PolicyAndValueNetwork(N_OBSERVATIONS, N_ACTIONS, 128)\n",
    "test_model.load_state_dict(torch.load('best_lunar_lander.pth'))\n",
    "\n",
    "n = 1\n",
    "total_rewards = 0.\n",
    "for _ in range(n):\n",
    "    state, info = test_env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    while not (terminated or truncated):\n",
    "        tensor_state = torch.from_numpy(state)\n",
    "        logits = test_model(tensor_state)\n",
    "        action_logits = logits[:-1]\n",
    "        action = select_action(action_logits)\n",
    "        state, reward, terminated, truncated, info = test_env.step(action)\n",
    "        total_rewards += reward\n",
    "\n",
    "print(total_rewards / n)\n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
