{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "N_OBSERVATIONS = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "N_AGENTS = 2\n",
    "T = 50\n",
    "GAMMA = 0.999\n",
    "EPSILON = 0.2\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "TEST = True\n",
    "TRAIN = False\n",
    "\n",
    "layer_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and selecting actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the same parameters for the policy and value networks. If this doesn't work well I can change it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAndValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(N_OBSERVATIONS, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            # +1 comes from value prediction\n",
    "            nn.Linear(layer_dim, N_ACTIONS + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select action based on log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_logits):\n",
    "    action_probs = torch.softmax(action_logits, dim=0)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_env = gym.make('CartPole-v1', render_mode='human')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "\n",
    "    for _ in range(1):\n",
    "        state, info = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        while not (terminated or truncated):\n",
    "            tensor_state = torch.from_numpy(state)\n",
    "            logits = test_model(tensor_state)\n",
    "            action_logits = logits[:-1]\n",
    "            action = select_action(action_logits)\n",
    "            state, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first collect the data while marking episode ends when they happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector():\n",
    "    def __init__(self):\n",
    "        self.states = [[] for _ in range(N_AGENTS)]\n",
    "        self.actions = [[] for _ in range(N_AGENTS)]\n",
    "        self.rewards = [[] for _ in range(N_AGENTS)]\n",
    "        self.pis = [[] for _ in range(N_AGENTS)]\n",
    "        self.episode_ends = [[] for _ in range(N_AGENTS)]\n",
    "\n",
    "    def add_step(self, agent_id, state, action, reward, pi):\n",
    "        self.states[agent_id].append(state)\n",
    "        self.actions[agent_id].append(action)\n",
    "        self.rewards[agent_id].append(reward)\n",
    "        self.pis[agent_id].append(pi)\n",
    "\n",
    "    def mark_episode_end(self, agent_id, timestep):\n",
    "        self.episode_ends[agent_id].append(timestep)\n",
    "\n",
    "    def fetch_data(self):\n",
    "        list_of_agent_states = [torch.stack(agent_states) for agent_states in self.states]\n",
    "        states = torch.stack([agent_states for agent_states in list_of_agent_states])\n",
    "        actions = self.actions\n",
    "        rewards = torch.tensor(self.rewards)\n",
    "        pis = torch.tensor(self.pis)\n",
    "        episode_ends = self.episode_ends\n",
    "\n",
    "        return states, actions, rewards, pis, episode_ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute advantages and target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll need to compute advantages and target values. We'll start from the end of the data for each agent and move backwards. This makes the desired values easier to compute and to take into account episode ends.\n",
    "\n",
    "Gradients should be on here because I'll use the computations here to optimize the model.\n",
    "\n",
    "> If the trajectory terminated due to the maximal trajectory length T\n",
    "being reached, Vωold (st+n ) denotes the state value associated with state st+n as predicted by the state value\n",
    "network. Otherwise, Vωold (st+n ) is set to 0\n",
    "\n",
    "Instead of computing r_t + V(s_t+1) I'm using V(s_t). It's similar enough and shouldn't be that much of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages_and_target_values(model, states, rewards, episode_ends):\n",
    "    # We'll add them from last to first, then reverse list\n",
    "    reversed_target_values = [[] for _ in range(N_AGENTS)]\n",
    "    reversed_advantages = [[] for _ in range(N_AGENTS)]\n",
    "\n",
    "    for agent in range(N_AGENTS):\n",
    "        for i in range(T):\n",
    "            # Get step state and reward and compute predicted value\n",
    "            state = states[agent, -i-1]\n",
    "            reward = rewards[agent, -i-1]\n",
    "            predicted_value = model(state)[-1]\n",
    "\n",
    "            # If step is final\n",
    "            if i in episode_ends[agent]:\n",
    "                target_value = reward\n",
    "            # If step is last but not final\n",
    "            elif i == 0:\n",
    "                target_value = predicted_value\n",
    "            # non-last non-final step\n",
    "            else:\n",
    "                # Get previous target value and compute \n",
    "                previous_target_value = reversed_target_values[agent][-1]\n",
    "                target_value = reward + GAMMA * previous_target_value\n",
    "            advantage = target_value - predicted_value\n",
    "\n",
    "            # Append target value and advantage\n",
    "            reversed_target_values[agent].append(target_value)\n",
    "            reversed_advantages[agent].append(advantage)\n",
    "\n",
    "    target_values = [torch.stack(agent_target_values[::-1]) for agent_target_values in reversed_target_values]\n",
    "    advantages = [torch.stack(agent_advantage[::-1]) for agent_advantage in reversed_advantages]\n",
    "\n",
    "    return target_values, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to modify the data so that I can sample random batches from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPODataset(Dataset):\n",
    "    def __init__(self, states, actions, pis, target_values, advantages):\n",
    "        # Reshape all into (N_AGENTS * T, whatever (possibly 0))\n",
    "        self.states = states.reshape(N_AGENTS * T, -1)\n",
    "        self.actions = [action for agent in actions for action in agent]\n",
    "        self.pis = pis.reshape(N_AGENTS * T)\n",
    "        self.target_values = torch.cat(target_values)\n",
    "        self.advantages = torch.cat(advantages)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        state = self.states[i]\n",
    "        action = self.actions[i]\n",
    "        pi = self.pis[i]\n",
    "        target_value = self.target_values[i]\n",
    "        advantage = self.advantages[i]\n",
    "\n",
    "        return state, action, pi, target_value, advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_batch: tensor([[ 8.7621e-02,  8.1141e-01, -1.6617e-01, -1.3870e+00],\n",
      "        [-2.1354e-02, -7.1292e-03,  1.5443e-02, -1.5917e-02],\n",
      "        [-5.6898e-02, -9.7270e-01,  5.5225e-02,  1.4389e+00],\n",
      "        [-4.3018e-02,  3.6789e-01,  2.1321e-02, -5.9012e-01],\n",
      "        [ 9.6179e-03, -4.0111e-01,  2.3109e-02,  5.8503e-01],\n",
      "        [ 2.9481e-02, -1.7103e-01, -2.2468e-02,  3.0906e-01],\n",
      "        [ 2.9451e-02,  4.8258e-02, -3.7498e-03,  3.6285e-02],\n",
      "        [ 2.3571e-02, -1.1336e-02,  4.0302e-03,  9.9151e-03],\n",
      "        [-1.8227e-02,  3.6026e-03,  6.2505e-04, -4.1077e-02],\n",
      "        [-3.8638e-02, -7.3204e-01,  1.2175e-01,  1.2100e+00]])\n",
      "action_batch: tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0])\n",
      "pi_batch: tensor([0.0183, 0.0127, 0.0572, 0.0193, 0.0896, 0.0230, 0.0907, 0.0916, 0.0128,\n",
      "        0.0511])\n",
      "target_value_batch: tensor([ 2.9303,  1.0000,  5.9850,  4.9154,  4.9900, 11.9342,  1.0000,  2.9970,\n",
      "        10.9452, 16.8647], grad_fn=<StackBackward0>)\n",
      "advantage_batch: tensor([ 3.0034,  1.0646,  6.0971,  4.9830,  5.0771, 12.0078,  1.0643,  3.0625,\n",
      "        11.0094, 16.9697], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    # Initialize training data, test environment\n",
    "    test_data_collector = DataCollector()\n",
    "    test_env = gym.make('CartPole-v1')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "    \n",
    "    for agent in range(N_AGENTS):\n",
    "        state, _ = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        \n",
    "        for step in range(T):\n",
    "            # Compute and divide model output\n",
    "            with torch.no_grad():\n",
    "                state = torch.from_numpy(state)\n",
    "                output = test_model(state)\n",
    "            action_logits, value_logit = output[:-1], output[-1]\n",
    "\n",
    "            # Select and perform action\n",
    "            action = select_action(action_logits)\n",
    "            next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "            # Compute pi\n",
    "            pi = action_logits[action]\n",
    "\n",
    "            # Store data\n",
    "            test_data_collector.add_step(agent, state, action, reward, pi)\n",
    "\n",
    "            # If terminated reset env and mark end, otherwise update state\n",
    "            if terminated or truncated:\n",
    "                test_data_collector.mark_episode_end(agent, step)\n",
    "                state, _ = test_env.reset()\n",
    "                terminated, truncated = False, False\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    # Fetch data from data collector\n",
    "    states, actions, rewards, pis, episode_ends = test_data_collector.fetch_data()\n",
    "\n",
    "    # Compute advantages and target values\n",
    "    target_values, advantages = compute_advantages_and_target_values(\n",
    "        model=test_model,\n",
    "        states=states,\n",
    "        rewards=rewards,\n",
    "        episode_ends=episode_ends,\n",
    "    )\n",
    "\n",
    "    # Add everything to dataset and create dataloader\n",
    "    test_dataset = PPODataset(states, actions, pis, target_values, advantages)\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    state_batch, action_batch, pi_batch, target_value_batch, advantage_batch = next(iter(test_dataloader))\n",
    "    print(f\"state_batch: {state_batch}\")\n",
    "print(f\"action_batch: {action_batch}\")\n",
    "print(f\"pi_batch: {pi_batch}\")\n",
    "print(f\"target_value_batch: {target_value_batch}\")\n",
    "print(f\"advantage_batch: {advantage_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's all working, fuck yeah. Now it's time to write the training code. By the way, I tested the code incrementally, not all at once, but now that it's all working I left only this big chunk of code, since the rest was mostly building blocks for this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
