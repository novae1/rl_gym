{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBSERVATIONS = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_AGENTS = 10\n",
    "T = 100\n",
    "\n",
    "TEST = True\n",
    "TRAIN = False\n",
    "\n",
    "layer_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, action and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the same parameters for the policy and value networks. If this doesn't work well I can change it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAndValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(N_OBSERVATIONS, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            # +1 comes from value prediction\n",
    "            nn.Linear(layer_dim, N_ACTIONS + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select action based on log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_logits):\n",
    "    action_probs = torch.softmax(action_logits, dim=0)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_env = gym.make('CartPole-v1', render_mode='human')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "\n",
    "    for _ in range(1):\n",
    "        state, info = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        while not (terminated or truncated):\n",
    "            tensor_state = torch.from_numpy(state)\n",
    "            logits = test_model(tensor_state)\n",
    "            action_logits = logits[:-1]\n",
    "            action = select_action(action_logits)\n",
    "            state, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to decide whether the best format for the training data is one list of tuples or many different lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'pi', 'advantage', 'target_value'])\n",
    "\n",
    "class TrainingData():\n",
    "    def __init__(self):\n",
    "        # List of Transitions\n",
    "        self.data = []\n",
    "        # Current batch index\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def push(self, state, action, reward, pi):\n",
    "        # Initially we don't have the advantage and target value\n",
    "        # they'll be computed during training\n",
    "        advantage, target_value = 0., 0.\n",
    "        self.data.append(Transition(state, action, reward, pi, advantage, target_value))\n",
    "\n",
    "    def push_A_and_V(self, idx, advantage, target_value):\n",
    "        # It makes sense to add the advantage and target value index by index\n",
    "        state, action, reward, pi, _, _ = self.data[idx]\n",
    "        complete_transition = Transition(state, action, reward, pi, advantage, target_value)\n",
    "        self.data[idx] = complete_transition\n",
    "\n",
    "    def randomize_order(self):\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "    def fetch_batch(self, batch_size):\n",
    "        i = self.batch_index\n",
    "        batch = self.data[i:i+batch_size]\n",
    "        \n",
    "        # Update index\n",
    "        self.batch_index += batch_size\n",
    "        if i+batch_size >= self.__len__():\n",
    "            # Reset index\n",
    "            self.batch_index = 0\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    def unpack(self, batch=None):\n",
    "        # batch should be list of Transitions or None\n",
    "        # in which case we unpack all of data\n",
    "        if not batch:\n",
    "            batch = self.data\n",
    "            \n",
    "        states = torch.stack([t.state for t in batch])\n",
    "        actions = torch.tensor([t.action for t in batch])\n",
    "        rewards = torch.tensor([t.reward for t in batch])\n",
    "        pis = torch.stack([t.pi for t in batch])\n",
    "        advantages = torch.tensor([t.advantage for t in batch])\n",
    "        target_values = torch.tensor([t.target_value for t in batch])\n",
    "\n",
    "        return states, actions, rewards, pis, advantages, target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: tensor([[-1.0995e-02, -9.2247e-03, -4.9037e-02,  3.5234e-02],\n",
      "        [-1.1180e-02,  1.8656e-01, -4.8333e-02, -2.7251e-01],\n",
      "        [-7.4484e-03, -7.8352e-03, -5.3783e-02,  4.5469e-03],\n",
      "        [-7.6051e-03, -2.0215e-01, -5.3692e-02,  2.7979e-01],\n",
      "        [-1.1648e-02, -6.3011e-03, -4.8096e-02, -2.9335e-02],\n",
      "        [-1.1774e-02,  1.8948e-01, -4.8683e-02, -3.3680e-01],\n",
      "        [-7.9845e-03,  3.8526e-01, -5.5419e-02, -6.4442e-01],\n",
      "        [-2.7937e-04,  1.9095e-01, -6.8307e-02, -3.6970e-01],\n",
      "        [ 3.5396e-03,  3.8697e-01, -7.5701e-02, -6.8311e-01],\n",
      "        [ 1.1279e-02,  5.8306e-01, -8.9363e-02, -9.9863e-01],\n",
      "        [ 2.2940e-02,  3.8924e-01, -1.0934e-01, -7.3530e-01],\n",
      "        [ 3.0725e-02,  5.8569e-01, -1.2404e-01, -1.0603e+00],\n",
      "        [ 4.2439e-02,  7.8221e-01, -1.4525e-01, -1.3892e+00],\n",
      "        [ 5.8083e-02,  9.7881e-01, -1.7303e-01, -1.7235e+00],\n",
      "        [ 7.7659e-02,  7.8604e-01, -2.0750e-01, -1.4893e+00]])\n",
      "action: tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0])\n",
      "reward: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "pi: tensor([[0.0145, 0.0737],\n",
      "        [0.0160, 0.0656],\n",
      "        [0.0149, 0.0725],\n",
      "        [0.0104, 0.0826],\n",
      "        [0.0150, 0.0713],\n",
      "        [0.0151, 0.0631],\n",
      "        [0.0165, 0.0541],\n",
      "        [0.0154, 0.0613],\n",
      "        [0.0172, 0.0521],\n",
      "        [0.0203, 0.0439],\n",
      "        [0.0180, 0.0493],\n",
      "        [0.0222, 0.0406],\n",
      "        [0.0276, 0.0327],\n",
      "        [0.0316, 0.0248],\n",
      "        [0.0302, 0.0288]])\n",
      "0\n",
      "[Transition(state=tensor([-0.0110, -0.0092, -0.0490,  0.0352]), action=1, reward=1.0, pi=tensor([0.0145, 0.0737]), advantage=0.0, target_value=0.0)]\n",
      "1\n",
      "[Transition(state=tensor([-0.0112,  0.1866, -0.0483, -0.2725]), action=0, reward=1.0, pi=tensor([0.0160, 0.0656]), advantage=0.0, target_value=0.0)]\n",
      "2\n",
      "[Transition(state=tensor([-0.0074, -0.0078, -0.0538,  0.0045]), action=0, reward=1.0, pi=tensor([0.0149, 0.0725]), advantage=0.0, target_value=0.0)]\n",
      "3\n",
      "[Transition(state=tensor([-0.0076, -0.2021, -0.0537,  0.2798]), action=1, reward=1.0, pi=tensor([0.0104, 0.0826]), advantage=0.0, target_value=0.0)]\n",
      "4\n",
      "[Transition(state=tensor([-0.0116, -0.0063, -0.0481, -0.0293]), action=1, reward=1.0, pi=tensor([0.0150, 0.0713]), advantage=0.0, target_value=0.0)]\n",
      "5\n",
      "[Transition(state=tensor([-0.0118,  0.1895, -0.0487, -0.3368]), action=1, reward=1.0, pi=tensor([0.0151, 0.0631]), advantage=0.0, target_value=0.0)]\n",
      "6\n",
      "[Transition(state=tensor([-0.0080,  0.3853, -0.0554, -0.6444]), action=0, reward=1.0, pi=tensor([0.0165, 0.0541]), advantage=0.0, target_value=0.0)]\n",
      "7\n",
      "[Transition(state=tensor([-2.7937e-04,  1.9095e-01, -6.8307e-02, -3.6970e-01]), action=1, reward=1.0, pi=tensor([0.0154, 0.0613]), advantage=0.0, target_value=0.0)]\n",
      "8\n",
      "[Transition(state=tensor([ 0.0035,  0.3870, -0.0757, -0.6831]), action=1, reward=1.0, pi=tensor([0.0172, 0.0521]), advantage=0.0, target_value=0.0)]\n",
      "9\n",
      "[Transition(state=tensor([ 0.0113,  0.5831, -0.0894, -0.9986]), action=0, reward=1.0, pi=tensor([0.0203, 0.0439]), advantage=0.0, target_value=0.0)]\n",
      "10\n",
      "[Transition(state=tensor([ 0.0229,  0.3892, -0.1093, -0.7353]), action=1, reward=1.0, pi=tensor([0.0180, 0.0493]), advantage=0.0, target_value=0.0)]\n",
      "11\n",
      "[Transition(state=tensor([ 0.0307,  0.5857, -0.1240, -1.0603]), action=1, reward=1.0, pi=tensor([0.0222, 0.0406]), advantage=0.0, target_value=0.0)]\n",
      "12\n",
      "[Transition(state=tensor([ 0.0424,  0.7822, -0.1452, -1.3892]), action=1, reward=1.0, pi=tensor([0.0276, 0.0327]), advantage=0.0, target_value=0.0)]\n",
      "13\n",
      "[Transition(state=tensor([ 0.0581,  0.9788, -0.1730, -1.7235]), action=0, reward=1.0, pi=tensor([0.0316, 0.0248]), advantage=0.0, target_value=0.0)]\n",
      "14\n",
      "[Transition(state=tensor([ 0.0777,  0.7860, -0.2075, -1.4893]), action=0, reward=1.0, pi=tensor([0.0302, 0.0288]), advantage=0.0, target_value=0.0)]\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    # Initialize training data, test environment\n",
    "    test_training_data = TrainingData()\n",
    "    test_env = gym.make('CartPole-v1')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "    \n",
    "    state, _ = test_env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    \n",
    "    while not (terminated or truncated):\n",
    "        # Compute and divide model output\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state)\n",
    "            output = test_model(state)\n",
    "        action_logits, value_logit = output[:-1], output[-1]\n",
    "\n",
    "        # Select and perform action\n",
    "        action = select_action(action_logits)\n",
    "        next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "        # Compute pi\n",
    "        pi = action_logits\n",
    "\n",
    "        # Store data\n",
    "        test_training_data.push(state, action, reward, pi)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "    # Unpack training data\n",
    "    state, action, reward, pi, _, _ = test_training_data.unpack()\n",
    "    print(f\"state: {state}\")\n",
    "    print(f\"action: {action}\")\n",
    "    print(f\"reward: {reward}\")\n",
    "    print(f\"pi: {pi}\")\n",
    "\n",
    "    # Compute advantage and target value for each transition\n",
    "    for i in range(len(test_training_data)):\n",
    "        print(test_training_data.batch_index)\n",
    "        i_data = test_training_data.fetch_batch(1)\n",
    "        print(i_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
