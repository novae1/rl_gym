{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TransformObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_env(env):\n",
    "    env = TransformObservation(env, lambda x: torch.from_numpy(x), env.observation_space)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pytorch_env(gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,\n",
    "    gravity=-10.0,\n",
    "    enable_wind=False,\n",
    "    wind_power=15.0,\n",
    "    turbulence_power=1.5,\n",
    "))\n",
    "\n",
    "N_OBSERVATIONS = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "\n",
    "TEST = False\n",
    "TRAIN = False\n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_agents = 8\n",
    "n_timesteps = 512\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "h = 0.01 # Entropy constant\n",
    "v = 1.0 # Value constant\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "n_epochs = 4\n",
    "n_trainsteps = 200\n",
    "max_grad_norm = 0.5\n",
    "layer_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAndValueNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, layer_dim):\n",
    "        super().__init__()\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(n_observations, layer_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, n_actions)\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_output = self.shared_layers(x)\n",
    "        action_logits = self.policy_head(shared_output)\n",
    "        value = self.value_head(shared_output)\n",
    "        return torch.cat([action_logits, value], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select action based on action logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_logits):\n",
    "    action_probs = torch.softmax(action_logits, dim=0)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPODataset(Dataset):\n",
    "\tdef __init__(self, n_agents, n_timesteps, n_observations, n_actions, gamma):\n",
    "\t\t# Store variables\n",
    "\t\tself.n_agents = n_agents\n",
    "\t\tself.n_timesteps = n_timesteps\n",
    "\t\tself.n_observations = n_observations\n",
    "\t\tself.n_actions = n_actions\n",
    "\t\tself.gamma = gamma\n",
    "\n",
    "\t\t# Create tensors\n",
    "\t\tself.states = torch.zeros((n_agents, n_timesteps, n_observations))\n",
    "\t\tself.actions = torch.zeros((n_agents, n_timesteps))\n",
    "\t\tself.rewards = torch.zeros((n_agents, n_timesteps))\n",
    "\t\tself.logprobs = torch.zeros((n_agents, n_timesteps, n_actions))\n",
    "\t\tself.target_values = torch.zeros((n_agents, n_timesteps))\n",
    "\t\tself.advantages = torch.zeros((n_agents, n_timesteps))\n",
    "\n",
    "\t\t# Store episode ends\n",
    "\t\tself.episode_ends = [[] for _ in range(n_agents)]\n",
    "\n",
    "\tdef add_step(self, agent, t, state, action, reward, logprob):\n",
    "\t\tself.states[agent, t] = state\n",
    "\t\tself.actions[agent, t] = action\n",
    "\t\tself.rewards[agent, t] = reward\n",
    "\t\tself.logprobs[agent, t] = logprob\n",
    "\n",
    "\tdef mark_episode_end(self, agent, t):\n",
    "\t\tself.episode_ends[agent].append(t)\n",
    "\n",
    "\tdef compute_advantages_and_target_values(self, model):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor agent in range(self.n_agents):\n",
    "\t\t\t\t# Iterate from last to first\n",
    "\t\t\t\tfor t in range(self.n_timesteps-1, -1, -1):\n",
    "\t\t\t\t\t# Get step and reward and compute predicted value\n",
    "\t\t\t\t\tstate = self.states[agent, t]\n",
    "\t\t\t\t\treward = self.rewards[agent, t]\n",
    "\t\t\t\t\tpred_value = model(state)[-1]\n",
    "\n",
    "\t\t\t\t\t# If step is terminal\n",
    "\t\t\t\t\tif t in self.episode_ends[agent]:\n",
    "\t\t\t\t\t\ttarget_value = reward\n",
    "\t\t\t\t\t# If step is last but not terminal\n",
    "\t\t\t\t\telif t == self.n_timesteps - 1:\n",
    "\t\t\t\t\t\ttarget_value = pred_value\n",
    "\t\t\t\t\t# non-terminal non-last step\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Get previous target value (of t+1) and compute\n",
    "\t\t\t\t\t\tprevious_target_value = self.target_values[agent][t+1]\n",
    "\t\t\t\t\t\ttarget_value = reward + self.gamma * previous_target_value\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# Compute advantage\n",
    "\t\t\t\t\tadvantage = target_value - pred_value\n",
    "\n",
    "\t\t\t\t\t# Store target value and advantage\n",
    "\t\t\t\t\tself.target_values[agent, t] = target_value\n",
    "\t\t\t\t\tself.advantages[agent, t] = advantage\n",
    "\n",
    "\tdef process_data(self):\n",
    "\t\t# Flatten all tensors but keep last dimension of states and lobprobs\n",
    "\t\tself.states = self.states.view(-1, self.n_observations)\n",
    "\t\tself.actions = self.actions.view(-1)\n",
    "\t\tself.rewards = self.rewards.view(-1)\n",
    "\t\tself.logprobs = self.logprobs.view(-1, self.n_actions)\n",
    "\t\tself.target_values = self.target_values.view(-1)\n",
    "\t\tself.advantages = self.advantages.view(-1)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n_agents * self.n_timesteps\n",
    "\t\n",
    "\tdef __getitem__(self, i):\n",
    "\t\t# Don't use before calling compute_advantages_and_target_values and flatten_data\n",
    "\t\tstate = self.states[i]\n",
    "\t\taction = self.actions[i]\n",
    "\t\tlogprob = self.logprobs[i]\n",
    "\t\ttarget_value = self.target_values[i]\n",
    "\t\tadvantage = self.advantages[i]\n",
    "\n",
    "\t\treturn state, action, logprob, target_value, advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 3-in-1 loss function. We want to minimize\n",
    "\n",
    "- -L_CLIP  + V * L_V - H * L_H\n",
    "\n",
    "I'll write each of the separate functions then combine them in one that will be backpropagated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should get the names here right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_objective_fn(pred_logprob, logprob_batch, action_batch, advantage_batch, epsilon):\n",
    "    # Compute pi_new\n",
    "    pred_logprob_batch = pred_logprob[torch.arange(len(action_batch)), action_batch]\n",
    "\n",
    "    # Compute p_ratio, ratio of new and old probabilities\n",
    "    # note that it's unnormalized\n",
    "    p_ratio = torch.exp(pred_logprob_batch - logprob_batch)\n",
    "\n",
    "    # Compute unclipped and clipped surrogate objectives\n",
    "    unclipped_surrogate_objective = p_ratio * advantage_batch\n",
    "    clipped_surrogate_objective = torch.clamp(p_ratio, 1. - epsilon, 1. + epsilon) * advantage_batch\n",
    "\n",
    "    # Compute elementwise minimum of two and return\n",
    "    clipped_objective = torch.mean(torch.min(unclipped_surrogate_objective, clipped_surrogate_objective))\n",
    "    return clipped_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_bonus_fn(log_probs):\n",
    "    # Compute probabilities\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    # Compute and return entropy\n",
    "    entropy = -torch.mean(torch.sum(probs * log_probs, dim=-1))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_loss_fn(pred_values, target_value_batch):\n",
    "    # Compute and return loss\n",
    "    smoothl1 = nn.SmoothL1Loss(reduction='mean')\n",
    "    loss = smoothl1(pred_values, target_value_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, state_batch, action_batch, logprob_batch, target_value_batch, advantage_batch, v, h):\n",
    "    # Get pred log probs and values\n",
    "    output = model(state_batch)\n",
    "    pred_logprob_batch, pred_value_batch = torch.log_softmax(output[:, :-1], dim=-1), output[:, -1]\n",
    "\n",
    "    # Compute individual losses\n",
    "    clipped_objective = clipped_objective_fn(pred_logprob_batch, logprob_batch, action_batch, advantage_batch)\n",
    "    value_loss = value_loss_fn(pred_value_batch, target_value_batch)\n",
    "    entropy_bonus = entropy_bonus_fn(pred_logprob_batch)\n",
    "\n",
    "    # Compute and return total loss\n",
    "    loss = -clipped_objective + v * value_loss - h * entropy_bonus\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not complete but almost the only thing left is to fix and add wrappers to the envs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, env, loss_fn, optimizer, batch_size, n_epochs, n_trainsteps, n_agents, n_timesteps, n_observations, n_actions, gamma):\n",
    "\tlosses = []\n",
    "\tavg_rewards = []\n",
    "\t\n",
    "\t# ADD CODE TO TRANSFORM ENVIRONMENT INTO PYTORCH WHATEVER AND MAKE COPIES\n",
    "\tenv = TransformObservation(env, lambda x: torch.from_numpy(x), env.observation_space)\n",
    "\t# Initilize all environments\n",
    "\tenvs = [[env] for _ in range(n_agents)]\n",
    "\tfor agent_env in envs:\n",
    "\t\tstate, _ = agent_env[0].reset()\n",
    "\t\tterminated, truncated = False, False\n",
    "\t\tagent_env += [state, terminated, truncated]\n",
    "\t\t\t\n",
    "\tfor i in range(n_trainsteps):\n",
    "\t\t# Initialize dataset\n",
    "\t\tdataset = PPODataset(n_agents, n_timesteps, n_observations, n_actions, gamma)\n",
    "\n",
    "\t\t# Collect data\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor agent in range(n_agents):\n",
    "\t\t\t\tenv = envs[agent][0]\n",
    "\t\t\t\tstate, terminated, truncated = envs[agent][1:]\n",
    "\t\n",
    "\t\t\t\tfor t in range(n_timesteps):\n",
    "\t\t\t\t\t# Compute and split model output\n",
    "\t\t\t\t\toutput = model(state)\n",
    "\t\t\t\t\taction_logits = output[:-1]\n",
    "\n",
    "\t\t\t\t\t# Select and perform action\n",
    "\t\t\t\t\taction = select_action(action_logits)\n",
    "\t\t\t\t\tnext_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "\t\t\t\t\t# Compute pi\n",
    "\t\t\t\t\tlogprob = torch.log_softmax(action_logits, dim=-1)[action]\n",
    "\n",
    "\t\t\t\t\t# Store data\n",
    "\t\t\t\t\tdataset.add_step(agent, t, state, action, reward, logprob)\n",
    "\n",
    "\t\t\t\t\t# If terminated reset env and mark end, otherwise update state\n",
    "\t\t\t\t\tif terminated or truncated:\n",
    "\t\t\t\t\t\tdataset.mark_episode_end(agent, t)\n",
    "\t\t\t\t\t\tstate, _ = env.reset()\n",
    "\t\t\t\t\t\tterminated, truncated = False, False\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tstate = next_state\n",
    "\n",
    "\t\t# Compute target values and advantages and process data\n",
    "\t\tdataset.compute_advantages_and_target_values(model)\n",
    "\t\tdataset.process_data()\n",
    "\n",
    "\t\t# Create dataloader\n",
    "\t\tdataloader = DataLoader(\n",
    "\t\t\tdataset=dataset,\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t)\n",
    "\n",
    "\t\t# Train model on policy for N_EPOCHS\n",
    "\t\tfor _ in range(n_epochs):\n",
    "\t\t\tfor state_batch, action_batch, pi_batch, target_value_batch, advantage_batch in dataloader:\n",
    "\t\t\t\t# Compute loss and optimize\n",
    "\t\t\t\tloss, clipped_objective, value_loss, entropy_bonus = loss_fn(\n",
    "\t\t\t\t\tmodel,\n",
    "\t\t\t\t\tstate_batch,\n",
    "\t\t\t\t\taction_batch,\n",
    "\t\t\t\t\tpi_batch,\n",
    "\t\t\t\t\ttarget_value_batch,\n",
    "\t\t\t\t\tadvantage_batch\n",
    "\t\t\t\t)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\t# Clip gradients\n",
    "\t\t\t\tclip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\t# Append losses\n",
    "\t\t\t\tlosses.append(loss.item())\n",
    "\n",
    "\treturn losses, avg_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolicyAndValueNetwork()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot(data, title=\"\", xlabel=\"\", ylabel=\"\", grid=True, sleep=0.01):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(grid)\n",
    "    plt.show()\n",
    "    time.sleep(sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mTRAIN\u001b[49m:\n\u001b[0;32m      2\u001b[0m     losses, avg_rewards \u001b[38;5;241m=\u001b[39m train_loop(model, loss_fn, optimizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TRAIN' is not defined"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    losses, avg_rewards = train_loop(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlosses\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(losses, label=\"losses\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\novae\\AppData\\Local\\Temp\\ipykernel_6488\\1507193917.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_model.load_state_dict(torch.load('best_lunar_lander.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.6601765585789\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\n",
    "    \"LunarLander-v3\",\n",
    "    continuous=False,\n",
    "    gravity=-10.0,\n",
    "    enable_wind=False,\n",
    "    wind_power=15.0,\n",
    "    turbulence_power=1.5,\n",
    "    render_mode='human'\n",
    ")\n",
    "\n",
    "test_model = PolicyAndValueNetwork(N_OBSERVATIONS, N_ACTIONS, 128)\n",
    "test_model.load_state_dict(torch.load('best_lunar_lander.pth'))\n",
    "\n",
    "n = 1\n",
    "total_rewards = 0.\n",
    "for _ in range(n):\n",
    "    state, info = test_env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    while not (terminated or truncated):\n",
    "        tensor_state = torch.from_numpy(state)\n",
    "        logits = test_model(tensor_state)\n",
    "        action_logits = logits[:-1]\n",
    "        action = select_action(action_logits)\n",
    "        state, reward, terminated, truncated, info = test_env.step(action)\n",
    "        total_rewards += reward\n",
    "\n",
    "print(total_rewards / n)\n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
