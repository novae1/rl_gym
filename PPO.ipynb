{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "N_OBSERVATIONS = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "N_AGENTS = 10\n",
    "T = 100\n",
    "GAMMA = 0.999\n",
    "EPSILON = 0.2\n",
    "H = 0.1 # Entropy constant\n",
    "V = 0.1 # Value constant\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCHS = 10\n",
    "N_TRAIN_LOOPS = 100\n",
    "\n",
    "TEST = False\n",
    "TRAIN = True\n",
    "\n",
    "layer_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and selecting actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the same parameters for the policy and value networks. If this doesn't work well I can change it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAndValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(N_OBSERVATIONS, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            # +1 comes from value prediction\n",
    "            nn.Linear(layer_dim, N_ACTIONS + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select action based on log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_logits):\n",
    "    action_probs = torch.softmax(action_logits, dim=0)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_env = gym.make('CartPole-v1', render_mode='human')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "\n",
    "    for _ in range(1):\n",
    "        state, info = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        while not (terminated or truncated):\n",
    "            tensor_state = torch.from_numpy(state)\n",
    "            logits = test_model(tensor_state)\n",
    "            action_logits = logits[:-1]\n",
    "            action = select_action(action_logits)\n",
    "            state, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first collect the data while marking episode ends when they happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector():\n",
    "    def __init__(self):\n",
    "        self.states = [[] for _ in range(N_AGENTS)]\n",
    "        self.actions = [[] for _ in range(N_AGENTS)]\n",
    "        self.rewards = [[] for _ in range(N_AGENTS)]\n",
    "        self.pis = [[] for _ in range(N_AGENTS)]\n",
    "        self.episode_ends = [[] for _ in range(N_AGENTS)]\n",
    "\n",
    "    def add_step(self, agent_id, state, action, reward, pi):\n",
    "        self.states[agent_id].append(state)\n",
    "        self.actions[agent_id].append(action)\n",
    "        self.rewards[agent_id].append(reward)\n",
    "        self.pis[agent_id].append(pi)\n",
    "\n",
    "    def mark_episode_end(self, agent_id, timestep):\n",
    "        self.episode_ends[agent_id].append(timestep)\n",
    "\n",
    "    def fetch_data(self):\n",
    "        list_of_agent_states = [torch.stack(agent_states) for agent_states in self.states]\n",
    "        states = torch.stack([agent_states for agent_states in list_of_agent_states])\n",
    "        actions = self.actions\n",
    "        rewards = torch.tensor(self.rewards)\n",
    "        pis = torch.tensor(self.pis)\n",
    "        episode_ends = self.episode_ends\n",
    "\n",
    "        return states, actions, rewards, pis, episode_ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute advantages and target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll need to compute advantages and target values. We'll start from the end of the data for each agent and move backwards. This makes the desired values easier to compute and to take into account episode ends.\n",
    "\n",
    "Gradients should be on here because I'll use the computations here to optimize the model.\n",
    "\n",
    "> If the trajectory terminated due to the maximal trajectory length T\n",
    "being reached, Vωold (st+n ) denotes the state value associated with state st+n as predicted by the state value\n",
    "network. Otherwise, Vωold (st+n ) is set to 0\n",
    "\n",
    "Instead of computing r_t + V(s_t+1) I'm using V(s_t). It's similar enough and shouldn't be that much of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages_and_target_values(model, states, rewards, episode_ends):\n",
    "    # We'll add them from last to first, then reverse list\n",
    "    reversed_target_values = [[] for _ in range(N_AGENTS)]\n",
    "    reversed_advantages = [[] for _ in range(N_AGENTS)]\n",
    "\n",
    "    for agent in range(N_AGENTS):\n",
    "        for i in range(T):\n",
    "            # Get step state and reward and compute predicted value\n",
    "            state = states[agent, -i-1]\n",
    "            reward = rewards[agent, -i-1]\n",
    "            predicted_value = model(state)[-1]\n",
    "\n",
    "            # If step is final\n",
    "            if i in episode_ends[agent]:\n",
    "                target_value = reward\n",
    "            # If step is last but not final\n",
    "            elif i == 0:\n",
    "                target_value = predicted_value\n",
    "            # non-last non-final step\n",
    "            else:\n",
    "                # Get previous target value and compute \n",
    "                previous_target_value = reversed_target_values[agent][-1]\n",
    "                target_value = reward + GAMMA * previous_target_value\n",
    "            advantage = target_value - predicted_value\n",
    "\n",
    "            # Append target value and advantage\n",
    "            reversed_target_values[agent].append(target_value)\n",
    "            reversed_advantages[agent].append(advantage)\n",
    "\n",
    "    target_values = [torch.stack(agent_target_values[::-1]) for agent_target_values in reversed_target_values]\n",
    "    advantages = [torch.stack(agent_advantage[::-1]) for agent_advantage in reversed_advantages]\n",
    "\n",
    "    return target_values, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to modify the data so that I can sample random batches from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPODataset(Dataset):\n",
    "    def __init__(self, states, actions, pis, target_values, advantages):\n",
    "        # Reshape all into (N_AGENTS * T, whatever (possibly 0))\n",
    "        self.states = states.reshape(N_AGENTS * T, -1)\n",
    "        self.actions = [action for agent in actions for action in agent]\n",
    "        self.pis = pis.reshape(N_AGENTS * T)\n",
    "        self.target_values = torch.cat(target_values)\n",
    "        self.advantages = torch.cat(advantages)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        state = self.states[i]\n",
    "        action = self.actions[i]\n",
    "        pi = self.pis[i]\n",
    "        target_value = self.target_values[i]\n",
    "        advantage = self.advantages[i]\n",
    "\n",
    "        return state, action, pi, target_value, advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    # Initialize training data, test environment\n",
    "    test_data_collector = DataCollector()\n",
    "    test_env = gym.make('CartPole-v1')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "    \n",
    "    for agent in range(N_AGENTS):\n",
    "        state, _ = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        \n",
    "        for step in range(T):\n",
    "            # Compute and divide model output\n",
    "            with torch.no_grad():\n",
    "                state = torch.from_numpy(state)\n",
    "                output = test_model(state)\n",
    "            action_logits, value_logit = output[:-1], output[-1]\n",
    "\n",
    "            # Select and perform action\n",
    "            action = select_action(action_logits)\n",
    "            next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "            # Compute pi\n",
    "            print(f\"pi: {action_logits[action]}\")\n",
    "            pi = torch.log_softmax(action_logits, dim=-1)[action]\n",
    "            print(f\"new pi: {pi}\")\n",
    "\n",
    "            # Store data\n",
    "            test_data_collector.add_step(agent, state, action, reward, pi)\n",
    "\n",
    "            # If terminated reset env and mark end, otherwise update state\n",
    "            if terminated or truncated:\n",
    "                test_data_collector.mark_episode_end(agent, step)\n",
    "                state, _ = test_env.reset()\n",
    "                terminated, truncated = False, False\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    # Fetch data from data collector\n",
    "    states, actions, rewards, pis, episode_ends = test_data_collector.fetch_data()\n",
    "\n",
    "    # Compute advantages and target values\n",
    "    target_values, advantages = compute_advantages_and_target_values(\n",
    "        model=test_model,\n",
    "        states=states,\n",
    "        rewards=rewards,\n",
    "        episode_ends=episode_ends,\n",
    "    )\n",
    "\n",
    "    # Add everything to dataset and create dataloader\n",
    "    test_dataset = PPODataset(states, actions, pis, target_values, advantages)\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    state_batch, action_batch, pi_batch, target_value_batch, advantage_batch = next(iter(test_dataloader))\n",
    "    print(f\"state_batch: {state_batch}\")\n",
    "    print(f\"action_batch: {action_batch}\")\n",
    "    print(f\"pi_batch: {pi_batch}\")\n",
    "    print(f\"target_value_batch: {target_value_batch}\")\n",
    "    print(f\"advantage_batch: {advantage_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's all working, fuck yeah. Now it's time to write the training code. By the way, I tested the code incrementally, not all at once, but now that it's all working I left only this big chunk of code, since the rest was mostly building blocks for this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 3-in-1 loss function. We want to minimize\n",
    "\n",
    "- -L_CLIP  + V * L_V - H * L_H\n",
    "\n",
    "I'll write each of the separate functions then combine them in one that will be backpropagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_objective_fn(log_probs, pi_batch, action_batch, advantage_batch):\n",
    "    # Compute pi_new\n",
    "    pi_new = log_probs[torch.arange(len(action_batch)), action_batch]\n",
    "\n",
    "    # Compute p_ratio, ratio of new and old probabilities\n",
    "    # note that it's unnormalized\n",
    "    p_ratio = torch.exp(pi_new - pi_batch)\n",
    "\n",
    "    # Compute unclipped and clipped surrogate objectives\n",
    "    unclipped_surrogate_objective = p_ratio * advantage_batch\n",
    "    clipped_surrogate_objective = torch.clamp(p_ratio, 1. - EPSILON, 1. + EPSILON) * advantage_batch\n",
    "\n",
    "    # Compute elementwise minimum of two and return\n",
    "    clipped_objective = torch.mean(torch.min(unclipped_surrogate_objective, clipped_surrogate_objective))\n",
    "    return clipped_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: I'm not calculating the actual probabilities. For that I'd have to compute softmax in each and then divide. The problem is that the sum of new and old exponentials might differ, and thus doing that I'm doing leads to a different value.\n",
    "\n",
    "Claude said this shouldn't be an issue but it might be we'll see if the model learns or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_bonus_fn(log_probs):\n",
    "    print(\"ola\", log_probs.shape)\n",
    "    # Compute probabilities\n",
    "    probs = torch.softmax(log_probs, dim=-1)\n",
    "\n",
    "    # Compute and return entropy\n",
    "    entropy = -torch.mean(torch.sum(probs * log_probs, dim=-1))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_loss_fn(pred_values, target_value_batch):\n",
    "    # Compute and return loss\n",
    "    mseloss = nn.MSELoss(reduction='mean')\n",
    "    loss = mseloss(pred_values, target_value_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, state_batch, action_batch, pi_batch, target_value_batch, advantage_batch):\n",
    "    # Get pred log probs and values\n",
    "    output = model(state_batch)\n",
    "    log_probs, pred_values = torch.log_softmax(output[:, :-1], dim=-1), output[:, -1]\n",
    "\n",
    "    # Compute individual losses\n",
    "    clipped_objective = clipped_objective_fn(log_probs, pi_batch, action_batch, advantage_batch)\n",
    "    value_loss = value_loss_fn(pred_values, target_value_batch)\n",
    "    entropy_bonus = entropy_bonus_fn(log_probs)\n",
    "    print(f\"clipped: {clipped_objective}\")\n",
    "    print(f\"value: {value_loss}\")\n",
    "    print(f\"entropy: {entropy_bonus}\")\n",
    "\n",
    "    # Compute and return total loss\n",
    "    loss = -clipped_objective + V * value_loss - H * entropy_bonus\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0486, -0.0430, -0.0499, -0.0407, -0.0506, -0.0462, -0.0520, -0.0421,\n",
      "        -0.0372, -0.0497, -0.0452, -0.0433, -0.0478, -0.0503, -0.0484, -0.0480,\n",
      "        -0.0493, -0.0446, -0.0410, -0.0473, -0.0490, -0.0480, -0.0491, -0.0491,\n",
      "        -0.0474, -0.0373, -0.0443, -0.0468, -0.0484, -0.0505, -0.0522, -0.0402,\n",
      "        -0.0493, -0.0486, -0.0422, -0.0484, -0.0358, -0.0491, -0.0440, -0.0522,\n",
      "        -0.0471, -0.0526, -0.0477, -0.0419, -0.0413, -0.0460, -0.0496, -0.0482,\n",
      "        -0.0364, -0.0426, -0.0278, -0.0509, -0.0487, -0.0492, -0.0391, -0.0418,\n",
      "        -0.0433, -0.0320, -0.0488, -0.0469, -0.0448, -0.0342, -0.0482, -0.0476,\n",
      "        -0.0425, -0.0437, -0.0482, -0.0440, -0.0495, -0.0511, -0.0508, -0.0494,\n",
      "        -0.0454, -0.0507, -0.0503, -0.0510, -0.0452, -0.0514, -0.0393, -0.0449,\n",
      "        -0.0439, -0.0439, -0.0500, -0.0499, -0.0446, -0.0506, -0.0430, -0.0485,\n",
      "        -0.0455, -0.0418, -0.0474, -0.0436, -0.0436, -0.0490, -0.0373, -0.0477,\n",
      "        -0.0501, -0.0303, -0.0442, -0.0506, -0.0366, -0.0414, -0.0482, -0.0422,\n",
      "        -0.0316, -0.0488, -0.0484, -0.0488, -0.0497, -0.0462, -0.0479, -0.0490,\n",
      "        -0.0400, -0.0504, -0.0506, -0.0452, -0.0488, -0.0446, -0.0482, -0.0469,\n",
      "        -0.0472, -0.0495, -0.0396, -0.0508, -0.0434, -0.0400, -0.0503, -0.0465],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "ola torch.Size([128, 2])\n",
      "clipped: 13.098289489746094\n",
      "value: 265.92364501953125\n",
      "entropy: 0.6929453611373901\n",
      "tensor(13.4248, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test_model(state_batch)[:, -1])\n",
    "loss = loss_fn(test_model, state_batch, action_batch, pi_batch, target_value_batch, advantage_batch)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude noted a problem that i don't have log probs anywhere, but rather logits. I need to fix this maybe but maybe not.\n",
    "\n",
    "There's a function called log_softmax that looks good for the task.\n",
    "\n",
    "I implemented it but idk if it's correct. Fuck it I just want to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolicyAndValueNetwork()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, loss_fn, optimizer):\n",
    "    losses = []\n",
    "    for _ in range(N_TRAIN_LOOPS):\n",
    "        data_collector = DataCollector()\n",
    "        \n",
    "        for agent in range(N_AGENTS):\n",
    "            state, _ = env.reset()\n",
    "            terminated, truncated = False, False\n",
    "\n",
    "            for step in range(T):\n",
    "                # Compute and divide model output\n",
    "                with torch.no_grad():\n",
    "                    state = torch.from_numpy(state)\n",
    "                    output = model(state)\n",
    "                action_logits = output[:-1]\n",
    "\n",
    "                # Select and perform action\n",
    "                action = select_action(action_logits)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                # Compute pi\n",
    "                pi = torch.log_softmax(action_logits, dim=-1)[action]\n",
    "\n",
    "                # Store data\n",
    "                data_collector.add_step(agent, state, action, reward, pi)\n",
    "\n",
    "                # If terminated reset env and mark end, otherwise update state\n",
    "                if terminated or truncated:\n",
    "                    data_collector.mark_episode_end(agent, step)\n",
    "                    state, _ = env.reset()\n",
    "                    terminated, truncated = False, False\n",
    "                else:\n",
    "                    state = next_state\n",
    "\n",
    "        # Fetch data from data collector\n",
    "        states, actions, rewards, pis, episode_ends = data_collector.fetch_data()\n",
    "\n",
    "        # Compute advantages and target values\n",
    "        target_values, advantages = compute_advantages_and_target_values(\n",
    "            model=model,\n",
    "            states=states,\n",
    "            rewards=rewards,\n",
    "            episode_ends=episode_ends,\n",
    "        )\n",
    "\n",
    "        # Add everything to dataset and create dataloader\n",
    "        dataset = PPODataset(states, actions, pis, target_values, advantages)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Train model on policy\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            for state_batch, action_batch, pi_batch, target_value_batch, advantage_batch in dataloader:\n",
    "                loss = loss_fn(model, state_batch, action_batch, pi_batch, target_value_batch, advantage_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ola torch.Size([128, 2])\n",
      "clipped: 11.798755645751953\n",
      "value: 209.6201171875\n",
      "entropy: 0.6875829100608826\n",
      "ola torch.Size([128, 2])\n",
      "clipped: 12.696859359741211\n",
      "value: 262.3168640136719\n",
      "entropy: 0.6905707120895386\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN:\n\u001b[0;32m----> 2\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[94], line 58\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state_batch, action_batch, pi_batch, target_value_batch, advantage_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     57\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(model, state_batch, action_batch, pi_batch, target_value_batch, advantage_batch)\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     60\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    losses = train_loop(model, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
