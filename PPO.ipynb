{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "N_OBSERVATIONS = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "N_AGENTS = 2\n",
    "T = 50\n",
    "GAMMA = 0.999\n",
    "EPSILON = 0.2\n",
    "H = 0.1 # Entropy constant\n",
    "V = 0.1 # Value constant\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "TEST = True\n",
    "TRAIN = False\n",
    "\n",
    "layer_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and selecting actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the same parameters for the policy and value networks. If this doesn't work well I can change it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAndValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(N_OBSERVATIONS, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            # +1 comes from value prediction\n",
    "            nn.Linear(layer_dim, N_ACTIONS + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select action based on log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_logits):\n",
    "    action_probs = torch.softmax(action_logits, dim=0)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_env = gym.make('CartPole-v1', render_mode='human')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "\n",
    "    for _ in range(1):\n",
    "        state, info = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        while not (terminated or truncated):\n",
    "            tensor_state = torch.from_numpy(state)\n",
    "            logits = test_model(tensor_state)\n",
    "            action_logits = logits[:-1]\n",
    "            action = select_action(action_logits)\n",
    "            state, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first collect the data while marking episode ends when they happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector():\n",
    "    def __init__(self):\n",
    "        self.states = [[] for _ in range(N_AGENTS)]\n",
    "        self.actions = [[] for _ in range(N_AGENTS)]\n",
    "        self.rewards = [[] for _ in range(N_AGENTS)]\n",
    "        self.pis = [[] for _ in range(N_AGENTS)]\n",
    "        self.episode_ends = [[] for _ in range(N_AGENTS)]\n",
    "\n",
    "    def add_step(self, agent_id, state, action, reward, pi):\n",
    "        self.states[agent_id].append(state)\n",
    "        self.actions[agent_id].append(action)\n",
    "        self.rewards[agent_id].append(reward)\n",
    "        self.pis[agent_id].append(pi)\n",
    "\n",
    "    def mark_episode_end(self, agent_id, timestep):\n",
    "        self.episode_ends[agent_id].append(timestep)\n",
    "\n",
    "    def fetch_data(self):\n",
    "        list_of_agent_states = [torch.stack(agent_states) for agent_states in self.states]\n",
    "        states = torch.stack([agent_states for agent_states in list_of_agent_states])\n",
    "        actions = self.actions\n",
    "        rewards = torch.tensor(self.rewards)\n",
    "        pis = torch.tensor(self.pis)\n",
    "        episode_ends = self.episode_ends\n",
    "\n",
    "        return states, actions, rewards, pis, episode_ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute advantages and target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll need to compute advantages and target values. We'll start from the end of the data for each agent and move backwards. This makes the desired values easier to compute and to take into account episode ends.\n",
    "\n",
    "Gradients should be on here because I'll use the computations here to optimize the model.\n",
    "\n",
    "> If the trajectory terminated due to the maximal trajectory length T\n",
    "being reached, Vωold (st+n ) denotes the state value associated with state st+n as predicted by the state value\n",
    "network. Otherwise, Vωold (st+n ) is set to 0\n",
    "\n",
    "Instead of computing r_t + V(s_t+1) I'm using V(s_t). It's similar enough and shouldn't be that much of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages_and_target_values(model, states, rewards, episode_ends):\n",
    "    # We'll add them from last to first, then reverse list\n",
    "    reversed_target_values = [[] for _ in range(N_AGENTS)]\n",
    "    reversed_advantages = [[] for _ in range(N_AGENTS)]\n",
    "\n",
    "    for agent in range(N_AGENTS):\n",
    "        for i in range(T):\n",
    "            # Get step state and reward and compute predicted value\n",
    "            state = states[agent, -i-1]\n",
    "            reward = rewards[agent, -i-1]\n",
    "            predicted_value = model(state)[-1]\n",
    "\n",
    "            # If step is final\n",
    "            if i in episode_ends[agent]:\n",
    "                target_value = reward\n",
    "            # If step is last but not final\n",
    "            elif i == 0:\n",
    "                target_value = predicted_value\n",
    "            # non-last non-final step\n",
    "            else:\n",
    "                # Get previous target value and compute \n",
    "                previous_target_value = reversed_target_values[agent][-1]\n",
    "                target_value = reward + GAMMA * previous_target_value\n",
    "            advantage = target_value - predicted_value\n",
    "\n",
    "            # Append target value and advantage\n",
    "            reversed_target_values[agent].append(target_value)\n",
    "            reversed_advantages[agent].append(advantage)\n",
    "\n",
    "    target_values = [torch.stack(agent_target_values[::-1]) for agent_target_values in reversed_target_values]\n",
    "    advantages = [torch.stack(agent_advantage[::-1]) for agent_advantage in reversed_advantages]\n",
    "\n",
    "    return target_values, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to modify the data so that I can sample random batches from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPODataset(Dataset):\n",
    "    def __init__(self, states, actions, pis, target_values, advantages):\n",
    "        # Reshape all into (N_AGENTS * T, whatever (possibly 0))\n",
    "        self.states = states.reshape(N_AGENTS * T, -1)\n",
    "        self.actions = [action for agent in actions for action in agent]\n",
    "        self.pis = pis.reshape(N_AGENTS * T)\n",
    "        self.target_values = torch.cat(target_values)\n",
    "        self.advantages = torch.cat(advantages)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        state = self.states[i]\n",
    "        action = self.actions[i]\n",
    "        pi = self.pis[i]\n",
    "        target_value = self.target_values[i]\n",
    "        advantage = self.advantages[i]\n",
    "\n",
    "        return state, action, pi, target_value, advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_batch: tensor([[ 0.0267,  0.2385, -0.0040, -0.2688],\n",
      "        [ 0.0243, -0.0305, -0.0231,  0.0156],\n",
      "        [-0.0338, -0.0283,  0.0256,  0.0757],\n",
      "        [-0.0366, -0.0275,  0.0290,  0.0570],\n",
      "        [-0.0373,  0.0101,  0.1854,  0.5756],\n",
      "        [-0.0371,  0.1672,  0.0301, -0.2264],\n",
      "        [-0.0310, -0.0291,  0.0229,  0.0922],\n",
      "        [ 0.0315,  0.4337, -0.0094, -0.5628],\n",
      "        [ 0.0636,  0.3631, -0.0883, -0.6461],\n",
      "        [-0.0344,  0.1664,  0.0271, -0.2089]])\n",
      "action_batch: tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0])\n",
      "pi_batch: tensor([0.0354, 0.0385, 0.0372, 0.0380, 0.0390, 0.0304, 0.0364, 0.0285, 0.0206,\n",
      "        0.0306])\n",
      "target_value_batch: tensor([15.8806,  4.9900,  6.0028,  7.9898, 22.7663,  6.9968,  4.0118, 14.8955,\n",
      "        10.9452,  5.0078], grad_fn=<StackBackward0>)\n",
      "advantage_batch: tensor([15.8562,  4.9687,  5.9832,  7.9695, 22.7610,  6.9713,  3.9928, 14.8737,\n",
      "        10.9211,  4.9825], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    # Initialize training data, test environment\n",
    "    test_data_collector = DataCollector()\n",
    "    test_env = gym.make('CartPole-v1')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "    \n",
    "    for agent in range(N_AGENTS):\n",
    "        state, _ = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        \n",
    "        for step in range(T):\n",
    "            # Compute and divide model output\n",
    "            with torch.no_grad():\n",
    "                state = torch.from_numpy(state)\n",
    "                output = test_model(state)\n",
    "            action_logits, value_logit = output[:-1], output[-1]\n",
    "\n",
    "            # Select and perform action\n",
    "            action = select_action(action_logits)\n",
    "            next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "            # Compute pi\n",
    "            pi = action_logits[action]\n",
    "\n",
    "            # Store data\n",
    "            test_data_collector.add_step(agent, state, action, reward, pi)\n",
    "\n",
    "            # If terminated reset env and mark end, otherwise update state\n",
    "            if terminated or truncated:\n",
    "                test_data_collector.mark_episode_end(agent, step)\n",
    "                state, _ = test_env.reset()\n",
    "                terminated, truncated = False, False\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    # Fetch data from data collector\n",
    "    states, actions, rewards, pis, episode_ends = test_data_collector.fetch_data()\n",
    "\n",
    "    # Compute advantages and target values\n",
    "    target_values, advantages = compute_advantages_and_target_values(\n",
    "        model=test_model,\n",
    "        states=states,\n",
    "        rewards=rewards,\n",
    "        episode_ends=episode_ends,\n",
    "    )\n",
    "\n",
    "    # Add everything to dataset and create dataloader\n",
    "    test_dataset = PPODataset(states, actions, pis, target_values, advantages)\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    state_batch, action_batch, pi_batch, target_value_batch, advantage_batch = next(iter(test_dataloader))\n",
    "    print(f\"state_batch: {state_batch}\")\n",
    "    print(f\"action_batch: {action_batch}\")\n",
    "    print(f\"pi_batch: {pi_batch}\")\n",
    "    print(f\"target_value_batch: {target_value_batch}\")\n",
    "    print(f\"advantage_batch: {advantage_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's all working, fuck yeah. Now it's time to write the training code. By the way, I tested the code incrementally, not all at once, but now that it's all working I left only this big chunk of code, since the rest was mostly building blocks for this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 3-in-1 loss function. We want to minimize\n",
    "\n",
    "- -L_CLIP  + V * L_V - H * L_H\n",
    "\n",
    "I'll write each of the separate functions then combine them in one that will be backpropagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_objective_fn(log_probs, pi_batch, action_batch, advantage_batch):\n",
    "    # Compute pi_new\n",
    "    pi_new = log_probs[torch.arange(len(action_batch)), action_batch]\n",
    "\n",
    "    # Compute p_ratio, ratio of new and old probabilities\n",
    "    # note that it's unnormalized\n",
    "    p_ratio = torch.exp(pi_new - pi_batch)\n",
    "\n",
    "    # Compute unclipped and clipped surrogate objectives\n",
    "    unclipped_surrogate_objective = p_ratio * advantage_batch\n",
    "    clipped_surrogate_objective = torch.clamp(p_ratio, 1. - EPSILON, 1. + EPSILON) * advantage_batch\n",
    "\n",
    "    # Compute elementwise minimum of two and return\n",
    "    clipped_objective = torch.mean(torch.min(unclipped_surrogate_objective, clipped_surrogate_objective))\n",
    "    return clipped_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0354, 0.0385, 0.0372, 0.0380, 0.0390, 0.0304, 0.0364, 0.0285, 0.0206,\n",
      "        0.0306])\n",
      "tensor(9.9280, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "log_probs = test_model(state_batch)[:, :N_ACTIONS]\n",
    "print(pi_batch)\n",
    "print(clipped_objective_fn(log_probs, pi_batch, action_batch, advantage_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: I'm not calculating the actual probabilities. For that I'd have to compute softmax in each and then divide. The problem is that the sum of new and old exponentials might differ, and thus doing that I'm doing leads to a different value.\n",
    "\n",
    "Claude said this shouldn't be an issue but it might be we'll see if the model learns or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_bonus_fn(log_probs):\n",
    "    print(\"ola\", log_probs.shape)\n",
    "    # Compute probabilities\n",
    "    probs = torch.softmax(log_probs, dim=-1)\n",
    "\n",
    "    # Compute and return entropy\n",
    "    entropy = -torch.mean(torch.sum(probs * log_probs, dim=-1))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_loss_fn(pred_values, target_value_batch):\n",
    "    # Compute and return loss\n",
    "    mseloss = nn.MSELoss(reduction='mean')\n",
    "    loss = mseloss(pred_values, target_value_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, state_batch, action_batch, pi_batch, target_value_batch, advantage_batch):\n",
    "    # Get pred log probs and values\n",
    "    output = model(state_batch)\n",
    "    log_probs, pred_values = output[:, :N_ACTIONS], output[:, -1]\n",
    "\n",
    "    # Compute individual losses\n",
    "    clipped_objective = clipped_objective_fn(log_probs, pi_batch, action_batch, advantage_batch)\n",
    "    value_loss = value_loss_fn(pred_values, target_value_batch)\n",
    "    entropy_bonus = entropy_bonus_fn(log_probs)\n",
    "    print(f\"clipped: {clipped_objective}\")\n",
    "    print(f\"value: {value_loss}\")\n",
    "    print(f\"entropy: {entropy_bonus}\")\n",
    "\n",
    "    # Compute and return total loss\n",
    "    loss = -clipped_objective + V * value_loss - H * entropy_bonus\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0244, 0.0214, 0.0196, 0.0203, 0.0052, 0.0255, 0.0190, 0.0218, 0.0240,\n",
      "        0.0253], grad_fn=<SelectBackward0>)\n",
      "ola torch.Size([10, 2])\n",
      "clipped: 9.927999496459961\n",
      "value: 132.33462524414062\n",
      "entropy: -0.032601069658994675\n",
      "tensor(3.3087, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(test_model(state_batch)[:, -1])\n",
    "loss = loss_fn(test_model, state_batch, action_batch, pi_batch, target_value_batch, advantage_batch)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude noted a problem that i don't have log probs anywhere, but rather logits. I need to fix this maybe but maybe not.\n",
    "\n",
    "There's a function called log_softmax that looks good for the task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
