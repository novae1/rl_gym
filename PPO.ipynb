{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "N_OBSERVATIONS = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_AGENTS = 2\n",
    "T = 50\n",
    "GAMMA = 1.\n",
    "\n",
    "TEST = True\n",
    "TRAIN = False\n",
    "\n",
    "layer_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and selecting actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use the same parameters for the policy and value networks. If this doesn't work well I can change it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyAndValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(N_OBSERVATIONS, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.ReLU(),\n",
    "            # +1 comes from value prediction\n",
    "            nn.Linear(layer_dim, N_ACTIONS + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.network(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select action based on log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_logits):\n",
    "    action_probs = torch.softmax(action_logits, dim=0)\n",
    "    action = torch.multinomial(action_probs, num_samples=1).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    test_env = gym.make('CartPole-v1', render_mode='human')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "\n",
    "    for _ in range(1):\n",
    "        state, info = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        while not (terminated or truncated):\n",
    "            tensor_state = torch.from_numpy(state)\n",
    "            logits = test_model(tensor_state)\n",
    "            action_logits = logits[:-1]\n",
    "            action = select_action(action_logits)\n",
    "            state, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first collect the data while marking episode ends when they happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector():\n",
    "    def __init__(self, n_agents):\n",
    "        self.states = [[] for _ in range(n_agents)]\n",
    "        self.actions = [[] for _ in range(n_agents)]\n",
    "        self.rewards = [[] for _ in range(n_agents)]\n",
    "        self.pis = [[] for _ in range(n_agents)]\n",
    "        self.episode_ends = [[] for _ in range(n_agents)]\n",
    "\n",
    "    def add_step(self, agent_id, state, action, reward, pi):\n",
    "        self.states[agent_id].append(state)\n",
    "        self.actions[agent_id].append(action)\n",
    "        self.rewards[agent_id].append(reward)\n",
    "        self.pis[agent_id].append(pi)\n",
    "\n",
    "    def mark_episode_end(self, agent_id, timestep):\n",
    "        self.episode_ends[agent_id].append(timestep)\n",
    "\n",
    "    def fetch_data(self):\n",
    "        list_of_agent_states = [torch.stack(agent_states) for agent_states in self.states]\n",
    "        states = torch.stack([agent_states for agent_states in list_of_agent_states])\n",
    "        actions = self.actions\n",
    "        rewards = torch.tensor(self.rewards)\n",
    "        pis = torch.tensor(self.pis)\n",
    "        episode_ends = self.episode_ends\n",
    "\n",
    "        return states, actions, rewards, pis, episode_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: tensor([[[ 3.1238e-02,  1.9498e-02,  2.6910e-02,  4.2010e-02],\n",
      "         [ 3.1628e-02,  2.1422e-01,  2.7750e-02, -2.4206e-01],\n",
      "         [ 3.5912e-02,  1.8717e-02,  2.2909e-02,  5.9243e-02],\n",
      "         [ 3.6287e-02, -1.7673e-01,  2.4093e-02,  3.5906e-01],\n",
      "         [ 3.2752e-02, -3.7218e-01,  3.1275e-02,  6.5925e-01],\n",
      "         [ 2.5309e-02, -1.7751e-01,  4.4460e-02,  3.7657e-01],\n",
      "         [ 2.1759e-02,  1.6955e-02,  5.1991e-02,  9.8233e-02],\n",
      "         [ 2.2098e-02, -1.7887e-01,  5.3956e-02,  4.0686e-01],\n",
      "         [ 1.8520e-02,  1.5445e-02,  6.2093e-02,  1.3166e-01],\n",
      "         [ 1.8829e-02, -1.8051e-01,  6.4726e-02,  4.4327e-01],\n",
      "         [ 1.5219e-02,  1.3640e-02,  7.3591e-02,  1.7167e-01],\n",
      "         [ 1.5492e-02,  2.0764e-01,  7.7025e-02, -9.6920e-02],\n",
      "         [ 1.9644e-02,  4.0157e-01,  7.5086e-02, -3.6434e-01],\n",
      "         [ 2.7676e-02,  5.9555e-01,  6.7800e-02, -6.3244e-01],\n",
      "         [ 3.9587e-02,  3.9955e-01,  5.5151e-02, -3.1920e-01],\n",
      "         [ 4.7578e-02,  2.0369e-01,  4.8767e-02, -9.6425e-03],\n",
      "         [ 5.1652e-02,  7.9054e-03,  4.8574e-02,  2.9802e-01],\n",
      "         [ 5.1810e-02, -1.8787e-01,  5.4534e-02,  6.0562e-01],\n",
      "         [ 4.8052e-02,  6.4446e-03,  6.6647e-02,  3.3060e-01],\n",
      "         [ 4.8181e-02, -1.8956e-01,  7.3259e-02,  6.4353e-01],\n",
      "         [ 4.4390e-02, -3.8562e-01,  8.6129e-02,  9.5835e-01],\n",
      "         [ 3.6678e-02, -1.9176e-01,  1.0530e-01,  6.9393e-01],\n",
      "         [ 3.2843e-02,  1.7588e-03,  1.1918e-01,  4.3616e-01],\n",
      "         [ 3.2878e-02,  1.9501e-01,  1.2790e-01,  1.8329e-01],\n",
      "         [ 3.6778e-02, -1.6880e-03,  1.3156e-01,  5.1343e-01],\n",
      "         [ 3.6744e-02, -1.9839e-01,  1.4183e-01,  8.4451e-01],\n",
      "         [ 3.2776e-02, -5.4623e-03,  1.5872e-01,  5.9958e-01],\n",
      "         [ 3.2667e-02,  1.8712e-01,  1.7071e-01,  3.6080e-01],\n",
      "         [ 3.6410e-02, -9.9604e-03,  1.7793e-01,  7.0207e-01],\n",
      "         [ 3.6210e-02,  1.8231e-01,  1.9197e-01,  4.7026e-01],\n",
      "         [ 3.9856e-02, -1.4934e-02,  2.0138e-01,  8.1678e-01],\n",
      "         [ 1.5191e-03,  3.1534e-02, -3.5368e-02,  3.7289e-02],\n",
      "         [ 2.1498e-03, -1.6306e-01, -3.4622e-02,  3.1861e-01],\n",
      "         [-1.1114e-03, -3.5768e-01, -2.8250e-02,  6.0017e-01],\n",
      "         [-8.2649e-03, -5.5239e-01, -1.6246e-02,  8.8383e-01],\n",
      "         [-1.9313e-02, -3.5705e-01,  1.4302e-03,  5.8608e-01],\n",
      "         [-2.6454e-02, -5.5219e-01,  1.3152e-02,  8.7921e-01],\n",
      "         [-3.7498e-02, -3.5725e-01,  3.0736e-02,  5.9069e-01],\n",
      "         [-4.4643e-02, -1.6257e-01,  4.2550e-02,  3.0785e-01],\n",
      "         [-4.7894e-02,  3.1916e-02,  4.8707e-02,  2.8883e-02],\n",
      "         [-4.7256e-02,  2.2631e-01,  4.9285e-02, -2.4804e-01],\n",
      "         [-4.2730e-02,  3.0517e-02,  4.4324e-02,  5.9769e-02],\n",
      "         [-4.2119e-02,  2.2498e-01,  4.5519e-02, -2.1861e-01],\n",
      "         [-3.7620e-02,  2.9234e-02,  4.1147e-02,  8.8080e-02],\n",
      "         [-3.7035e-02,  2.2374e-01,  4.2909e-02, -1.9134e-01],\n",
      "         [-3.2560e-02,  4.1823e-01,  3.9082e-02, -4.7019e-01],\n",
      "         [-2.4196e-02,  2.2257e-01,  2.9678e-02, -1.6545e-01],\n",
      "         [-1.9744e-02,  4.1726e-01,  2.6369e-02, -4.4862e-01],\n",
      "         [-1.1399e-02,  6.1200e-01,  1.7397e-02, -7.3288e-01],\n",
      "         [ 8.4075e-04,  4.1664e-01,  2.7391e-03, -4.3477e-01]],\n",
      "\n",
      "        [[ 3.5256e-02,  2.1980e-02,  4.7356e-03, -2.9621e-02],\n",
      "         [ 3.5696e-02,  2.1703e-01,  4.1432e-03, -3.2081e-01],\n",
      "         [ 4.0037e-02,  2.1853e-02, -2.2729e-03, -2.6819e-02],\n",
      "         [ 4.0474e-02,  2.1701e-01, -2.8093e-03, -3.2022e-01],\n",
      "         [ 4.4814e-02,  2.1926e-02, -9.2137e-03, -2.8423e-02],\n",
      "         [ 4.5252e-02, -1.7306e-01, -9.7821e-03,  2.6134e-01],\n",
      "         [ 4.1791e-02, -3.6804e-01, -4.5553e-03,  5.5092e-01],\n",
      "         [ 3.4430e-02, -1.7286e-01,  6.4631e-03,  2.5681e-01],\n",
      "         [ 3.0973e-02, -3.6807e-01,  1.1599e-02,  5.5152e-01],\n",
      "         [ 2.3612e-02, -1.7311e-01,  2.2630e-02,  2.6251e-01],\n",
      "         [ 2.0149e-02, -3.6855e-01,  2.7880e-02,  5.6225e-01],\n",
      "         [ 1.2778e-02, -5.6405e-01,  3.9125e-02,  8.6358e-01],\n",
      "         [ 1.4972e-03, -3.6949e-01,  5.6397e-02,  5.8345e-01],\n",
      "         [-5.8925e-03, -1.7520e-01,  6.8066e-02,  3.0906e-01],\n",
      "         [-9.3964e-03,  1.8892e-02,  7.4247e-02,  3.8592e-02],\n",
      "         [-9.0186e-03, -1.7721e-01,  7.5019e-02,  3.5375e-01],\n",
      "         [-1.2563e-02, -3.7332e-01,  8.2093e-02,  6.6911e-01],\n",
      "         [-2.0029e-02, -1.7943e-01,  9.5476e-02,  4.0336e-01],\n",
      "         [-2.3618e-02,  1.4222e-02,  1.0354e-01,  1.4224e-01],\n",
      "         [-2.3333e-02,  2.0772e-01,  1.0639e-01, -1.1607e-01],\n",
      "         [-1.9179e-02,  1.1248e-02,  1.0407e-01,  2.0820e-01],\n",
      "         [-1.8954e-02,  2.0474e-01,  1.0823e-01, -4.9931e-02],\n",
      "         [-1.4859e-02,  8.2456e-03,  1.0723e-01,  2.7484e-01],\n",
      "         [-1.4694e-02, -1.8823e-01,  1.1273e-01,  5.9933e-01],\n",
      "         [-1.8459e-02,  5.1494e-03,  1.2472e-01,  3.4417e-01],\n",
      "         [-1.8356e-02,  1.9830e-01,  1.3160e-01,  9.3274e-02],\n",
      "         [-1.4390e-02,  1.5583e-03,  1.3346e-01,  4.2441e-01],\n",
      "         [-1.4359e-02,  1.9456e-01,  1.4195e-01,  1.7660e-01],\n",
      "         [-1.0467e-02,  3.8740e-01,  1.4548e-01, -6.8150e-02],\n",
      "         [-2.7194e-03,  1.9052e-01,  1.4412e-01,  2.6667e-01],\n",
      "         [ 1.0910e-03, -6.3315e-03,  1.4945e-01,  6.0111e-01],\n",
      "         [ 9.6438e-04,  1.8642e-01,  1.6148e-01,  3.5898e-01],\n",
      "         [ 4.6928e-03,  3.7892e-01,  1.6866e-01,  1.2125e-01],\n",
      "         [ 1.2271e-02,  1.8183e-01,  1.7108e-01,  4.6204e-01],\n",
      "         [ 1.5908e-02,  3.7418e-01,  1.8032e-01,  2.2779e-01],\n",
      "         [ 2.3391e-02,  1.7700e-01,  1.8488e-01,  5.7149e-01],\n",
      "         [ 2.6931e-02,  3.6911e-01,  1.9631e-01,  3.4228e-01],\n",
      "         [ 3.4314e-02,  5.6098e-01,  2.0315e-01,  1.1735e-01],\n",
      "         [ 4.5533e-02,  3.6361e-01,  2.0550e-01,  4.6662e-01],\n",
      "         [ 3.2968e-02,  3.4238e-02,  2.8422e-02,  3.6698e-02],\n",
      "         [ 3.3653e-02, -1.6128e-01,  2.9156e-02,  3.3821e-01],\n",
      "         [ 3.0427e-02, -3.5680e-01,  3.5920e-02,  6.3994e-01],\n",
      "         [ 2.3291e-02, -1.6220e-01,  4.8719e-02,  3.5878e-01],\n",
      "         [ 2.0047e-02, -3.5798e-01,  5.5895e-02,  6.6642e-01],\n",
      "         [ 1.2887e-02, -5.5383e-01,  6.9223e-02,  9.7617e-01],\n",
      "         [ 1.8105e-03, -3.5970e-01,  8.8746e-02,  7.0601e-01],\n",
      "         [-5.3836e-03, -5.5594e-01,  1.0287e-01,  1.0253e+00],\n",
      "         [-1.6502e-02, -7.5227e-01,  1.2337e-01,  1.3484e+00],\n",
      "         [-3.1548e-02, -9.4870e-01,  1.5034e-01,  1.6770e+00],\n",
      "         [-5.0522e-02, -7.5561e-01,  1.8388e-01,  1.4346e+00]]])\n",
      "actions: [[1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1], [1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0]]\n",
      "rewards: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "pis: tensor([[-0.0370, -0.0693, -0.0691, -0.0757, -0.0580, -0.0484, -0.0693, -0.0494,\n",
      "         -0.0695, -0.0505, -0.0402, -0.0295, -0.0311, -0.0829, -0.0699, -0.0684,\n",
      "         -0.0715, -0.0529, -0.0726, -0.0854, -0.0559, -0.0538, -0.0464, -0.0703,\n",
      "         -0.0791, -0.0536, -0.0505, -0.0733, -0.0513, -0.0768, -0.0908, -0.0701,\n",
      "         -0.0727, -0.0843, -0.0620, -0.0842, -0.0626, -0.0570, -0.0461, -0.0368,\n",
      "         -0.0710, -0.0374, -0.0708, -0.0380, -0.0290, -0.0767, -0.0291, -0.0326,\n",
      "         -0.0888, -0.0326],\n",
      "        [-0.0350, -0.0704, -0.0350, -0.0703, -0.0691, -0.0717, -0.0572, -0.0718,\n",
      "         -0.0573, -0.0724, -0.0843, -0.0628, -0.0576, -0.0469, -0.0702, -0.0760,\n",
      "         -0.0583, -0.0499, -0.0399, -0.0708, -0.0413, -0.0703, -0.0720, -0.0538,\n",
      "         -0.0439, -0.0711, -0.0461, -0.0360, -0.0694, -0.0715, -0.0504, -0.0410,\n",
      "         -0.0689, -0.0433, -0.0703, -0.0464, -0.0364, -0.0658, -0.0388, -0.0691,\n",
      "         -0.0749, -0.0574, -0.0756, -0.0866, -0.0621, -0.0882, -0.0932, -0.0927,\n",
      "         -0.0740, -0.0937]])\n",
      "episode ends: [[30], [38, 49]]\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    # Initialize training data, test environment\n",
    "    test_data_collector = DataCollector(N_AGENTS)\n",
    "    test_env = gym.make('CartPole-v1')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "    \n",
    "    for agent_id in range(N_AGENTS):\n",
    "        state, _ = test_env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        \n",
    "        for step in range(T):\n",
    "            # Compute and divide model output\n",
    "            with torch.no_grad():\n",
    "                state = torch.from_numpy(state)\n",
    "                output = test_model(state)\n",
    "            action_logits, value_logit = output[:-1], output[-1]\n",
    "\n",
    "            # Select and perform action\n",
    "            action = select_action(action_logits)\n",
    "            next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "            # Compute pi\n",
    "            pi = action_logits[action]\n",
    "\n",
    "            # Store data\n",
    "            test_data_collector.add_step(agent_id, state, action, reward, pi)\n",
    "\n",
    "            # If terminated reset env and mark end, otherwise update state\n",
    "            if terminated or truncated:\n",
    "                test_data_collector.mark_episode_end(agent_id, step)\n",
    "                state, _ = test_env.reset()\n",
    "                terminated, truncated = False, False\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    states, actions, rewards, pis, episode_ends = test_data_collector.fetch_data()\n",
    "    print(f\"states: {states}\")\n",
    "    print(f\"actions: {actions}\")\n",
    "    print(f\"rewards: {rewards}\")\n",
    "    print(f\"pis: {pis}\")\n",
    "    print(f\"episode ends: {episode_ends}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute advantages and target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll need to compute advantages and target values. We'll start from the end of the data for each agent and move backwards. This makes the desired values easier to compute and to take into account episode ends.\n",
    "\n",
    "Gradients should be on here because I'll use the computations here to optimize the model.\n",
    "\n",
    "> If the trajectory terminated due to the maximal trajectory length T\n",
    "being reached, Vωold (st+n ) denotes the state value associated with state st+n as predicted by the state value\n",
    "network. Otherwise, Vωold (st+n ) is set to 0\n",
    "\n",
    "Instead of computing r_t + V(s_t+1) I'm using V(s_t). It's similar enough and shouldn't be that much of a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages_and_target_values(model, states, rewards, episode_ends):\n",
    "    n_agents = len(episode_ends)\n",
    "\n",
    "    # We'll add them from last to first, then reverse list\n",
    "    reversed_target_values = [[] for _ in range(n_agents)]\n",
    "    reversed_advantages = [[] for _ in range(n_agents)]\n",
    "\n",
    "    for agent in range(n_agents):\n",
    "        for i in range(T):\n",
    "            # Get step state and reward and compute predicted value\n",
    "            state = states[agent, -i-1]\n",
    "            reward = rewards[agent, -i-1]\n",
    "            predicted_value = model(state)[-1]\n",
    "\n",
    "            # If step is final\n",
    "            if i in episode_ends[agent]:\n",
    "                target_value = reward\n",
    "            # If step is last but not final\n",
    "            elif i == 0:\n",
    "                target_value = predicted_value\n",
    "            # non-last non-final step\n",
    "            else:\n",
    "                # Get previous target value and compute \n",
    "                previous_target_value = reversed_target_values[agent][-1]\n",
    "                target_value = reward + GAMMA * previous_target_value\n",
    "            advantage = target_value - predicted_value\n",
    "\n",
    "            # Append target value and advantage\n",
    "            reversed_target_values[agent].append(target_value)\n",
    "            reversed_advantages[agent].append(advantage)\n",
    "\n",
    "    target_values = [torch.stack(agent_target_values[::-1]) for agent_target_values in reversed_target_values]\n",
    "    advantages = [torch.stack(agent_advantage[::-1]) for agent_advantage in reversed_advantages]\n",
    "\n",
    "    return target_values, advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target values: [tensor([ 2.0000e+01,  1.9000e+01,  1.8000e+01,  1.7000e+01,  1.6000e+01,\n",
      "         1.5000e+01,  1.4000e+01,  1.3000e+01,  1.2000e+01,  1.1000e+01,\n",
      "         1.0000e+01,  9.0000e+00,  8.0000e+00,  7.0000e+00,  6.0000e+00,\n",
      "         5.0000e+00,  4.0000e+00,  3.0000e+00,  2.0000e+00,  1.0000e+00,\n",
      "         2.8972e+01,  2.7972e+01,  2.6972e+01,  2.5972e+01,  2.4972e+01,\n",
      "         2.3972e+01,  2.2972e+01,  2.1972e+01,  2.0972e+01,  1.9972e+01,\n",
      "         1.8972e+01,  1.7972e+01,  1.6972e+01,  1.5972e+01,  1.4972e+01,\n",
      "         1.3972e+01,  1.2972e+01,  1.1972e+01,  1.0972e+01,  9.9725e+00,\n",
      "         8.9725e+00,  7.9725e+00,  6.9725e+00,  5.9725e+00,  4.9725e+00,\n",
      "         3.9725e+00,  2.9725e+00,  1.9725e+00,  9.7249e-01, -2.7508e-02],\n",
      "       grad_fn=<StackBackward0>), tensor([ 1.0000e+00,  1.1000e+01,  1.0000e+01,  9.0000e+00,  8.0000e+00,\n",
      "         7.0000e+00,  6.0000e+00,  5.0000e+00,  4.0000e+00,  3.0000e+00,\n",
      "         2.0000e+00,  1.0000e+00,  3.6999e+01,  3.5999e+01,  3.4999e+01,\n",
      "         3.3999e+01,  3.2999e+01,  3.1999e+01,  3.0999e+01,  2.9999e+01,\n",
      "         2.8999e+01,  2.7999e+01,  2.6999e+01,  2.5999e+01,  2.4999e+01,\n",
      "         2.3999e+01,  2.2999e+01,  2.1999e+01,  2.0999e+01,  1.9999e+01,\n",
      "         1.8999e+01,  1.7999e+01,  1.6999e+01,  1.5999e+01,  1.4999e+01,\n",
      "         1.3999e+01,  1.2999e+01,  1.1999e+01,  1.0999e+01,  9.9986e+00,\n",
      "         8.9986e+00,  7.9986e+00,  6.9986e+00,  5.9986e+00,  4.9986e+00,\n",
      "         3.9986e+00,  2.9986e+00,  1.9986e+00,  9.9860e-01, -1.3967e-03],\n",
      "       grad_fn=<StackBackward0>)]\n",
      "advantages: [tensor([20.0224, 19.0272, 18.0224, 17.0139, 16.0027, 15.0123, 14.0208, 13.0107,\n",
      "        12.0200, 11.0090, 10.0188,  9.0235,  8.0266,  7.0228,  6.0265,  5.0228,\n",
      "         4.0167,  3.0032,  2.0153,  1.0018, 28.9722, 27.9727, 26.9824, 25.9889,\n",
      "        24.9787, 23.9698, 22.9750, 21.9829, 20.9719, 19.9796, 18.9689, 17.9963,\n",
      "        16.9902, 15.9784, 14.9767, 13.9777, 12.9765, 11.9764, 10.9873,  9.9931,\n",
      "         8.9980,  7.9930,  6.9975,  5.9928,  4.9972,  3.9991,  2.9974,  1.9996,\n",
      "         0.9945,  0.0000], grad_fn=<StackBackward0>), tensor([ 1.0245, 11.0298, 10.0248,  9.0300,  8.0251,  7.0200,  6.0075,  5.0193,\n",
      "         4.0068,  3.0182,  2.0057,  1.0043, 37.0024, 36.0131, 35.0190, 34.0111,\n",
      "        33.0003, 32.0084, 31.0168, 30.0213, 29.0151, 28.0200, 27.0133, 26.0005,\n",
      "        25.0111, 24.0166, 23.0081, 22.0147, 21.0200, 20.0123, 19.0008, 18.0091,\n",
      "        17.0167, 16.0062, 15.0128, 14.0026, 13.0091, 12.0189, 11.0056, 10.0210,\n",
      "         9.0133,  8.0012,  7.0118,  6.0004,  5.0014,  3.9998,  3.0009,  2.0008,\n",
      "         0.9997,  0.0000], grad_fn=<StackBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    target_values, advantages = compute_advantages_and_target_values(test_model, states, rewards, episode_ends)\n",
    "    print(f\"target values: {target_values}\")\n",
    "    print(f\"advantages: {advantages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'pi'])\n",
    "\n",
    "class TrainingData():\n",
    "    def __init__(self):\n",
    "        # List of Transitions\n",
    "        self.data = []\n",
    "        # Current batch index\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def push(self, state, action, reward, pi):\n",
    "        # Initially we don't have the advantage and target value\n",
    "        # they'll be computed during training\n",
    "        advantage, target_value = 0., 0.\n",
    "        self.data.append(Transition(state, action, reward, pi, advantage, target_value))\n",
    "\n",
    "    def push_A_and_V(self, idx, advantage, target_value):\n",
    "        # It makes sense to add the advantage and target value index by index\n",
    "        state, action, reward, pi, _, _ = self.data[idx]\n",
    "        complete_transition = Transition(state, action, reward, pi, advantage, target_value)\n",
    "        self.data[idx] = complete_transition\n",
    "\n",
    "    def randomize_order(self):\n",
    "        random.shuffle(self.data)\n",
    "\n",
    "    def fetch_batch(self, batch_size):\n",
    "        i = self.batch_index\n",
    "        batch = self.data[i:i+batch_size]\n",
    "        \n",
    "        # Update index\n",
    "        self.batch_index += batch_size\n",
    "        if i+batch_size >= self.__len__():\n",
    "            # Reset index\n",
    "            self.batch_index = 0\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    def unpack(self, batch=None):\n",
    "        # batch should be list of Transitions or None\n",
    "        # in which case we unpack all of data\n",
    "        if not batch:\n",
    "            batch = self.data\n",
    "            \n",
    "        states = torch.stack([t.state for t in batch])\n",
    "        actions = torch.tensor([t.action for t in batch])\n",
    "        rewards = torch.tensor([t.reward for t in batch])\n",
    "        pis = torch.stack([t.pi for t in batch])\n",
    "        advantages = torch.tensor([t.advantage for t in batch])\n",
    "        target_values = torch.tensor([t.target_value for t in batch])\n",
    "\n",
    "        return states, actions, rewards, pis, advantages, target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transition.__new__() takes 5 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m pi \u001b[38;5;241m=\u001b[39m action_logits\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Store data\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtest_training_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Update state\u001b[39;00m\n\u001b[1;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mTrainingData.push\u001b[0;34m(self, state, action, reward, pi)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpush\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, reward, pi):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Initially we don't have the advantage and target value\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# they'll be computed during training\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     advantage, target_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mappend(\u001b[43mTransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_value\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Transition.__new__() takes 5 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    # Initialize training data, test environment\n",
    "    test_training_data = TrainingData()\n",
    "    test_env = gym.make('CartPole-v1')\n",
    "    test_model = PolicyAndValueNetwork()\n",
    "    \n",
    "    state, _ = test_env.reset()\n",
    "    terminated, truncated = False, False\n",
    "    \n",
    "    while not (terminated or truncated):\n",
    "        # Compute and divide model output\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state)\n",
    "            output = test_model(state)\n",
    "        action_logits, value_logit = output[:-1], output[-1]\n",
    "\n",
    "        # Select and perform action\n",
    "        action = select_action(action_logits)\n",
    "        next_state, reward, terminated, truncated, _ = test_env.step(action)\n",
    "\n",
    "        # Compute pi\n",
    "        pi = action_logits\n",
    "\n",
    "        # Store data\n",
    "        test_training_data.push(state, action, reward, pi)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "    # Unpack training data\n",
    "    state, action, reward, pi, _, _ = test_training_data.unpack()\n",
    "    print(f\"state: {state}\")\n",
    "    print(f\"action: {action}\")\n",
    "    print(f\"reward: {reward}\")\n",
    "    print(f\"pi: {pi}\")\n",
    "\n",
    "    # Compute advantage and target value for each transition\n",
    "    for i in range(len(test_training_data)):\n",
    "        print(test_training_data.batch_index)\n",
    "        i_data = test_training_data.fetch_batch(1)\n",
    "        print(i_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
