{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/danielsadoc/RL_jack_car_rental/blob/main/blackwell_jack_rental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C09wohRaTsHd",
   "metadata": {
    "id": "C09wohRaTsHd"
   },
   "source": [
    "# Jack's Car Rental — Discounted vs Average Reward (Blackwell Optimality)\n",
    "\n",
    "This notebook illustrates the **Blackwell optimality theorem** through the Jack’s Car Rental MDP:\n",
    "\n",
    "- Solve **average-reward MDP** via occupancy measure LP\n",
    "- Solve **discounted MDP** for several \\( \\gamma \\uparrow 1 \\)\n",
    "- Compare optimal policies \\( \\pi^*_{\\gamma} \\) and \\( \\pi^{\\text{avg}} \\)\n",
    "- Identify empirical Blackwell region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NdoWKOKATsHf",
   "metadata": {
    "id": "NdoWKOKATsHf"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from scipy.optimize import linprog\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595a1b3d",
   "metadata": {},
   "source": [
    "# Professor's code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ouNgXiXUTsHg",
   "metadata": {
    "id": "ouNgXiXUTsHg"
   },
   "source": [
    "## 1. Problem parameters and state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-7eN83nqTsHg",
   "metadata": {
    "id": "-7eN83nqTsHg"
   },
   "outputs": [],
   "source": [
    "MAX_CARS = 20\n",
    "A_MAX_MOVE = 5\n",
    "ACTIONS_ALL = np.arange(-A_MAX_MOVE, A_MAX_MOVE+1)\n",
    "\n",
    "LAMBDA_X1, LAMBDA_Y1 = 3.0, 3.0\n",
    "LAMBDA_X2, LAMBDA_Y2 = 4.0, 2.0\n",
    "RENT_REWARD = 10.0\n",
    "MOVE_COST = 2.0\n",
    "\n",
    "STATES = [(n1, n2) for n1 in range(MAX_CARS+1) for n2 in range(MAX_CARS+1)]\n",
    "S_index = {s: i for i, s in enumerate(STATES)}\n",
    "NUM_STATES = len(STATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vf6CknOTTsHh",
   "metadata": {
    "id": "Vf6CknOTTsHh"
   },
   "source": [
    "## 2. Feasible actions and Poisson helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QIxKTZbeTsHh",
   "metadata": {
    "id": "QIxKTZbeTsHh"
   },
   "outputs": [],
   "source": [
    "def feasible_actions(n1, n2):\n",
    "    feas = []\n",
    "    for a in ACTIONS_ALL:\n",
    "        if a >= 0:\n",
    "            if n1 >= a and n2 + a <= MAX_CARS:\n",
    "                feas.append(a)\n",
    "        else:\n",
    "            if n2 >= -a and n1 - a <= MAX_CARS:\n",
    "                feas.append(a)\n",
    "    return feas\n",
    "\n",
    "FEAS_ACTIONS = [feasible_actions(n1, n2) for (n1, n2) in STATES]\n",
    "\n",
    "def poisson_pmf(lam, k):\n",
    "    if k < 0:\n",
    "        return 0.0\n",
    "    return math.exp(-lam) * (lam**k) / math.factorial(k)\n",
    "\n",
    "def poisson_cdf(lam, k):\n",
    "    return sum(poisson_pmf(lam, i) for i in range(k+1))\n",
    "\n",
    "def poisson_tail_prob(lam, k):\n",
    "    if k <= 0:\n",
    "        return 1.0\n",
    "    return 1.0 - poisson_cdf(lam, k-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lfySJKeMTsHh",
   "metadata": {
    "id": "lfySJKeMTsHh"
   },
   "source": [
    "## 3. Transition and reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5Svdnj_TsHh",
   "metadata": {
    "id": "b5Svdnj_TsHh"
   },
   "outputs": [],
   "source": [
    "RENTAL_CACHE = {}\n",
    "NEXTCOUNT_CACHE = {}\n",
    "\n",
    "def rental_probabilities(c, lam_x):\n",
    "    probs = np.zeros(c+1)\n",
    "    if c == 0:\n",
    "        probs[0] = 1.0\n",
    "        return probs\n",
    "    for r in range(c):\n",
    "        probs[r] = poisson_pmf(lam_x, r)\n",
    "    probs[c] = poisson_tail_prob(lam_x, c)\n",
    "    return probs / probs.sum()\n",
    "\n",
    "def next_count_distribution_given_rentals(c, r, lam_y):\n",
    "    remaining = c - r\n",
    "    probs = np.zeros(MAX_CARS+1)\n",
    "    for nprime in range(MAX_CARS):\n",
    "        y = nprime - remaining\n",
    "        if y >= 0:\n",
    "            probs[nprime] = poisson_pmf(lam_y, y)\n",
    "    tail_needed = MAX_CARS - remaining\n",
    "    probs[MAX_CARS] = poisson_tail_prob(lam_y, max(tail_needed, 0))\n",
    "    return probs / probs.sum()\n",
    "\n",
    "def get_rental_probs(c, lam_x):\n",
    "    key = (c, lam_x)\n",
    "    if key not in RENTAL_CACHE:\n",
    "        RENTAL_CACHE[key] = rental_probabilities(c, lam_x)\n",
    "    return RENTAL_CACHE[key]\n",
    "\n",
    "def get_nextcount_probs(c, r, lam_y):\n",
    "    key = (c, r, lam_y)\n",
    "    if key not in NEXTCOUNT_CACHE:\n",
    "        NEXTCOUNT_CACHE[key] = next_count_distribution_given_rentals(c, r, lam_y)\n",
    "    return NEXTCOUNT_CACHE[key]\n",
    "\n",
    "def post_move_counts(n1, n2, a):\n",
    "    c1 = n1 - a\n",
    "    c2 = n2 + a\n",
    "    return c1, c2\n",
    "\n",
    "def expected_rentals_for_lot(c, lam_x):\n",
    "    probs = get_rental_probs(c, lam_x)\n",
    "    support = np.arange(len(probs))\n",
    "    return (support * probs).sum()\n",
    "\n",
    "def transition_and_reward_for_state_action(n1, n2, a):\n",
    "    c1, c2 = post_move_counts(n1, n2, a)\n",
    "    e_r1 = expected_rentals_for_lot(c1, LAMBDA_X1)\n",
    "    e_r2 = expected_rentals_for_lot(c2, LAMBDA_X2)\n",
    "    reward = RENT_REWARD * (e_r1 + e_r2) - MOVE_COST * abs(a)\n",
    "    r1_probs = get_rental_probs(c1, LAMBDA_X1)\n",
    "    r2_probs = get_rental_probs(c2, LAMBDA_X2)\n",
    "    n1prime_given_r1 = [get_nextcount_probs(c1, r1, LAMBDA_Y1) for r1 in range(c1+1)]\n",
    "    n2prime_given_r2 = [get_nextcount_probs(c2, r2, LAMBDA_Y2) for r2 in range(c2+1)]\n",
    "    P_row = np.zeros(NUM_STATES)\n",
    "    for r1 in range(c1+1):\n",
    "        for r2 in range(c2+1):\n",
    "            p_r = r1_probs[r1] * r2_probs[r2]\n",
    "            joint = np.outer(n1prime_given_r1[r1], n2prime_given_r2[r2]) * p_r\n",
    "            for n1p in range(MAX_CARS+1):\n",
    "                base = n1p * (MAX_CARS+1)\n",
    "                P_row[base:base + (MAX_CARS+1)] += joint[n1p, :]\n",
    "    return P_row, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UkhohzPLTsHi",
   "metadata": {
    "id": "UkhohzPLTsHi"
   },
   "source": [
    "## 4. Average-reward LP (occupancy measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3vlHoLKqTsHi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "3vlHoLKqTsHi",
    "outputId": "5ca770a0-9928-4fd4-89ae-42b7adb31cd6"
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "R_vec = []\n",
    "P_dense_rows = []\n",
    "\n",
    "for s_idx, (n1, n2) in enumerate(STATES):\n",
    "    for a in FEAS_ACTIONS[s_idx]:\n",
    "        P_row, R_sa = transition_and_reward_for_state_action(n1, n2, a)\n",
    "        pairs.append((s_idx, a))\n",
    "        R_vec.append(R_sa)\n",
    "        P_dense_rows.append(P_row)\n",
    "\n",
    "K = len(pairs)\n",
    "R_vec = np.array(R_vec)\n",
    "P_dense = np.vstack(P_dense_rows)\n",
    "\n",
    "A_eq = np.zeros((NUM_STATES+1, K))\n",
    "b_eq = np.zeros(NUM_STATES+1)\n",
    "for k, (s_idx, a) in enumerate(pairs):\n",
    "    A_eq[s_idx, k] += 1.0\n",
    "    A_eq[:NUM_STATES, k] -= P_dense[k, :]\n",
    "A_eq[NUM_STATES, :] = 1.0\n",
    "b_eq[NUM_STATES] = 1.0\n",
    "\n",
    "res = linprog(-R_vec, A_eq=A_eq, b_eq=b_eq, bounds=[(0,None)]*K, method='highs')\n",
    "z = res.x\n",
    "avg_reward = R_vec @ z\n",
    "print(f\"Optimal average reward per day: {avg_reward:.4f}\")\n",
    "\n",
    "pi = {s_idx: {} for s_idx in range(NUM_STATES)}\n",
    "z_state = np.zeros(NUM_STATES)\n",
    "for k, (s_idx, a) in enumerate(pairs):\n",
    "    pi[s_idx][a] = z[k]\n",
    "    z_state[s_idx] += z[k]\n",
    "avg_policy_actions = np.zeros(NUM_STATES, dtype=int)\n",
    "for s_idx in range(NUM_STATES):\n",
    "    if z_state[s_idx] > 0:\n",
    "        avg_policy_actions[s_idx] = max(pi[s_idx].items(), key=lambda kv: kv[1])[0]\n",
    "    else:\n",
    "        feas = FEAS_ACTIONS[s_idx]\n",
    "        avg_policy_actions[s_idx] = min(feas, key=abs) if feas else 0\n",
    "\n",
    "plt.imshow(avg_policy_actions.reshape((MAX_CARS+1, MAX_CARS+1)), origin='lower')\n",
    "plt.title('Average Reward Policy (argmax z)')\n",
    "plt.xlabel('N2'); plt.ylabel('N1'); plt.colorbar(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oRjP-r_zTsHi",
   "metadata": {
    "id": "oRjP-r_zTsHi"
   },
   "source": [
    "## 5. Discounted LP for a ladder of $\\gamma$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qbNzgXJHTsHi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbNzgXJHTsHi",
    "outputId": "7adfd91d-0215-4986-fd75-aa58f4ba0798"
   },
   "outputs": [],
   "source": [
    "def solve_discounted_lp_and_policy(gamma):\n",
    "    A = np.zeros((K, NUM_STATES))\n",
    "    b = np.zeros(K)\n",
    "    for k, (s_idx, a) in enumerate(pairs):\n",
    "        A[k, :] = gamma * P_dense[k, :]\n",
    "        A[k, s_idx] -= 1.0\n",
    "        b[k] = -R_vec[k]\n",
    "    res = linprog(np.ones(NUM_STATES), A_ub=A, b_ub=b, bounds=[(None,None)]*NUM_STATES, method='highs')\n",
    "    v = res.x\n",
    "    policy = np.zeros(NUM_STATES, dtype=int)\n",
    "    for s_idx, (n1, n2) in enumerate(STATES):\n",
    "        best_q, best_a = -1e300, 0\n",
    "        for a in FEAS_ACTIONS[s_idx]:\n",
    "            k = next(k for k,(ss,aa) in enumerate(pairs) if ss==s_idx and aa==a)\n",
    "            q = R_vec[k] + gamma * (P_dense[k,:] @ v)\n",
    "            if q > best_q:\n",
    "                best_q, best_a = q, a\n",
    "        policy[s_idx] = best_a\n",
    "    return policy\n",
    "\n",
    "gammas = [0.95, 0.97, 0.98, 0.99, 0.995, 0.997, 0.999]\n",
    "policies_disc = {g: solve_discounted_lp_and_policy(g) for g in gammas}\n",
    "rates = {g: np.mean(policies_disc[g]==avg_policy_actions) for g in gammas}\n",
    "for g in gammas:\n",
    "    print(f\"γ={g}: match rate = {rates[g]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6IL5pj0NTsHi",
   "metadata": {
    "id": "6IL5pj0NTsHi"
   },
   "source": [
    "## 6. Visualization of Blackwell phenomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "px5Sk3ZLTsHj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "px5Sk3ZLTsHj",
    "outputId": "c24758cb-ed6e-42ed-987b-8b62d663850e"
   },
   "outputs": [],
   "source": [
    "cols = len(gammas)+1\n",
    "plt.figure(figsize=(3*cols,5))\n",
    "ax = plt.subplot(1,cols,1)\n",
    "ax.imshow(avg_policy_actions.reshape((MAX_CARS+1, MAX_CARS+1)), origin='lower')\n",
    "ax.set_title('Average')\n",
    "\n",
    "for j,g in enumerate(gammas, start=2):\n",
    "    ax = plt.subplot(1,cols,j)\n",
    "    ax.imshow(policies_disc[g].reshape((MAX_CARS+1, MAX_CARS+1)), origin='lower')\n",
    "    ax.set_title(f\"γ={g}\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53056ce9",
   "metadata": {},
   "source": [
    "# Our code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vaqq6yrmvme",
   "metadata": {},
   "source": [
    "## 7. Optimized Implementation Using Pre-computed Tensors\n",
    "\n",
    "This section reimplements the LP formulations using optimizations from the policy/value iteration approach:\n",
    "- Pre-computed transition probability tensor\n",
    "- Pre-computed expected rentals lookup tables  \n",
    "- Pre-computed reward matrix\n",
    "- MDP class with no gamma dependency (reusable for gamma ladder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04eccvbzi",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "\tdef __init__(self, action_range: tuple[int,int], location_capacity: tuple[int,int], \n",
    "\t             rental_params: tuple[int,int], return_params: tuple[int,int]):\n",
    "\t\t\"\"\"\n",
    "\t\tInitializes the MDP environment for the car rental problem.\n",
    "\t\tNote: gamma is NOT a parameter - it's an algorithm parameter, not environment parameter.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\taction_range: A tuple (min_action, max_action) defining the inclusive range of actions.\n",
    "\t\t\tlocation_capacity: The maximum car capacity for each location, e.g., (20, 20).\n",
    "\t\t\trental_params: The average rental requests (λ) for each location.\n",
    "\t\t\treturn_params: The average car returns (λ) for each location.\n",
    "\t\t\"\"\"\n",
    "\t\tself.action_range = action_range\n",
    "\t\tself.actions = np.arange(self.action_range[0], self.action_range[1] + 1)\n",
    "\t\tself.location_capacity = location_capacity\n",
    "\t\tself.rental_params = rental_params\n",
    "\t\tself.return_params = return_params\n",
    "\n",
    "\t\t# Pre-compute expensive operations once\n",
    "\t\tprint(\"Pre-computing transition probability matrix...\")\n",
    "\t\tself.transition_prob_matrix = self.build_transition_prob_matrix()\n",
    "\t\tprint(\"Pre-computing expected reward matrix...\")\n",
    "\t\tself.expected_reward_matrix = self.build_expected_reward_array()\n",
    "\t\tprint(\"MDP initialization complete!\")\n",
    "\n",
    "\t# 1 ---- Compute transition matrix ----\n",
    "\t@staticmethod\n",
    "\tdef compute_single_location_transition_prob(cars_morning: int, cars_evening: int, \n",
    "\t                                            capacity: int, lambda_rent: float, \n",
    "\t                                            lambda_return: float) -> float:\n",
    "\t\t\"\"\"Calculates the probability of transitioning from cars_morning to cars_evening at a single location.\"\"\"\n",
    "\t\tif cars_evening > capacity or cars_evening < 0:\n",
    "\t\t\treturn 0.0\n",
    "\n",
    "\t\ttotal_prob = 0.0\n",
    "\n",
    "\t\tfor cars_rented in range(cars_morning + 1):\n",
    "\t\t\tif cars_rented < cars_morning:\n",
    "\t\t\t\tprob_rentals = poisson.pmf(cars_rented, lambda_rent)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprob_rentals = poisson.sf(cars_rented - 1, lambda_rent)\n",
    "\n",
    "\t\t\tcars_after_rentals = cars_morning - cars_rented\n",
    "\t\t\treturns_needed = cars_evening - cars_after_rentals\n",
    "\n",
    "\t\t\tif returns_needed < 0:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tif cars_evening < capacity:\n",
    "\t\t\t\tprob_returns = poisson.pmf(returns_needed, lambda_return)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprob_returns = poisson.sf(returns_needed - 1, lambda_return)\n",
    "\n",
    "\t\t\ttotal_prob += prob_rentals * prob_returns\n",
    "\n",
    "\t\treturn total_prob\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build_single_location_transition_prob_matrix(capacity: int, lambda_rent: float, \n",
    "\t                                                  lambda_return: float) -> np.ndarray:\n",
    "\t\t\"\"\"Builds the (capacity+1, capacity+1) state transition matrix for a single location.\"\"\"\n",
    "\t\tnum_states = capacity + 1\n",
    "\t\ttransition_matrix = np.zeros((num_states, num_states))\n",
    "\n",
    "\t\tfor morning_state in range(num_states):\n",
    "\t\t\tfor evening_state in range(num_states):\n",
    "\t\t\t\tprob = MDP.compute_single_location_transition_prob(\n",
    "\t\t\t\t\tmorning_state, evening_state, capacity, lambda_rent, lambda_return\n",
    "\t\t\t\t)\n",
    "\t\t\t\ttransition_matrix[morning_state, evening_state] = prob\n",
    "\n",
    "\t\treturn transition_matrix\n",
    "\n",
    "\tdef build_transition_prob_matrix(self) -> np.ndarray:\n",
    "\t\t\"\"\"\n",
    "\t\tBuilds the 4D transition probability tensor P[m1, m2, e1, e2].\n",
    "\t\tThis tensor gives the probability of transitioning from a morning state (m1, m2)\n",
    "\t\tto an evening state (e1, e2).\n",
    "\t\t\"\"\"\n",
    "\t\tp_loc1 = MDP.build_single_location_transition_prob_matrix(\n",
    "\t\t\tself.location_capacity[0], self.rental_params[0], self.return_params[0]\n",
    "\t\t)\n",
    "\t\tp_loc2 = MDP.build_single_location_transition_prob_matrix(\n",
    "\t\t\tself.location_capacity[1], self.rental_params[1], self.return_params[1]\n",
    "\t\t)\n",
    "\n",
    "\t\t# Combine the matrices into a 4D tensor using an outer product.\n",
    "\t\t# P[m1, m2, e1, e2] = p_loc1[m1, e1] * p_loc2[m2, e2]\n",
    "\t\ttransition_tensor = np.einsum('ik,jl->ijkl', p_loc1, p_loc2)\n",
    "\n",
    "\t\treturn transition_tensor\n",
    "\n",
    "\t# 2 ---- Compute reward matrix ----\n",
    "\t@staticmethod\n",
    "\tdef build_expected_rentals_array(capacity: int, lambda_param: float) -> np.ndarray:\n",
    "\t\t\"\"\"Creates an array for expected rentals.\"\"\"\n",
    "\t\texpected_rentals = np.zeros(capacity + 1)\n",
    "\n",
    "\t\tfor available_cars in range(capacity + 1):\n",
    "\t\t\tcurrent_expected_rentals = 0.0\n",
    "\t\t\tfor demand in range(available_cars + 1):\n",
    "\t\t\t\tprob = poisson.pmf(demand, lambda_param)\n",
    "\t\t\t\tcurrent_expected_rentals += demand * prob\n",
    "\n",
    "\t\t\t# Whenever demand > available_cars, exactly available_cars cars are rented.\n",
    "\t\t\ttail_prob = poisson.sf(available_cars, lambda_param)\n",
    "\t\t\tcurrent_expected_rentals += available_cars * tail_prob\n",
    "\n",
    "\t\t\texpected_rentals[available_cars] = current_expected_rentals\n",
    "\n",
    "\t\treturn expected_rentals\n",
    "\n",
    "\tdef build_expected_reward_array(self) -> np.ndarray:\n",
    "\t\t\"\"\"\n",
    "\t\tBuilds the 3D reward array R[action, c1, c2] using pre-computed lookup tables.\n",
    "\t\t\"\"\"\n",
    "\t\tcap1, cap2 = self.location_capacity\n",
    "\t\tnum_actions = len(self.actions)\n",
    "\n",
    "\t\t# Pre-compute the expected rentals for all possible car counts at each location.\n",
    "\t\texpected_rentals_loc1 = MDP.build_expected_rentals_array(cap1, self.rental_params[0])\n",
    "\t\texpected_rentals_loc2 = MDP.build_expected_rentals_array(cap2, self.rental_params[1])\n",
    "\n",
    "\t\treward_array = np.zeros((num_actions, cap1 + 1, cap2 + 1))\n",
    "\n",
    "\t\tfor a_idx, action in enumerate(self.actions):\n",
    "\t\t\tfor c1 in range(cap1 + 1):\n",
    "\t\t\t\tfor c2 in range(cap2 + 1):\n",
    "\t\t\t\t\tcars_morn_loc1 = c1 - action\n",
    "\t\t\t\t\tcars_morn_loc2 = c2 + action\n",
    "\n",
    "\t\t\t\t\t# If the action is invalid, set the reward to -np.inf.\n",
    "\t\t\t\t\tif not (0 <= cars_morn_loc1 <= cap1 and 0 <= cars_morn_loc2 <= cap2):\n",
    "\t\t\t\t\t\treward_array[a_idx, c1, c2] = -np.inf\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Use the fast lookup tables instead of recalculating\n",
    "\t\t\t\t\t\trentals1 = expected_rentals_loc1[cars_morn_loc1]\n",
    "\t\t\t\t\t\trentals2 = expected_rentals_loc2[cars_morn_loc2]\n",
    "\n",
    "\t\t\t\t\t\trevenue = RENT_REWARD * (rentals1 + rentals2)\n",
    "\t\t\t\t\t\tcost = MOVE_COST * abs(action)\n",
    "\t\t\t\t\t\treward_array[a_idx, c1, c2] = revenue - cost\n",
    "\n",
    "\t\treturn reward_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6y2xzy9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MDP instance - expensive pre-computation happens here\n",
    "mdp_opt = MDP(\n",
    "    action_range=(-A_MAX_MOVE, A_MAX_MOVE),\n",
    "    location_capacity=(MAX_CARS, MAX_CARS),\n",
    "    rental_params=(LAMBDA_X1, LAMBDA_X2),\n",
    "    return_params=(LAMBDA_Y1, LAMBDA_Y2)\n",
    ")\n",
    "\n",
    "# Build feasible state-action pairs using pre-computed tensors\n",
    "pairs_opt = []\n",
    "R_vec_opt = []\n",
    "P_dense_rows_opt = []\n",
    "\n",
    "print(f\"Building LP structures from pre-computed tensors...\")\n",
    "for s_idx, (n1, n2) in enumerate(STATES):\n",
    "    for a in FEAS_ACTIONS[s_idx]:\n",
    "        # Compute morning state after action\n",
    "        m1, m2 = n1 - a, n2 + a\n",
    "        \n",
    "        # Extract transition probabilities from pre-computed tensor (just array indexing!)\n",
    "        P_row = mdp_opt.transition_prob_matrix[m1, m2, :, :].flatten()\n",
    "        \n",
    "        # Extract reward from pre-computed array (just array indexing!)\n",
    "        a_idx = a - mdp_opt.action_range[0]  # Convert action to index\n",
    "        R_sa = mdp_opt.expected_reward_matrix[a_idx, n1, n2]\n",
    "        \n",
    "        pairs_opt.append((s_idx, a))\n",
    "        R_vec_opt.append(R_sa)\n",
    "        P_dense_rows_opt.append(P_row)\n",
    "\n",
    "K_opt = len(pairs_opt)\n",
    "R_vec_opt = np.array(R_vec_opt)\n",
    "P_dense_opt = np.vstack(P_dense_rows_opt)\n",
    "\n",
    "print(f\"Built {K_opt} state-action pairs\")\n",
    "print(f\"P_dense shape: {P_dense_opt.shape}\")\n",
    "print(f\"R_vec shape: {R_vec_opt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd881qc5jdu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve average-reward LP using optimized structures\n",
    "A_eq_opt = np.zeros((NUM_STATES+1, K_opt))\n",
    "b_eq_opt = np.zeros(NUM_STATES+1)\n",
    "\n",
    "# Flow balance constraints\n",
    "for k, (s_idx, a) in enumerate(pairs_opt):\n",
    "    A_eq_opt[s_idx, k] += 1.0\n",
    "    A_eq_opt[:NUM_STATES, k] -= P_dense_opt[k, :]\n",
    "\n",
    "# Normalization constraint\n",
    "A_eq_opt[NUM_STATES, :] = 1.0\n",
    "b_eq_opt[NUM_STATES] = 1.0\n",
    "\n",
    "print(\"Solving average-reward LP...\")\n",
    "res_opt = linprog(-R_vec_opt, A_eq=A_eq_opt, b_eq=b_eq_opt, bounds=[(0,None)]*K_opt, method='highs')\n",
    "z_opt = res_opt.x\n",
    "avg_reward_opt = R_vec_opt @ z_opt\n",
    "print(f\"Optimal average reward per day: {avg_reward_opt:.4f}\")\n",
    "\n",
    "# Extract policy\n",
    "pi_opt = {s_idx: {} for s_idx in range(NUM_STATES)}\n",
    "z_state_opt = np.zeros(NUM_STATES)\n",
    "for k, (s_idx, a) in enumerate(pairs_opt):\n",
    "    pi_opt[s_idx][a] = z_opt[k]\n",
    "    z_state_opt[s_idx] += z_opt[k]\n",
    "\n",
    "avg_policy_actions_opt = np.zeros(NUM_STATES, dtype=int)\n",
    "for s_idx in range(NUM_STATES):\n",
    "    if z_state_opt[s_idx] > 0:\n",
    "        avg_policy_actions_opt[s_idx] = max(pi_opt[s_idx].items(), key=lambda kv: kv[1])[0]\n",
    "    else:\n",
    "        feas = FEAS_ACTIONS[s_idx]\n",
    "        avg_policy_actions_opt[s_idx] = min(feas, key=abs) if feas else 0\n",
    "\n",
    "plt.imshow(avg_policy_actions_opt.reshape((MAX_CARS+1, MAX_CARS+1)), origin='lower')\n",
    "plt.title('Average Reward Policy (Optimized)')\n",
    "plt.xlabel('N2'); plt.ylabel('N1'); plt.colorbar(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rdazzsuollj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve discounted LP for gamma ladder - reusing the SAME MDP instance!\n",
    "def solve_discounted_lp_optimized(gamma):\n",
    "    \"\"\"Solve discounted LP using pre-computed tensors. No recomputation needed!\"\"\"\n",
    "    A = np.zeros((K_opt, NUM_STATES))\n",
    "    b = np.zeros(K_opt)\n",
    "    \n",
    "    for k, (s_idx, a) in enumerate(pairs_opt):\n",
    "        # Bellman constraint: v(s) >= r(s,a) + gamma * sum P(s'|s,a) v(s')\n",
    "        # Rearranged: -v(s) + gamma * sum P(s'|s,a) v(s') <= -r(s,a)\n",
    "        A[k, :] = gamma * P_dense_opt[k, :]\n",
    "        A[k, s_idx] -= 1.0\n",
    "        b[k] = -R_vec_opt[k]\n",
    "    \n",
    "    res = linprog(np.ones(NUM_STATES), A_ub=A, b_ub=b, \n",
    "                  bounds=[(None,None)]*NUM_STATES, method='highs')\n",
    "    v = res.x\n",
    "    \n",
    "    # Extract policy\n",
    "    policy = np.zeros(NUM_STATES, dtype=int)\n",
    "    for s_idx, (n1, n2) in enumerate(STATES):\n",
    "        best_q, best_a = -1e300, 0\n",
    "        for a in FEAS_ACTIONS[s_idx]:\n",
    "            k = next(k for k,(ss,aa) in enumerate(pairs_opt) if ss==s_idx and aa==a)\n",
    "            q = R_vec_opt[k] + gamma * (P_dense_opt[k,:] @ v)\n",
    "            if q > best_q:\n",
    "                best_q, best_a = q, a\n",
    "        policy[s_idx] = best_a\n",
    "    return policy\n",
    "\n",
    "gammas_opt = [0.95, 0.97, 0.98, 0.99, 0.995, 0.997, 0.999]\n",
    "policies_disc_opt = {}\n",
    "\n",
    "print(\"Solving discounted LPs for gamma ladder (reusing same MDP)...\")\n",
    "for g in gammas_opt:\n",
    "    print(f\"  Solving for gamma = {g}...\")\n",
    "    policies_disc_opt[g] = solve_discounted_lp_optimized(g)\n",
    "\n",
    "# Compare with average-reward policy\n",
    "rates_opt = {g: np.mean(policies_disc_opt[g]==avg_policy_actions_opt) for g in gammas_opt}\n",
    "for g in gammas_opt:\n",
    "    print(f\"γ={g}: match rate = {rates_opt[g]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07uwtask6wn4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Blackwell phenomenon with optimized implementation\n",
    "cols = len(gammas_opt)+1\n",
    "plt.figure(figsize=(3*cols,5))\n",
    "\n",
    "ax = plt.subplot(1,cols,1)\n",
    "ax.imshow(avg_policy_actions_opt.reshape((MAX_CARS+1, MAX_CARS+1)), origin='lower')\n",
    "ax.set_title('Average (Optimized)')\n",
    "ax.set_xlabel('N2')\n",
    "ax.set_ylabel('N1')\n",
    "\n",
    "for j,g in enumerate(gammas_opt, start=2):\n",
    "    ax = plt.subplot(1,cols,j)\n",
    "    ax.imshow(policies_disc_opt[g].reshape((MAX_CARS+1, MAX_CARS+1)), origin='lower')\n",
    "    ax.set_title(f\"γ={g}\")\n",
    "    ax.set_xlabel('N2')\n",
    "    ax.set_ylabel('N1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ya07abbdmj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify optimized results match original results\n",
    "print(\"\\n=== Comparison with Original Implementation ===\")\n",
    "print(f\"Average reward (original): {avg_reward:.4f}\")\n",
    "print(f\"Average reward (optimized): {avg_reward_opt:.4f}\")\n",
    "print(f\"Difference: {abs(avg_reward - avg_reward_opt):.6f}\")\n",
    "\n",
    "print(\"\\nAverage-reward policy agreement:\")\n",
    "policy_match = np.mean(avg_policy_actions == avg_policy_actions_opt)\n",
    "print(f\"  {policy_match*100:.1f}% of states have same action\")\n",
    "\n",
    "print(\"\\nDiscounted policy agreement for each gamma:\")\n",
    "for g in gammas:\n",
    "    if g in gammas_opt:\n",
    "        match = np.mean(policies_disc[g] == policies_disc_opt[g])\n",
    "        print(f\"  γ={g}: {match*100:.1f}% agreement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gj7ya95wzl",
   "metadata": {},
   "source": [
    "### Key Optimizations\n",
    "\n",
    "The optimized implementation provides significant speedups through:\n",
    "\n",
    "1. **Pre-computed Transition Tensor** (4D array): Instead of computing transition probabilities with nested loops over rental/return scenarios for each state-action pair, we compute a single 4D tensor once using `np.einsum` and then just index into it.\n",
    "\n",
    "2. **Pre-computed Expected Rentals Lookup Tables**: Rather than summing Poisson probabilities for each state-action pair, we compute expected rentals for all possible car counts (0-20) once and use O(1) lookups.\n",
    "\n",
    "3. **Pre-computed Reward Matrix** (3D array): All rewards computed once during initialization, then just indexed during LP construction.\n",
    "\n",
    "4. **scipy.stats.poisson**: Uses optimized C implementation instead of Python's `math.factorial()`.\n",
    "\n",
    "5. **Gamma-independent MDP**: The MDP class doesn't depend on gamma, so we build it once and reuse it for the entire gamma ladder (7 different gamma values), avoiding redundant computation.\n",
    "\n",
    "**Result**: The expensive pre-computation happens once in `MDP.__init__()`, then LP construction and solving are much faster due to simple array indexing instead of nested Poisson calculations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
