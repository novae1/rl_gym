{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TransformObservation\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_env(env):\n",
    "    env = TransformObservation(env, lambda x: torch.from_numpy(x).float(), env.observation_space)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = pytorch_env(gym.make('HalfCheetah-v5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot(data, title=\"\", xlabel=\"\", ylabel=\"\", grid=True, sleep=0.01):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(grid)\n",
    "    plt.show()\n",
    "    time.sleep(sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBSERVATIONS = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.shape[0]\n",
    "\n",
    "TEST = False\n",
    "TRAIN = True\n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(N_ACTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_agents = 4\n",
    "n_timesteps = 128\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "v = 1.0 # Value constant\n",
    "learning_rate = 3e-4\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "n_trainsteps = 20\n",
    "max_grad_norm = 0.5\n",
    "layer_dim = 64\n",
    "std = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aren't there some inconsistencies with the naming here (obs vs state_batch)? Also, get_distributions doesn't handle the case where only one observation is given as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, layer_dim):\n",
    "        \"\"\"\n",
    "        Actor network with learned standard deviations and vectorized operations.\n",
    "        \n",
    "        Args:\n",
    "            n_observations (int): Dimension of the observation space\n",
    "            n_actions (int): Dimension of the action space\n",
    "            layer_dim (int): Dimension of hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Network for computing mean of actions\n",
    "        self.mu_network = nn.Sequential(\n",
    "            nn.Linear(n_observations, layer_dim),\n",
    "            nn.LayerNorm(layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, layer_dim),\n",
    "            nn.LayerNorm(layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer_dim, n_actions)\n",
    "        )\n",
    "\n",
    "        # Learnable log stds\n",
    "        # Initialize to log(0.5) = -0.69\n",
    "        self.log_std = nn.Parameter(torch.ones(n_actions) * -0.69)\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Compute the mean actions for given observations.\n",
    "\n",
    "        Args:\n",
    "            obs (torch.Tensor): Observations, shape [batch_size, n_observations] or [n_observations] for a single observation\n",
    "        Returns:\n",
    "            torch.Tensor: Mean actions\n",
    "        \"\"\"\n",
    "        if obs.dim() == 1:\n",
    "            obs = obs.unsqueeze(0)\n",
    "        \n",
    "        means = self.mu_network(obs)\n",
    "        return means\n",
    "       \n",
    "    def get_distribution(self, state_batch):\n",
    "        \"\"\"\n",
    "        Create a batched MultivariateNormal distribution for the given states\n",
    "\n",
    "        Args:\n",
    "            state_batch (torch.Tensor): Batch of states, shape [batch_size, n_observations]\n",
    "        Returns:\n",
    "            torch.distributions.MultivariateNormal: Distribution with batched parameters\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            means = self.forward(state)\n",
    "        cov_matrix = torch.eye(self.n_actions) * (self.std ** 2)\n",
    "        action_distribution = torch.distributions.MultivariateNormal(means, cov_matrix)\n",
    "        action = action_distribution.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\tdef __init__(self, n_observations, layer_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.v_network = nn.Sequential(\n",
    "\t\t\tnn.Linear(n_observations, layer_dim),\n",
    "\t\t\tnn.LayerNorm(layer_dim),\n",
    "\t\t\tnn.Tanh(),\n",
    "\t\t\tnn.Linear(layer_dim, layer_dim),\n",
    "\t\t\tnn.LayerNorm(layer_dim),\n",
    "\t\t\tnn.Tanh(),\n",
    "\t\t\tnn.Linear(layer_dim, 1)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, obs):\n",
    "\t\tvalue = self.v_network(obs)\n",
    "\t\treturn value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(actor, env):\n",
    "\tepisode_reward = 0.\n",
    "\tstate, _ = env.reset()\n",
    "\tterminated, truncated = False, False\n",
    "\twhile not (terminated or truncated):\n",
    "\t\taction = actor.select_action(state)\n",
    "\t\tstate, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "\t\tepisode_reward += reward\n",
    "\tenv.close()\n",
    "\treturn episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPODataset(Dataset):\n",
    "\tdef __init__(self, n_agents, n_timesteps, n_observations, n_actions, gamma):\n",
    "\t\t# Store variables\n",
    "\t\tself.n_agents = n_agents\n",
    "\t\tself.n_timesteps = n_timesteps\n",
    "\t\tself.n_observations = n_observations\n",
    "\t\tself.n_actions = n_actions\n",
    "\t\tself.gamma = gamma\n",
    "\n",
    "\t\t# Create tensors\n",
    "\t\tself.states = torch.zeros((n_agents, n_timesteps, n_observations))\n",
    "\t\tself.actions = torch.zeros((n_agents, n_timesteps, N_ACTIONS))\n",
    "\t\tself.rewards = torch.zeros((n_agents, n_timesteps))\n",
    "\t\tself.logprobs = torch.zeros((n_agents, n_timesteps))\n",
    "\t\tself.pred_values = torch.zeros((n_agents, n_timesteps))\n",
    "\t\tself.target_values = torch.zeros((n_agents, n_timesteps))\n",
    "\t\tself.advantages = torch.zeros((n_agents, n_timesteps))\n",
    "\n",
    "\t\t# Store episode ends\n",
    "\t\tself.episode_ends = [[] for _ in range(n_agents)]\n",
    "\n",
    "\tdef add_step(self, agent, t, state, action, reward, logprob, pred_value):\n",
    "\t\tself.states[agent, t] = state\n",
    "\t\tself.actions[agent, t] = action\n",
    "\t\tself.rewards[agent, t] = reward\n",
    "\t\tself.logprobs[agent, t] = logprob\n",
    "\t\tself.pred_values[agent, t] = pred_value\n",
    "\n",
    "\tdef mark_episode_end(self, agent, t):\n",
    "\t\tself.episode_ends[agent].append(t)\n",
    "\n",
    "\tdef __compute_advantages_and_target_values(self):\n",
    "\t\tfor agent in range(self.n_agents):\n",
    "\t\t\t# Iterate from last to first\n",
    "\t\t\tfor t in range(self.n_timesteps-1, -1, -1):\n",
    "\t\t\t\t# Get reward and predicted value\n",
    "\t\t\t\treward = self.rewards[agent, t]\n",
    "\t\t\t\tpred_value = self.pred_values[agent, t]\n",
    "\n",
    "\t\t\t\t# If step is terminal (end of episode)\n",
    "\t\t\t\tif t in self.episode_ends[agent]:\n",
    "\t\t\t\t\ttarget_value = reward\n",
    "\t\t\t\t# If step is last (t = n_timesteps - 1) but not terminal\n",
    "\t\t\t\telif t == self.n_timesteps - 1:\n",
    "\t\t\t\t\ttarget_value = pred_value\n",
    "\t\t\t\t# non-terminal non-last step\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Get previous target value (of t+1) and compute\n",
    "\t\t\t\t\tprevious_target_value = self.target_values[agent][t+1]\n",
    "\t\t\t\t\ttarget_value = reward + self.gamma * previous_target_value\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Compute advantage\n",
    "\t\t\t\tadvantage = target_value - pred_value\n",
    "\n",
    "\t\t\t\t# Store target value and advantage\n",
    "\t\t\t\tself.target_values[agent, t] = target_value\n",
    "\t\t\t\tself.advantages[agent, t] = advantage\n",
    "\n",
    "\tdef __flatten_data(self):\n",
    "\t\t# Flatten all tensors that will be fetched\n",
    "\t\tself.states = self.states.view(-1, self.n_observations)\n",
    "\t\tself.actions = self.actions.view(-1)\n",
    "\t\tself.logprobs = self.logprobs.view(-1, self.n_actions)\n",
    "\t\tself.target_values = self.target_values.view(-1)\n",
    "\t\tself.advantages = self.advantages.view(-1)\n",
    "\n",
    "\tdef compute_advantages_and_target_values_and_flatten_data(self):\n",
    "\t\tself.__compute_advantages_and_target_values()\n",
    "\t\tself.__flatten_data()\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n_agents * self.n_timesteps\n",
    "\t\n",
    "\tdef __getitem__(self, i):\n",
    "\t\t# Don't use before calling compute_advantages_and_target_values and flatten_data\n",
    "\t\tstate = self.states[i]\n",
    "\t\taction = self.actions[i]\n",
    "\t\tlogprob = self.logprobs[i]\n",
    "\t\ttarget_value = self.target_values[i]\n",
    "\t\tadvantage = self.advantages[i]\n",
    "\n",
    "\t\treturn state, action, logprob, target_value, advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_objective_fn(pred_logprob_batch, old_logprob_batch, action_batch, advantage_batch, epsilon):\n",
    "\t# Compute p_ratio, ratio of new and old probabilities\n",
    "    p_ratio = torch.exp(pred_logprob_batch - old_logprob_batch)\n",
    "\n",
    "    # Compute unclipped and clipped surrogate objectives\n",
    "    unclipped_surrogate_objective = p_ratio * advantage_batch\n",
    "    clipped_surrogate_objective = torch.clamp(p_ratio, 1. - epsilon, 1. + epsilon) * advantage_batch\n",
    "\n",
    "    # Compute elementwise minimum of two and return\n",
    "    clipped_objective = torch.mean(torch.min(unclipped_surrogate_objective, clipped_surrogate_objective))\n",
    "    return clipped_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_loss_fn(pred_values, target_value_batch):\n",
    "    # Compute and return loss\n",
    "    smoothl1 = nn.SmoothL1Loss(reduction='mean')\n",
    "    loss = smoothl1(pred_values, target_value_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an unreadable mess add comments for the love of god"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(actor, critic, state_batch, action_batch, old_logprob_batch, target_value_batch, advantage_batch, epsilon, v):\n",
    "    # Get pred log probs and values\n",
    "    distribution_list = actor.distribution_list(state_batch)\n",
    "    pred_logprob_batch = []\n",
    "    for i in range(len(state_batch)):\n",
    "        distribution = distribution_list[i]\n",
    "        action = action_batch[i].unsqueeze(0)\n",
    "        pred_logprob = distribution.log_prob(action)\n",
    "        pred_logprob_batch.append(pred_logprob)\n",
    "    pred_logprob_batch = torch.stack(pred_logprob_batch)\n",
    "    pred_value_batch = critic(state_batch)\n",
    "\n",
    "    # Compute individual losses\n",
    "    clipped_objective = clipped_objective_fn(pred_logprob_batch, old_logprob_batch, action_batch, advantage_batch, epsilon)\n",
    "    value_loss = value_loss_fn(pred_value_batch, target_value_batch)\n",
    "\n",
    "    # Compute and return total loss\n",
    "    loss = - clipped_objective + v * value_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(actor, critic, optimizer, batch_size, n_epochs, n_trainsteps, n_agents, n_timesteps, n_observations, n_actions, gamma, epsilon, v, max_grad_norm):\n",
    "\t# Initialize rewards and losses\n",
    "\trewards = []\n",
    "\tlosses = []\n",
    "\n",
    "\t# Initilize all environments\n",
    "\tenvs = []\n",
    "\tfor agent in range(n_agents):\n",
    "\t\tenv = pytorch_env(gym.make('HalfCheetah-v5'))\n",
    "\t\tstate, _ = env.reset()\n",
    "\t\tterminated, truncated = False, False\n",
    "\t\tenvs.append([env, state, terminated, truncated])\n",
    "\t\t\t\n",
    "\tfor _ in range(n_trainsteps):\n",
    "\t\t# Initialize dataset\n",
    "\t\tdataset = PPODataset(n_agents, n_timesteps, n_observations, n_actions, gamma)\n",
    "\n",
    "\t\t# Collect data\n",
    "\t\tfor agent in range(n_agents):\n",
    "\t\t\tenv = envs[agent][0]\n",
    "\t\t\tstate, terminated, truncated = envs[agent][1:]\n",
    "\n",
    "\t\t\tfor t in range(n_timesteps):\n",
    "\t\t\t\t# THIS SUCKS I SHOULD HAVE A BETTER WAY TO COMPUTE DISTRIBUTION AND ACTION\n",
    "\t\t\t\t# But it works or at least seems to work\n",
    "\t\t\t\t# Compute distributions predicted value\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tpred_value = critic(state)\n",
    "\t\t\t\t\tdistribution = actor.distribution_list([state])[0]\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Select and perform action\n",
    "\t\t\t\taction = actor.select_action(state)\n",
    "\t\t\t\tnext_state, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "\n",
    "\t\t\t\t# Compute logprob\n",
    "\t\t\t\tlogprob = distribution.log_prob(action)\n",
    "\n",
    "\t\t\t\t# Store data\n",
    "\t\t\t\tdataset.add_step(agent, t, state, action, reward, logprob, pred_value)\n",
    "\n",
    "\t\t\t\t# If terminated reset env and mark end, otherwise update state\n",
    "\t\t\t\tif terminated or truncated:\n",
    "\t\t\t\t\tdataset.mark_episode_end(agent, t)\n",
    "\t\t\t\t\tstate, _ = env.reset()\n",
    "\t\t\t\t\tterminated, truncated = False, False\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tstate = next_state\n",
    "\n",
    "\t\t# Compute target values and advantages and flatten data\n",
    "\t\tdataset.compute_advantages_and_target_values_and_flatten_data()\n",
    "\n",
    "\t\t# Create dataloader\n",
    "\t\tdataloader = DataLoader(\n",
    "\t\t\tdataset=dataset,\n",
    "\t\t\tbatch_size=batch_size,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t)\n",
    "\n",
    "\t\t# Train model on policy for N_EPOCHS\n",
    "\t\tfor _ in range(n_epochs):\n",
    "\t\t\tfor state_batch, action_batch, logprob_batch, target_value_batch, advantage_batch in dataloader:\n",
    "\t\t\t\t# Compute loss and optimize\n",
    "\t\t\t\tloss = loss_fn(\n",
    "\t\t\t\t\tactor,\n",
    "\t\t\t\t\tcritic,\n",
    "\t\t\t\t\tstate_batch,\n",
    "\t\t\t\t\taction_batch,\n",
    "\t\t\t\t\tlogprob_batch,\n",
    "\t\t\t\t\ttarget_value_batch,\n",
    "\t\t\t\t\tadvantage_batch,\n",
    "\t\t\t\t\tepsilon,\n",
    "\t\t\t\t\tv,\n",
    "\t\t\t\t)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\t# Clip gradients DISABLED FOR NOW\n",
    "\t\t\t\t#clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t\t# Append losses\n",
    "\t\t\t\tlosses.append(loss.item())\n",
    "\n",
    "\t\t# Test model, append reward and display\n",
    "\t\ttest_reward = test_model(actor, env)\n",
    "\t\trewards.append(test_reward)\n",
    "\t\tupdate_plot(rewards, \"Test rewards\", \"Reward\", \"Train loop\")\n",
    "\n",
    "\treturn rewards, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be working now, although EXTREMELY slowly. So next I should fix the speed. But I'm really glad it's working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(N_OBSERVATIONS, N_ACTIONS, layer_dim, std)\n",
    "critic = Critic(N_OBSERVATIONS, layer_dim)\n",
    "optimizer = optim.Adam(chain(actor.parameters(), critic.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 6]' is invalid for input of size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN:\n\u001b[1;32m----> 2\u001b[0m     rewards, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trainsteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_agents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mN_OBSERVATIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mN_ACTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 50\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(actor, critic, optimizer, batch_size, n_epochs, n_trainsteps, n_agents, n_timesteps, n_observations, n_actions, gamma, epsilon, v, max_grad_norm)\u001b[0m\n\u001b[0;32m     47\u001b[0m \t\t\tstate \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Compute target values and advantages and flatten data\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_advantages_and_target_values_and_flatten_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Create dataloader\u001b[39;00m\n\u001b[0;32m     53\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     54\u001b[0m \tdataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m     55\u001b[0m \tbatch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     56\u001b[0m \tshuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m )\n",
      "Cell \u001b[1;32mIn[22], line 69\u001b[0m, in \u001b[0;36mPPODataset.compute_advantages_and_target_values_and_flatten_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_advantages_and_target_values_and_flatten_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     68\u001b[0m \t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__compute_advantages_and_target_values()\n\u001b[1;32m---> 69\u001b[0m \t\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__flatten_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 63\u001b[0m, in \u001b[0;36mPPODataset.__flatten_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_observations)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogprobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_values\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantages\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 6]' is invalid for input of size 512"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    rewards, losses = train_loop(\n",
    "        actor,\n",
    "        critic,\n",
    "        optimizer,\n",
    "        batch_size,\n",
    "        n_epochs,\n",
    "        n_trainsteps,\n",
    "        n_agents,\n",
    "        n_timesteps,\n",
    "        N_OBSERVATIONS,\n",
    "        N_ACTIONS,\n",
    "        gamma,\n",
    "        epsilon,\n",
    "        v,\n",
    "        max_grad_norm,\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(losses, label=\"losses\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.0"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(actor, pytorch_env(gym.make('HalfCheetah-v5', render_mode='human')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 242\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# Create and train PPO agent\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     ppo \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHalfCheetah-v5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 242\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# Plot final results\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[1;32mIn[2], line 216\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, n_iterations)\u001b[0m\n\u001b[0;32m    213\u001b[0m states, actions, rewards, log_probs, values, dones, last_value, episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect_rollout()\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Compute returns and advantages\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_returns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m advantages \u001b[38;5;241m=\u001b[39m returns \u001b[38;5;241m-\u001b[39m values\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Normalize advantages\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 104\u001b[0m, in \u001b[0;36mPPO.compute_returns\u001b[1;34m(self, rewards, dones, last_value)\u001b[0m\n\u001b[0;32m    102\u001b[0m running_return \u001b[38;5;241m=\u001b[39m last_value\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reward, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(rewards), \u001b[38;5;28mreversed\u001b[39m(dones)):\n\u001b[1;32m--> 104\u001b[0m     running_return \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m running_return \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m)\n\u001b[0;32m    105\u001b[0m     returns\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, running_return)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(returns)\n",
      "File \u001b[1;32mc:\\Users\\novae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:41\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\novae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:962\u001b[0m, in \u001b[0;36mTensor.__rsub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;129m@_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_VariableFunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TransformObservation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import sympy\n",
    "\n",
    "def pytorch_env(env):\n",
    "    \"\"\"Convert environment observations to PyTorch tensors.\"\"\"\n",
    "    return TransformObservation(env, lambda x: torch.from_numpy(x).float(), env.observation_space)\n",
    "\n",
    "def update_plot(data, title=\"\", xlabel=\"\", ylabel=\"\", grid=True, sleep=0.01):\n",
    "    \"\"\"Update training progress plot.\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(grid)\n",
    "    plt.show()\n",
    "    time.sleep(sleep)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_observations, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_actions),\n",
    "        )\n",
    "        # Initialize log_std outside the network\n",
    "        self.log_std = nn.Parameter(torch.zeros(n_actions))\n",
    "        \n",
    "    def forward(self, state):\n",
    "        mean = self.network(state)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        mean, std = self(state)\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        action = normal.sample()\n",
    "        log_prob = normal.log_prob(action).sum(-1)\n",
    "        return action, log_prob\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_observations, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_observations, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.network(state).squeeze(-1)\n",
    "\n",
    "class PPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_name,\n",
    "        hidden_dim=64,\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        epsilon=0.2,\n",
    "        value_coef=0.5,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        n_epochs=10\n",
    "    ):\n",
    "        # Initialize environment\n",
    "        self.env = pytorch_env(gym.make(env_name))\n",
    "        self.n_observations = self.env.observation_space.shape[0]\n",
    "        self.n_actions = self.env.action_space.shape[0]\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.actor = Actor(self.n_observations, self.n_actions, hidden_dim)\n",
    "        self.critic = Critic(self.n_observations, hidden_dim)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.n_steps = n_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "    def compute_returns(self, rewards, dones, last_value):\n",
    "        \"\"\"Compute returns with TD(λ) estimation.\"\"\"\n",
    "        returns = []\n",
    "        running_return = last_value\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            running_return = reward + self.gamma * running_return * (1 - done)\n",
    "            returns.insert(0, running_return)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def collect_rollout(self):\n",
    "        \"\"\"Collect experience data.\"\"\"\n",
    "        states, actions, rewards, log_probs, values, dones = [], [], [], [], [], []\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        \n",
    "        # Collect steps\n",
    "        episode_reward = 0\n",
    "        for _ in range(self.n_steps):\n",
    "            # Get action and value\n",
    "            with torch.no_grad():\n",
    "                action, log_prob = self.actor.sample_action(state)\n",
    "                value = self.critic(state)\n",
    "            \n",
    "            # Take step in environment\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action.numpy())\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            dones.append(done)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "                done = False\n",
    "            else:\n",
    "                state = next_state\n",
    "        \n",
    "        # Get final value for return computation\n",
    "        with torch.no_grad():\n",
    "            last_value = self.critic(state)\n",
    "        \n",
    "        return (\n",
    "            torch.stack(states),\n",
    "            torch.stack(actions),\n",
    "            torch.tensor(rewards),\n",
    "            torch.stack(log_probs),\n",
    "            torch.stack(values),\n",
    "            torch.tensor(dones),\n",
    "            last_value,\n",
    "            episode_reward\n",
    "        )\n",
    "\n",
    "    def update_policy(self, states, actions, old_log_probs, returns, advantages):\n",
    "        \"\"\"Update policy using PPO objective.\"\"\"\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32)\n",
    "        old_log_probs = torch.tensor(old_log_probs, dtype=torch.float32)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "        \n",
    "        for _ in range(self.n_epochs):\n",
    "            # Generate random indices\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            # Update in mini-batches\n",
    "            for start in range(0, len(states), self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                # Get batch data\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                \n",
    "                # Get current policy outputs\n",
    "                mean, std = self.actor(batch_states)\n",
    "                dist = torch.distributions.Normal(mean, std)\n",
    "                new_log_probs = dist.log_prob(batch_actions).sum(-1)\n",
    "                \n",
    "                # Compute ratio and surrogate objectives\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * batch_advantages\n",
    "                \n",
    "                # Compute actor and critic losses\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                critic_pred = self.critic(batch_states)\n",
    "                critic_loss = self.value_coef * nn.MSELoss()(critic_pred, batch_returns)\n",
    "                \n",
    "                # Update actor\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                \n",
    "                # Update critic\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "    def train(self, n_iterations=100):\n",
    "        \"\"\"Main training loop.\"\"\"\n",
    "        rewards_history = []\n",
    "        \n",
    "        for iteration in range(n_iterations):\n",
    "            # Collect experience\n",
    "            states, actions, rewards, log_probs, values, dones, last_value, episode_reward = self.collect_rollout()\n",
    "            \n",
    "            # Compute returns and advantages\n",
    "            returns = self.compute_returns(rewards, dones, last_value)\n",
    "            advantages = returns - values\n",
    "            \n",
    "            # Normalize advantages\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "            # Update policy\n",
    "            self.update_policy(states, actions, log_probs, returns, advantages)\n",
    "            \n",
    "            # Store rewards and update plot\n",
    "            rewards_history.append(episode_reward)\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                update_plot(\n",
    "                    rewards_history,\n",
    "                    \"Training Progress\",\n",
    "                    \"Iteration\",\n",
    "                    \"Episode Reward\"\n",
    "                )\n",
    "                print(f\"Iteration {iteration + 1}, Reward: {episode_reward:.2f}\")\n",
    "        \n",
    "        return rewards_history\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and train PPO agent\n",
    "    ppo = PPO(\"HalfCheetah-v5\")\n",
    "    rewards = ppo.train(n_iterations=100)\n",
    "    \n",
    "    # Plot final results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Training Results\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Episode Reward\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
