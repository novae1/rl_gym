{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO para Fine-tuning do Gemma-3 1B no GSM8K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O **GRPO (Group Relative Policy Optimization)** é um algoritmo de Aprendizado por Reforço para treinar LLMs, introduzido pela DeepSeek-AI no artigo *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models* como alternativa ao PPO. Ele preserva a ideia de atualização *clipped* e a regularização por divergência **KL**, mas elimina a necessidade de um *critic* (value network).\n",
    "\n",
    "Em vez de usar uma rede de valor para calcular vantagem token-a-token, o GRPO gera um **grupo** de respostas para cada prompt e estima a vantagem a partir da média e do desvio-padrão das recompensas do grupo. Isso reduz custo computacional e memória. Neste projeto aplicamos GRPO para melhorar o desempenho do Gemma3-1B no GSM8K.\n",
    "\n",
    "Este projeto foi inspirado no notebook da Unsloth **`https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(1B)-GRPO.ipynb`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup do ambiente e bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliotecas principais:\n",
    "\n",
    "- **Unsloth**: carregamento eficiente do Gemma-3 e aplicação de LoRA.\n",
    "- **TRL**: `GRPOTrainer` e `GRPOConfig`.\n",
    "- **Transformers / PEFT**: tokenização, geração e carregamento de adaptadores LoRA.\n",
    "- **Datasets**: carregamento do GSM8K.\n",
    "- **PyTorch**: backend do treino e da avaliação.\n",
    "\n",
    "Os parâmetros foram escolhidos para uma **RTX 4090**, para usar bastante VRAM (batch alto e `num_generations` relativamente grande). No entanto, os parâmetros do notebook atual são diferentes daqueles usados durante o treino. Isso foi feito para que seja possível executar o notebook inteiro dentro de um intervalo de tempo razoável.\n",
    "\n",
    "**OBS**: Pode ser necessário ter uma GPU para rodar esse notebook. Caso contrário, a biblioteca Unsloth retornará erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unsloth vllm==0.10.2 torchvision bitsandbytes xformers triton transformers==4.56.2 trl==0.22.2 datasets matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "from unsloth import FastModel\n",
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import TextStreamer\n",
    "from peft import PeftModel\n",
    "\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "import builtins\n",
    "from peft.tuners.lora import layer\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda disponível:\", torch.cuda.is_available())\n",
    "\n",
    "builtins.VARIANT_KWARG_KEYS = layer.VARIANT_KWARG_KEYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prompt e pré-processamento do GSM8K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset **GSM8K** contém problemas de matemática básica. As respostas vêm com o resultado final depois de `####`.  \n",
    "No GRPO, transformamos cada exemplo em um formato de chat: `system prompt` + pergunta como mensagem do usuário.\n",
    "\n",
    "O *system prompt* força o modelo a:\n",
    "1. escrever um raciocínio entre `<start_working_out>` e `<end_working_out>`;\n",
    "2. escrever a solução final entre `<SOLUTION>` e `</SOLUTION>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens especiais\n",
    "reasoning_start = \"<start_working_out>\"\n",
    "reasoning_end   = \"<end_working_out>\"\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are given a problem.\\n\"\n",
    "    \"Think about the problem and provide your working out.\\n\"\n",
    "    f\"Place it between {reasoning_start} and {reasoning_end}.\\n\"\n",
    "    f\"Then, provide your solution between {solution_start}{solution_end}\"\n",
    ")\n",
    "\n",
    "# Extrai o número final do GSM8K (depois de ####)\n",
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# Mapeia dataset para formato de chat + answer numérica\n",
    "def prepare_dataset(dataset):\n",
    "    return dataset.map(lambda x: {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "        ],\n",
    "        \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "    })\n",
    "\n",
    "# Exemplo rápido\n",
    "raw_train = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "train_ds = prepare_dataset(raw_train)\n",
    "print(train_ds[0][\"prompt\"])\n",
    "print(\"answer:\", train_ds[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Funções de recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As recompensas são o núcleo do GRPO: elas avaliam cada geração do modelo. Usamos quatro funções:\n",
    "\n",
    "1. **`match_format_exactly`**: recompensa máxima se o modelo segue exatamente o formato com tags.\n",
    "2. **`match_format_approximately`**: recompensa parcial por aparecerem as tags corretas (penaliza falta/excesso).\n",
    "3. **`check_answer`**: checagem estrita do conteúdo dentro de `<SOLUTION>`.\n",
    "4. **`check_numbers`**: checagem mais leniente que aceita extração numérica em `$\\\\boxed{...}$` ou após `<SOLUTION>`.\n",
    "\n",
    "Isso força o modelo a aprender **forma** (estrutura) e **conteúdo** (acerto numérico). Durante o desenvolvimento, essas funções foram validadas com um script de testes em `test_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex para formato e extração de números\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\n",
    "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
    "    rf\"{solution_start}(.+?){solution_end}\"\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "match_numbers = re.compile(\n",
    "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "match_boxed = re.compile(\n",
    "    r\"\\$\\\\boxed\\{([^}]+)\\}\\$\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        if match_format.search(response) is not None:\n",
    "            score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Checagem estrita:\n",
    "    - espera resposta apenas entre <SOLUTION> tags.\n",
    "    - recompensa maior para match exato, menor para whitespace, menor ainda para ratio próximo.\n",
    "    \"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    extracted_responses = [\n",
    "        guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        if guess == true_answer:\n",
    "            score += 3.0\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   0.9 <= ratio <= 1.1: score += 0.5\n",
    "                elif 0.8 <= ratio <= 1.2: score += 0.25\n",
    "                else: score -= 1.0\n",
    "            except:\n",
    "                score -= 0.5\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Checagem leniente:\n",
    "    - procura primeiro por $\\\\boxed{...}$ (última ocorrência),\n",
    "    - senão extrai o primeiro número após <SOLUTION>.\n",
    "    \"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = []\n",
    "    for r in responses:\n",
    "        boxed_matches = match_boxed.findall(r)\n",
    "        if boxed_matches:\n",
    "            extracted_responses.append(boxed_matches[-1])\n",
    "            continue\n",
    "        guess = match_numbers.search(r)\n",
    "        extracted_responses.append(guess.group(1) if guess is not None else None)\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            guess = float(guess.strip())\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Treinamento com GRPO + LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O treino está no `train.py`. O fluxo é:\n",
    "\n",
    "1. Carregar o modelo base `unsloth/gemma-3-1b-it` em FP16.\n",
    "2. Adicionar LoRA (rank=8) nas camadas principais.\n",
    "3. Preparar o GSM8K `train`.\n",
    "4. Configurar GRPO (batch 48, `num_generations=8`, seq 2048).\n",
    "5. Treinar com `trainer.train()`.\n",
    "\n",
    "Setup experimental (como apresentado nos slides): GSM8K **train** com 7.473 problemas; Gemma-3 1B Instruct com LoRA (rank=8) e `max_seq_length=2048`; `per_device_train_batch_size=48` e `num_generations=8`. O treino foi feito em uma RTX 4090 por 1 época completa (~41h).\n",
    "\n",
    "**Observação:** em CPU esse treino não é viável; requer GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup do modelo\n",
    "max_seq_length = 2048\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    load_in_8bit=False,\n",
    "    load_in_16bit=True,\n",
    "    full_finetuning=False,\n",
    ")\n",
    "\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=False,\n",
    "    finetune_language_layers=True,\n",
    "    finetune_attention_modules=True,\n",
    "    finetune_mlp_modules=True,\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "# Carregamento dos dados e preparação\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "dataset = prepare_dataset(dataset)\n",
    "\n",
    "# Configuração de treino\n",
    "max_prompt_length = 256\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=1, # Numa 4090, coloque 48\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_generations=2, # Numa 4090, coloque 8\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_seq_length - max_prompt_length,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=3, # remova isso para treinar uma época inteira\n",
    "    save_steps=50,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"outputs\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Treino\n",
    "trainer.train()\n",
    "\n",
    "# Salvar modelo e tokenizer\n",
    "model.save_pretrained(\"gemma-3\")\n",
    "tokenizer.save_pretrained(\"gemma-3\")\n",
    "\n",
    "# Salvar log de treino (compatível com parse_and_plot_training_log)\n",
    "with open(\"training.log\", \"w\") as f:\n",
    "    for entry in trainer.state.log_history:\n",
    "        f.write(str(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualização das métricas de treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O TRL imprime métricas por step no `training.log`.  \n",
    "O script `visualization.py` parseia esses dicionários e cria um painel de plots (loss, KL, recompensas etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_plot_training_log(log_file_path):\n",
    "    with open(log_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        log_text = f.read()\n",
    "\n",
    "    pattern = r\"\\{'loss':.*?\\}\"\n",
    "    matches = re.findall(pattern, log_text, re.DOTALL)\n",
    "\n",
    "    metrics_list = []\n",
    "    for match in matches:\n",
    "        try:\n",
    "            metrics_list.append(ast.literal_eval(match))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    steps = list(range(len(metrics_list)))\n",
    "\n",
    "    plots_config = [\n",
    "        (\"loss\", \"Loss\", \"tab:red\"),\n",
    "        (\"grad_norm\", \"Gradient Norm\", \"tab:orange\"),\n",
    "        (\"learning_rate\", \"Learning Rate\", \"tab:green\"),\n",
    "        (\"num_tokens\", \"Total Tokens\", \"tab:blue\"),\n",
    "        (\"completions/mean_length\", \"Mean Completion Length\", \"tab:purple\"),\n",
    "        (\"completions/clipped_ratio\", \"Clipped Ratio\", \"tab:brown\"),\n",
    "        (\"rewards/match_format_approximately/mean\", \"Format Match Reward\", \"tab:pink\"),\n",
    "        (\"rewards/check_numbers/mean\", \"Check Numbers Reward\", \"tab:olive\"),\n",
    "        (\"reward\", \"Total Reward\", \"tab:cyan\"),\n",
    "        (\"reward_std\", \"Reward Std Dev\", \"tab:gray\"),\n",
    "        (\"kl\", \"KL Divergence\", \"tab:red\"),\n",
    "        (\"epoch\", \"Epoch\", \"tab:blue\"),\n",
    "    ]\n",
    "\n",
    "    for idx, (key, title, color) in enumerate(plots_config, 1):\n",
    "        ax = plt.subplot(4, 3, idx)\n",
    "        values = [m.get(key, None) for m in metrics_list]\n",
    "        valid = [(i, v) for i, v in enumerate(values) if v is not None]\n",
    "        if valid:\n",
    "            valid_steps, valid_values = zip(*valid)\n",
    "            ax.plot(valid_steps, valid_values, color=color, alpha=0.4, label=\"Raw\")\n",
    "            if len(valid_values) > 5:\n",
    "                window_size = min(max(5, len(valid_values) // 20), len(valid_values))\n",
    "                smoothed = uniform_filter1d(valid_values, size=window_size)\n",
    "                ax.plot(valid_steps, smoothed, color=color, linewidth=2.5, label=\"Smoothed\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_title(title)\n",
    "            ax.legend()\n",
    "        else:\n",
    "            ax.set_title(title)\n",
    "            ax.text(0.5, 0.5, f\"No data for {key}\", ha=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return metrics_list, fig\n",
    "\n",
    "# Plota métricas de treino (descomente após treinar)\n",
    "metrics, fig = parse_and_plot_training_log(\"training.log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Avaliação no split de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A avaliação (`eval.py`) compara:\n",
    "\n",
    "- **modelo base** `unsloth/gemma-3-1b-it`;\n",
    "- **modelo GRPO** = base + adaptadores LoRA salvos.\n",
    "\n",
    "O split **test** do GSM8K tem 1.319 questões.  \n",
    "Para cada questão, geramos uma resposta, extraímos o número final e comparamos com o gabarito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração principal\n",
    "NUM_SAMPLES = 1 # Coloque 1319 para testar com o split de teste inteiro\n",
    "BATCH_SIZE = 256\n",
    "SEED = 42\n",
    "RUN_EVAL = True  # mude para True para executar\n",
    "\n",
    "# Parâmetros de geração\n",
    "TEMPERATURE = 1.0\n",
    "TOP_P = 0.95\n",
    "TOP_K = 64\n",
    "MAX_NEW_TOKENS = 1792   # 2048 - 256\n",
    "MAX_PROMPT_LENGTH = 256\n",
    "\n",
    "# Modelos\n",
    "BASE_MODEL_NAME = \"unsloth/gemma-3-1b-it\"\n",
    "GRPO_ADAPTER_PATH = \"gemma-3/\"\n",
    "\n",
    "# Observação: este bloco assume que `system_prompt` e `extract_hash_answer`\n",
    "# foram definidos na seção 3 deste documento.\n",
    "\n",
    "match_boxed = re.compile(r\"\\$\\\\boxed\\{([^}]+)\\}\\$\", flags=re.MULTILINE | re.DOTALL)\n",
    "match_any_number = re.compile(r\"[\\d\\.]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model():\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name=BASE_MODEL_NAME,\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=False,\n",
    "        load_in_8bit=False,\n",
    "        load_in_16bit=True,\n",
    "        full_finetuning=False,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_grpo_model():\n",
    "    model, tokenizer = load_base_model()\n",
    "    model = PeftModel.from_pretrained(model, GRPO_ADAPTER_PATH, is_trainable=False)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_and_prepare_dataset(num_samples, seed=42):\n",
    "    ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "    if num_samples < len(ds):\n",
    "        ds = ds.shuffle(seed=seed).select(range(num_samples))\n",
    "    ds = ds.map(lambda x: {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "        ],\n",
    "        \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "        \"question\": x[\"question\"],\n",
    "    })\n",
    "    return ds\n",
    "\n",
    "\n",
    "def extract_answer(response):\n",
    "    boxed = match_boxed.findall(response)\n",
    "    if boxed:\n",
    "        nums = match_any_number.findall(boxed[-1])\n",
    "        return (nums[-1] if nums else boxed[-1]), \"boxed\"\n",
    "    if \"<SOLUTION>\" in response:\n",
    "        part = response.split(\"<SOLUTION>\", 1)[1]\n",
    "        if \"</SOLUTION>\" in part:\n",
    "            part = part.split(\"</SOLUTION>\", 1)[0]\n",
    "        nums = match_any_number.findall(part)\n",
    "        if len(nums) == 1: return nums[0], \"solution_single\"\n",
    "        if len(nums) > 1: return nums[-1], \"solution_multiple\"\n",
    "        return None, \"solution_no_number\"\n",
    "    nums = match_any_number.findall(response)\n",
    "    if nums: return nums[-1], \"last_number\"\n",
    "    return None, \"no_number\"\n",
    "\n",
    "\n",
    "def evaluate_answer(extracted, truth):\n",
    "    if extracted is None: return False\n",
    "    try:\n",
    "        return abs(float(extracted) - float(truth)) < 1e-6\n",
    "    except Exception:\n",
    "        return str(extracted).strip() == str(truth).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(model, tokenizer, prompts, batch_size=8):\n",
    "    generations = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        texts = [\n",
    "            tokenizer.apply_chat_template(p, add_generation_prompt=True, tokenize=False)\n",
    "            for p in batch\n",
    "        ]\n",
    "        inputs = tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_PROMPT_LENGTH,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            top_k=TOP_K,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        for j, out in enumerate(outputs):\n",
    "            input_len = inputs[\"input_ids\"][j].shape[0]\n",
    "            generations.append(\n",
    "                tokenizer.decode(out[input_len:], skip_special_tokens=True)\n",
    "            )\n",
    "    return generations\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, batch_size=8, name=\"base\"):\n",
    "    prompts = [ex[\"prompt\"] for ex in dataset]\n",
    "    truths  = [ex[\"answer\"] for ex in dataset]\n",
    "    gens = generate_batch(model, tokenizer, prompts, batch_size=batch_size)\n",
    "\n",
    "    correct = 0\n",
    "    for g, t in zip(gens, truths):\n",
    "        extracted, _ = extract_answer(g)\n",
    "        if evaluate_answer(extracted, t):\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / len(dataset) * 100\n",
    "    return {\"accuracy\": acc, \"num_correct\": correct, \"num_examples\": len(dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_EVAL:\n",
    "    ds_test = load_and_prepare_dataset(NUM_SAMPLES, SEED)\n",
    "\n",
    "    base_model, tok = load_base_model()\n",
    "    base_res = evaluate_model(base_model, tok, ds_test, BATCH_SIZE, \"base\")\n",
    "    del base_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    grpo_model, tok = load_grpo_model()\n",
    "    grpo_res = evaluate_model(grpo_model, tok, ds_test, BATCH_SIZE, \"grpo\")\n",
    "\n",
    "    print(base_res)\n",
    "    print(grpo_res)\n",
    "\n",
    "    base_accuracy = base_res[\"accuracy\"]\n",
    "    grpo_accuracy = grpo_res[\"accuracy\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    models = [\"Base\", \"GRPO\"]\n",
    "    accuracies = [base_accuracy, grpo_accuracy]\n",
    "    colors = [\"#3498db\", \"#2ecc71\"]\n",
    "\n",
    "    bars = ax.bar(models, accuracies, color=colors, width=0.5)\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            h,\n",
    "            f\"{h:.2f}%\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_ylabel(\"Performance (%)\")\n",
    "    ax.set_title(\"Comparação de Performance no GSM8K\")\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.grid(axis=\"y\", alpha=0.3, linestyle=\"--\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Defina RUN_EVAL=True para rodar a avaliação e gerar o gráfico.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O fine-tuning por **GRPO + LoRA** trouxe um ganho de cerca de **2.5 p.p.** de acurácia no GSM8K para o Gemma-3 1B no nosso teste, após 41 horas de treino numa RTX 4090. \n",
    "Mesmo com recompensas relativamente simples (formato + correção numérica), o modelo passa a respeitar melhor a estrutura de raciocínio e melhorar a resposta final."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
