{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "n_observations = 4\n",
    "n_actions = 2\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "LR = 1e-4\n",
    "CAPACITY = 10000\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_observations, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, observation):\n",
    "        logits = self.network(observation)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement an epsilon-greedy policy, whith which we select a random action with probability epsilon and the action that maximizes Q otherwise. Here epsilon decays exponentially to a set minimum as the total number of steps increases.\n",
    "\n",
    "How do we perform optimization in batches where a random action was chosen? If the random action was not what the network predicted would yield most reward, we'll be penalizing our model for predicting the correct action. What should I do then?\n",
    "\n",
    "Maybe it doesn't matter. Since all we're predicting is the total reward given a certain action, it doesn't matter whether we chose a bad or good action. All that matters is how good the network's prediction was.\n",
    "\n",
    "Yes, it doesn't matter. We are indeed picking the action with highest expected reward. Nevertheless, we're training the model to predict expected reward given an action, so it doesn't matter which one we picked.\n",
    "\n",
    "Also, we input only one action at a time, not batches of actions. Batches come into play when we're computing the loss and optimizing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, observation, train=True):\n",
    "    global steps_done\n",
    "    # gym's actions are always ndarrays\n",
    "    tensor_observation = torch.from_numpy(observation)\n",
    "\n",
    "    # Compute epsilon\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-EPS_DECAY * steps_done)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_observation)\n",
    "        if sample > eps_threshold:\n",
    "            # Pick action with biggest predicted reward\n",
    "            action = logits.argmax().item()\n",
    "        else:\n",
    "            # Pick action at random\n",
    "            action = env.action_space.sample()\n",
    "    if train:\n",
    "        steps_done += 1\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the most recent state transitions up to CAPACITY which we'll randomly sample from to optimize our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ['observation', 'action', 'reward', 'next_observation'])\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.replay_memory = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.replay_memory)\n",
    "\n",
    "    def store(self, observation, action, reward, next_observation):\n",
    "        self.replay_memory.append(Transition(observation, action, reward, next_observation))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(self.__len__(), batch_size)\n",
    "        return random.sample(self.replay_memory, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization will continuously update the graph of the series of episode durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_plot(data, title, xlabel, ylabel, grid=True, sleep=0.5):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(grid)\n",
    "    plt.show()\n",
    "    time.sleep(sleep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to test the model. It'll continuously update a plot with the duration of each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, n_episodes):\n",
    "    durations = []\n",
    "    for _ in range(n_episodes):\n",
    "        episode_duration = 0\n",
    "        observation, _ = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        while not (terminated or truncated):\n",
    "            action, _ = select_action(model, observation, train=False)\n",
    "            observation, _, terminated, truncated, _ = env.step(action)\n",
    "            episode_duration += 1\n",
    "        durations.append(episode_duration)\n",
    "        update_plot(durations, 'Episode durations', 'Episode', 'Duration', grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(CAPACITY)\n",
    "model = DQN(n_observations, n_actions).to(device)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything below is a mess lol... I need to find a way to mask the final observations (not too hard, create tensor with 1s and 0s and multiply with Q) and figure out other things but it shouldn't take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, n_episodes, loss_fn, optimizer):\n",
    "    for _ in range(n_episodes):\n",
    "        # Initialize env\n",
    "        observation, _ = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        while not (terminated or truncated):\n",
    "            # Select and execute action\n",
    "            action = select_action(model, observation)\n",
    "            next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # CartPole always gives reward = 1 even when episode terminates\n",
    "            # This fixes the problem\n",
    "            if terminated or truncated:\n",
    "                reward = 0.0\n",
    "\n",
    "            # Store transition in memory\n",
    "            memory.store(observation, action, reward, next_observation)\n",
    "\n",
    "            # Sample and modify batch of transitions\n",
    "            transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "            observation_batch = torch.from_numpy(np.vstack([t.observation for t in transitions]))\n",
    "            action_batch = torch.from_numpy(np.vstack([t.action for t in transitions]))\n",
    "            reward_batch = torch.from_numpy(np.vstack([t.reward for t in transitions]))\n",
    "            next_observation_batch = torch.from_numpy(np.vstack([t.next_observation for t in transitions]))\n",
    "\n",
    "            pred = model(observation_batch)\n",
    "            next_pred = model()\n",
    "            target = reward_batch + GAMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "observation, _ = env.reset()\n",
    "terminated, truncated = False, False\n",
    "while not (terminated or truncated):\n",
    "    action = select_action(model, observation, train=False)\n",
    "    next_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    # Env gives reward = 1 even when episode terminates, this fixes the bug\n",
    "    if terminated or truncated:\n",
    "        reward = 0.0\n",
    "    memory.store(observation, action, reward, next_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Transition(observation=array([ 0.02976117, -0.03038454, -0.01162784, -0.0406074 ], dtype=float32), action=1, reward=1.0, next_observation=array([ 0.11333289,  0.9532838 , -0.16673163, -1.7010267 ], dtype=float32)),\n",
       " Transition(observation=array([ 0.02976117, -0.03038454, -0.01162784, -0.0406074 ], dtype=float32), action=0, reward=1.0, next_observation=array([ 0.05018069,  0.16634908, -0.05177746, -0.3692666 ], dtype=float32)),\n",
       " Transition(observation=array([ 0.02976117, -0.03038454, -0.01162784, -0.0406074 ], dtype=float32), action=0, reward=1.0, next_observation=array([ 0.08699369,  0.56024   , -0.11855218, -1.040774  ], dtype=float32))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = memory.sample(10)\n",
    "batch_observations = torch.from_numpy(np.vstack([t.observation for t in transitions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0298, -0.0304, -0.0116, -0.0406],\n",
       "        [ 0.0298, -0.0304, -0.0116, -0.0406],\n",
       "        [ 0.0298, -0.0304, -0.0116, -0.0406],\n",
       "        [ 0.0298, -0.0304, -0.0116, -0.0406],\n",
       "        [ 0.0298, -0.0304, -0.0116, -0.0406],\n",
       "        [ 0.0298, -0.0304, -0.0116, -0.0406],\n",
       "        [ 0.0298, -0.0304, -0.0116, -0.0406],\n",
       "        [ 0.0298, -0.0304, -0.0116, -0.0406],\n",
       "        [ 0.0298, -0.0304, -0.0116, -0.0406],\n",
       "        [ 0.0298, -0.0304, -0.0116, -0.0406]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
