{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04926112  0.1610219  -0.01655035 -0.3467851 ]\n",
      "1.0\n",
      "False\n",
      "False\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(observation)\n",
    "print(reward)\n",
    "print(terminated)\n",
    "print(truncated)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = 4\n",
    "n_actions = 2\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "EPS_START, EPS_END = 0.9, 0.05\n",
    "EPS_DECAY = 1000\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_observations, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, observation):\n",
    "        logits = self.network(observation)\n",
    "        return logits\n",
    "    \n",
    "model = DQN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement an epsilon-greedy policy, whith which we select a random action with probability epsilon and the action that maximizes Q otherwise. Here epsilon decays exponentially to a set minimum as the total number of steps increases.\n",
    "\n",
    "How do we perform optimization in batches where a random action was chosen? If the random action was not what the network predicted would yield most reward, we'll be penalizing our model for predicting the correct action. What should I do then?\n",
    "\n",
    "Maybe it doesn't matter. Since all we're predicting is the total reward given a certain action, it doesn't matter whether we chose a bad or good action. All that matters is how good the network's prediction was.\n",
    "\n",
    "Yes, it doesn't matter. We are indeed picking the action with highest expected reward. Nevertheless, we're training the model to predict expected reward given an action, so it doesn't matter which one we picked.\n",
    "\n",
    "Also, we input only one action at a time, not batches of actions. Batches come into play when we're computing the loss and optimizing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, observation):\n",
    "    global env, steps_done\n",
    "    # gym's actions are always ndarrays\n",
    "    tensor_observation = torch.from_numpy(observation)\n",
    "\n",
    "    # Compute epsilon\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-EPS_DECAY * steps_done)\n",
    "\n",
    "    logits = model(tensor_observation)\n",
    "    with torch.no_grad():\n",
    "        if sample > eps_threshold:\n",
    "            # Pick action with biggest predicted reward\n",
    "            action = logits.argmax().item()\n",
    "        else:\n",
    "            # Pick action at random\n",
    "            action = env.action_space.sample()\n",
    "    steps_done += 1\n",
    "    return action, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "observation, info = env.reset()\n",
    "\n",
    "t_start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    action, _ = select_action(model, observation)  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the most recent state transitions up to CAPACITY which we'll randomly sample from to optimize our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition', ['observation', 'action', 'reward', 'next_observation'])\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.replay_memory = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.replay_memory)\n",
    "\n",
    "    def store(self, observation, action, reward, next_observation):\n",
    "        self.replay_memory.append(Transition(observation, action, reward, next_observation))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(self.__len__(), batch_size)\n",
    "        return random.sample(self.replay_memory, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(memory))\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation, _ = env.reset()\n",
    "action = env.action_space.sample()\n",
    "next_observation, reward, _, _, _ = env.step(action)\n",
    "memory.store(observation, action, reward, next_observation)\n",
    "transition = memory.sample(5)\n",
    "print(len(transition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization will continuously update the graph of the series of episode durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations(episode_lenghts):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
